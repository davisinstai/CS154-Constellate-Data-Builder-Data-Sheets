MATEC Web of Conferences 201, 04004 (2018) https://doi.org/10.1051/matecconf/201820104004
ICI 2017
© The Authors, published by EDP Sciences. This is an open access article distributed under the terms of the Creative Commons Attribution 
License 4.0 (http://creativecommons.org/licenses/by/4.0/).
* Corresponding author:  meilingj@mail.tku.edu.tw 
Smart Learning of Porn Fake News in the Family-Friendly Filters 
Meiling Jow1,*, and Yaojung Shiao2 
1Department of Information and Communication, Tamkang University, Taipei, Taiwan 
2National Taipei University of Technology, Taipei, Taiwan 
Abstract. The proliferation of fake news on Facebook and Google has been a hot-button topic after the 
2016 US presidential election. Fake news phenomenon is not limited in the political sphere. The porn 
industries have been using affiliate marketers to send fake news to reach more consumers, even children. 
Easy availability of pornography for children on the internet has been an issue. In US, the average age of 
exposure to porn is 11 to 12. Frequent exposure to pornography may lead to normalization of harmful 
behaviors. Starting late 2013, internet service providers in Britain made “family-friendly filters,” which 
block X-rated websites, the default for customers, because kids are exposed to pornography at a young age. 
Google banned pornographic ads from its search engine from July 2014. Prostitution and escort services 
extend its market despite these efforts for the sake of the upsurge porn fake news. Porn fake news is 
produced purposefully to click, share, react, and comment. To mitigate the damage caused by porn fake 
news, designing a fully automated fake news detector is currently infeasible, because the problem at hand is 
too complex for technology alone. Even the subproblem of defining the criteria under which to classify 
news as “fake” creates ambiguity that requires human judgment. The ability to determine whether an article 
is real or fake requires more than just information about the article; it requires an understanding of cultural 
factors, for example “tea” maybe used by prostitution and escort services in Taiwan. This paper suggests 
one way to use artificial intelligence and human judgment to make it more valid to quarantine porn fake 
news. 
1 Introduction 
Fake news currently plagues the world. The proliferation 
of fake news on Facebook, Twitter and Google has been 
a hot-button topic after the 2016 US presidential election. 
Fake news is fueled in part by advances in technology. 
Wonder Woman star Gal Gadot’s face was pasted onto a 
porn actress’s body [1]. According to the Reuters report: 
“from bots that automatically fabricate headlines and 
entire stories to computer software that synthesizes 
Donald Trump’s voice and makes him read tweets to a 
new video editing app that makes it possible to create 
authentic-looking videos in which one person’s face is 
stitched onto another person’s body.” The war to fight 
fake news is not going to be easy. 
Fake news phenomenon is not limited in the political 
sphere. The porn industry has long been using affiliate 
marketers to send porn fake news to reach more 
consumers, among them even young children. Average 
age of first internet exposure to pornography is 11 years 
old, according to research. Dissemination of obscene 
material to children regardless of its form from 
traditional broadcasting is outlawed by almost every 
nation in the world. Criminal Offences relating to the 
publication of obscene material have long existed in 
England. But the distribution and marketing of porn 
industry evolves wisely along with the advance of 
communication technologies to avoid regulation, while 
age-verification requirements for online porn is not 
effectively working. 
Kids are exposed to pornography at a young age on 
the internet because of its easy accessibility. There are 
4.2 million pornographic websites, 12% of all websites. 
Some porn sites get more traffic than news sites like 
CNN. According to the website Paint Bottle, 30 percent 
of all data transferred online is porn. Pornhub is the 63rd 
most visited site on the Internet according to Alexa [2]. 
All kinds of people can access it, whether they are 
minors, adults, or people with psychological problems. 
Children are particularly vulnerable.  
Easy availability of pornography for children on the 
internet has been an issue worth worrying. A lot of 
online pornography is violent. Frequent exposure to 
pornography may lead to normalization of rape, risky 
sexual behavior, or sex at a young age among other 
effects. The easy availability of pornography is creating 
a dangerous situation where kids get caught up in 
sexually addictive behavior at an early age. Mental 
health professionals are fearful of the impact on future 
generations, comparing internet pornography to “crack 
cocaine” because of its highly addictive nature [3]. 
2 Banning from countries, search 
engines, and social media 
#
#2
MATEC Web of Conferences 201, 04004 (2018) https://doi.org/10.1051/matecconf/201820104004
ICI 2017
 
The first defense against child exposure to pornography 
and exploitation is to use a good internet filter, or 
subscribe to a filtered internet service. The internet 
regulation has relied on the system of user reporting, a 
“notify and take down” basis, to tackle the problem of 
online porn. In the United Kingdom the blacklist is open 
and available through institutions such as Internet Watch 
Foundation. 
To better crack down on online pornography and 
make the internet safer for children, internet service 
providers in Britain were asked to make “family-friendly 
filters,” which block X-rated websites, the default for 
customers starting from 2013, because kids are exposed 
to pornography at a young age. Instead of letting people 
choose what filtering they want, the "Yes" option for 
filtering was pre-ticked. However, the policy was been 
challenged over its censorship. Studies of filters on some 
UK ISPs have shown that well-known porn sites go 
unblocked while education sites about sexually 
transmitted diseases or sexual health are inaccessible [4]. 
At the same time, ISPs and search engines were 
under the pressure from the advocacy groups to tackle 
illegal pornography. Google changed its advertising 
policy in March 2014 to ban pornographic ads from its 
search engine, including ads promoting underage and 
non-consensual sexual content, as well as prostitution 
and escort services. 
Facebook restricts the display of nudity and sexual 
activity because of their audiences cultural background 
or age. Machine learning is used to see if something in a 
photo violates the company's "Community Standards." 
The social media gave people the option to block mature 
and suggestive content from their News Feeds from 
August in 2016. 
Although YouTube has systems in place to take swift 
action on obscene material both through machine 
learning and by increasing human and technical 
resources, a BBC Trending investigation has discovered 
a flaw in a tool that enables the public to report abuse. 
Part of YouTube's system for reporting sexualized 
comments left on children's videos has not been 
functioning correctly for more than a year [5]. 
 
3 Fighting back from the porn industry 
After the banning of pornographic ads and blocking 
mature contents, porn industries tried to utilize key 
influencers and embedded marketing in which references 
to porn websites or videos are incorporated into another 
work, such as a newspaper or television news, with a 
specific intent to promote porn websites or videos. Event 
marketing is entering a guerrilla era where the physical 
and the virtual cross paths, offering new options for 
marketing professionals who create buzz over a service 
or product. The created event is called pseudo-event in 
journalism. For example, the porn industry might create 
a seemingly newsworthy information to distribute a 
news, like an Irish reporter discovered the secret to use 
the keyword of “scannáin” being able to search for a lot 
of hidden pornographic videos on the YouTube [6]. 
 
 
Fig. 1. Google search result. This is a figure showing how the 
news was reported not in 2015 but also till 2018. 
The news would then be reported all over the world 
to give information to people, even children, how to find 
pornography and come along with other true news again 
and again like the hotnews below (Figs. 2 & 3). Even in 
2018, YouTubers will stumble upon what is really porn 
when searching for the keyword “scannán.” 
 
Fig. 2. Type of product placement. This is a figure showing 
how the porn sponsored news repeatedly appears on the 
webnews of a mainstream newspaper of Taiwan in 2018. 
 
Fig. 3. Type of product placement. This is a figure showing 
how porn promoted stories repeatedly appears on the webnews 
of a mainstream newspaper of Taiwan in 2018. 
Porn fake news is produced purposefully to click, 
share, react, and comment just like political fake news. 
Unlike a political fake news aiming to disrupt an election 
or provoke civil unrest, porn fake news is not an issue of 
#
#3
MATEC Web of Conferences 201, 04004 (2018) https://doi.org/10.1051/matecconf/201820104004
ICI 2017
 
what news sources to trust or whether the information is 
accurate or not. It is an event marketing by interactively 
hiring online key influencers to create fictional or makeup
news stories with clickbaiting headlines geared to 
travel on social media to click on and share as 
propaganda, which is spread even more widely by 
placement marketing of newspaper and television news 
broadcasting and will last long after the reporting by way 
of PR promotion as advertising (sponsored news you 
may like) to come along with other true news again and 
again to be clicked and to promote their porn websites or 
videos in the nature. 
 
Fig. 4. Type of product placement. This is a figure showing 
how porn fake news appears on the webnews of a mainstream 
newspaper of Taiwan in 2018 without marking as promoted. 
To overcome the issue of age-appropriateness and 
general decency, the pictures are carefully processed, as 
above, not to reach the level of obscenity or sexually 
explicit, so that volunteer moderators can not report it to 
the regulative body. PornHub is especially good at 
making porn fake news without violating ageappropriateness
and general decency. It launched its first 
US advertising campaign, a crowdsourcing competition 
to require entrants to submit a G-rated, suitable for work 
ad (print or multimedia) that clearly promotes the brand 
in 2014. The event received worldwide broadcasting and 
publication, and the winning advertising then was widely 
spread out in news and shared among social media on 
Christmas. 
Hence, a particular event coming from a legitimate 
site of news sources cannot guarantee the event is not a 
porn fake news. For example, CNBC had aired specials 
on prostitution and a documentary called “Porn: 
Business of Pleasure.” Viewers can see a pornographic 
product placement on TV and newspaper’s interviews 
with a series of porn-makers or porn star [7]. 
Production of the fake news is to spread out the 
information about a porn website, prostitution, porn 
related services, videos or products to attract maximum 
attention of people. According to Pornhub, nearly 92 
billion videos were watched over the course of 23 billion 
visits to the site by many millions of very horny visitors 
in 2016. That's 64 million visitors per day, or 44,000 
every minute. 
4 How AI and machine learning fight 
fake news. 
After coming under heavy public criticism for not taking 
full responsibility for fake news affecting the outcome of 
the 2016 presidential election, the technology tycoons 
such as Google, Facebook, and Twitter now plan to 
crack down on fake news. There are four ways 
commonly used by AI and machine learning [8]: 
 
1. Score Web Pages: Google takes the accuracy of facts 
presented to score web pages. The technology has grown 
in significance as it makes an attempt to understand 
pages’ context without relying on third-party signals. 
2. Weigh Facts: A Natural Language Processing engine 
can go through the subject of a story along with the 
headline, main body text, and the geo-location. Further, 
artificial intelligence will find out if other sites are 
reporting the same facts. In this way, facts are weighed 
against reputed media sources. 
3. Predict Reputation: Using predictive analytics backed 
by Machine Learning, a website’s reputation can be 
predicted through considering multiple features like 
domain name and Alexa web rank. 
4. Discover Sensational Words: When it comes to news 
items, the headline is the key to capturing the attention 
of the audience. Artificial Intelligence has been 
instrumental in discovering and flagging fake news 
headlines by using keyword analytics. 
 
These might be ways to combat political fake news, 
but they are not good ways to mitigate the damage 
caused by porn fake news for children. 
AI systems are used to fill the gaps left by online 
fact-checking outlets, whose human fact-checkers lack 
the bandwidth to evaluate every article that appears 
online. Designing a fully automated fake news detector 
is currently infeasible, because the problem at hand is 
too complex for technology alone. 
There are many different categories misinformation 
can fall into. Even the subproblem of defining the 
criteria under which to classify news as “fake” creates 
ambiguity that requires human judgment. The ability to 
determine whether an article is real or fake requires more 
than just information about the article; it requires an 
understanding of cultural factors, for example “tea” 
maybe used by prostitution and escort services in Taiwan, 
and “John” is prostitute's client in US slang. Algorithms 
will be helpful, but real progress on understanding or 
controlling the fake news phenomenon is ultimately 
about humans not machines. Current forms of AI can 
look at the style of the language, and the topic that the 
text is discussing, but it can’t figure out the meaning 
behind statements [9]. 
Even though an AI-based system like Enterra would 
be able to interpret the phrase "seeing red" in context as 
referring to a high level of frustration, because it can 
perform human-like reasoning and analyze the report 
#
#4
MATEC Web of Conferences 201, 04004 (2018) https://doi.org/10.1051/matecconf/201820104004
ICI 2017
 
contextually [10], it might not be able to detect the 
fraudulent stories disguised in the form of product 
placement in journalism. 
The way that Facebook polices mature content is 
using AI to monitor video on Facebook Live and flag it, 
if offensive content is found. But the study doesn’t 
recommend the machine learning to make a model to 
flag news content as porn fake news. Using a red banner 
to highlight that a story is fake to prevent it going viral is 
even worse for decreasing children’s accessibility to 
porn fake news by alerting their attention. 
Trying to predict whether a news site is a fake news 
site or not is one possible way to solve the porn fake 
news problem if these stories come from the bloggers or 
key influencers of social media who constantly produce 
porn fake news. But this way is not realistic for the porn 
fake news produced as an event marketing by the porn 
industry on the mainstream media. 
The study suggests one way to solve the problem: 
beyond predicting a website’s reputation and discovering 
sensational words, researchers need to create a database 
according to cultural codes and to use a Natural 
Language Processing engine to go through the message 
to find the websites or the keywords mentioned in the 
news, and then to follow and check the websites or the 
keywords. If they lead to an age-verification sign like 
Fig. 5 or obscene pictures or videos, then they must be a 
news to be filtered away from children. 
 
Fig. 5. Age-verification sign. This is a figure showing that an 
age-verification sign used in Taiwan, which can be easily 
cheated by children under 18 years old to click the “I’m over 
18 years old” button and pass age-verification. 
5 Conclusions 
The study points to a fact which needs equal attention 
when the world is focusing on the political fake news 
after the 2016 US presidential election. That is porn fake 
news which has much impact on children. To protect 
children from viewing inappropriate adult material, 
United Kingdom is going to activate the Age 
Verification for Online Pornography (clauses 15-25) on 
the Digital Economy Bill from 2018, which asks anyone 
who makes pornography available online on a 
commercial basis must ensure under 18s in the UK 
cannot access it. If machine learning can do a better job 
to combat the war by building a better family-friendly 
filter, government regulation can be loosen. The 
researcher observed different types of porn fake news 
and how they appear on different media to spread. These 
observations might provide researchers in machine 
learning or other AI tools some thoughts to build a more 
useful model to mitigate the danger of porn fake news to 
children. 
References 
1. P. Perry, Available online:  
http://bigthink.com/philip-perry/these-ai-toolscould-lead-to-the-next-generation-of-fake-news
(accessed on 2017). 
2. L. Gilkerson, Available online: 
http://www.covenanteyes.com/2013/02/19/pornogra
phy-statistics/ (accessed on 22 Sep. 2013). 
3. Available online:  
http://coheargroup.com/services/sexual-recoveryservices/pornography-addiction-in-adolescents/
(accessed on 2017). 
4. Available online:   
http://www.bbc.com/news/technology-23403068 
(accessed on 2017). 
5. BBC, Available online:  
http://citifmonline.com/2017/11/24/flaw-youtubesobscenity-tracking-tool/
(accessed on 2017). 
6. T. Lee, Available online:  
http://www.ubergizmo.com/2015/02/porn-uploadedand-disguised-on-youtube-using-irish-language-
titles/ (accessed on 2017). 
7. CNBC Available online:   
https://www.mrc.org/articles/cnbc-mainstreamsgross-out-porn
(accessed on 2017). 
8. R. Akiwatkar, Available online: 
https://channels.theinnovationenterprise.com/articles
/how-can-artificial-intelligence-combat-fake-news 
(accessed on 2017). 
9. Available online:  https://towardsdatascience.com/itrained-fake-news-detection-ai-with-95-accuracy-
and-almost-went-crazy-d10589aa57c (accessed on 
2017). 
10. A. Wolk, Available online:   
https://www.forbes.com/sites/alanwolk/2018/01/17/
can-enterras-advanced-ai-systems-stop-the-fakenews-epidemic/#6d264af92049
(accessed on 2018). 
Chinese Journal of Electronics
Vol.27, No.3, May 2018
Predicting Future Rumours∗
QIN Yumeng1, Dominik Wurzer2,3 and TANG Cunchen1
(1. School of Computer Science (International School of Software), Wuhan University, Wuhan 430072, China)
(2. School of Information Management, Wuhan University, Wuhan 430072, China)
(3. AI Machina Technologies, Vienna A-1010, Austria)
Abstract — Recent uproar of fake news and misinformation
on social media platforms has sparked the interest
in the scientific community to automatically detect and refute
them. The most popular research task to counteract
misinformation, Rumour detection, requires repeated signals
to reach adequate detection accurate. Consequently,
rumour detection recognizes rumours only when they have
started spreading and causing harm. We introduce a new
task called “rumour prediction” that assesses the possibility
of a document arriving from a social media stream
becoming a rumour in the future. Note that rumour prediction
differentiates itself from rumour detection through
instant decision making. This allows refuting misinformation
before it spreads and causes harm. Our approach to ru-
mour prediction harnesses content based features in combination
with novelty based features and pseudo feedback.
Our experiments show that we are able to accurately predict,
whether a document will become a rumour in the
future. Additionally, we show how rumour prediction can
significantly improve the accuracy of state-of-the-art Rumour
detection systems.
Key words — Real-time rumour prediction, Social media,
Data stream, Information retrieval, K -term hashing,
Pseudo feedback.
I. Introduction
Social media has evolved from friendship based networks
to a major source for the consumption of news[1].
On social media, news is decentralised as it provides everyone
the means to efficiently report and spread informa-
tion. In contrast to traditional news wire, social media services
spread information without intensive investigation,
fact or background checks. The combination of ease and
fast pace of sharing information provides a fertile breeding
ground for rumours, false- and disinformation. Rumours
and deliberate disinformation have already caused panic
and influenced public opinion. Cases like in Germany and
Austria∗1 in 2016, show how misleading and false information
about crimes committed by refugees negatively
influenced the opinion of citizens.
Rumours are by definition circulating information of
unconfirmed or doubtful truth. This imposes two conditions
for rumours: 1) rumours must contain false or
doubtful information 2) rumours must spread. A message
containing false information is not a rumour if it
does not spread. Only after it is picked up by others
and distributed, it becomes a rumour. Social media users
tend to share controversial information, while asking peers
about their opinions[2]. Instead of containing or refuting
rumours, this behaviour further amplifies their spread and
reach. Researchers try to refute wrong claims and prevent
rumours from spreading and causing harm by automatically
detecting them[3−5]. Following the definition of what
a rumour is, rumours can only be recognized and detected
once they have spread. The further a rumour has spread,
the more likely it is to be questioned by other users or
the traditional media[6]. Rumour detection relies on those
signals to distinguish between rumours and non-rumours.
Analysing contradicting claims and graphs propagation
patterns allows accurate rumour identification once they
have sufficiently spread. By then rumours might already
have caused harm or be debunked by other users. Recently,
researchers tried to address this problem by focus-
ing on “early rumour detection”, which detects rumours
with “short” delays of up to 24 hours[2,6−8]. Consequently,
automated rumour detectio and early rumour detection
lag behind social media users when it comes to recognizing
rumours and refuting wrong claims.
In this paper we present a new task called “rumour
prediction”, which is conceptually related to the existing
task of Rumour detection. Instead of detecting rumours
retrospectively, Rumour prediction computes the likelihood
of a message becoming a rumour in the future at its
publication time. Rumour prediction is a streaming task
∗Manuscript Received May 26, 2017; Accepted Nov. 10, 2017.
∗1 http://hoaxmap.org/ on 2016-2-15
c© 2018 Chinese Institute of Electronics. DOI:10.1049/cje.2018.03.008
#
#Predicting Future Rumours 515
that requires instant decision making with a single pass
over the data. In a nutshell, Rumour prediction assesses
each message at its publication time, whether it is likely
to become a rumour. This allows addressing the original
goals of Rumour detection by refuting wrong claims
through contradicting information, before rumours go viral
and cause harm. We emphasise the important of recog-
nizing rumours as early as possible – preferably instantly.
Recognizing and predicting rumours on social media
is challenging due to the short texts, creative lexical variations
and high volume of the streams. The task becomes
compound when performed at publication time, without
access to future information. We provide an effective and
highly scalable approach to predict rumours instantly after
they were posted with zero delay. To compensate for
the absence of other user’s reactions and propagation
graph patterns, we propose a new class of features, called
“novelty based features” that consult additional trustworthy
data sources in the form of news wire articles and veri-
fied accounts. Novelty based features capture the first part
of a rumour’s definition, which states that rumours must
contain wrong information. The second part of the definition
for rumours states that the wrong information must
circulate. We introduce Pseudo-feedback features to capture
repeated signals of circulating messages “without”
the need of retrospective operation or access to future information.
All features can be computed in constant time
and space allowing us to process high-volume streams in
real-time[9].
Our experiments reveal that novelty based features in
conjunction with Pseudo-feedback and additional content
features allows for accurate rumour prediction at publication
time. We additionally show how Rumour prediction
significantly increasing detection accuracy for two stateof-the-art
early Rumour detection algorithms.
II. Related Work
Recently, automated Rumour detection on social media
evolved into a popular research field whose motiva-
tion lies in debunking false claims and preventing them
from spreading and causing harm. The most successful approaches
rely on utilizing a feature set to train classifiers
for a detection or classification task. Commonly used feature
categories for the classification include lexical, user-
centric, sentiment analysis[5,10], propagation-based[4,10,11]
and cluster-based[2,3,6] features. The following sections
describe successful approaches to Rumour detection, because
it is conceptually related to our new task of Rumour
prediction.
Content-based features originate from a study by
Ref.[12].
They observed a significant correlation between the
trustworthiness of a tweet with content based characteristics
including hashtags, punctuation characters and sen-
timent polarity. When assessing the credibility of a tweet,
they also assessed the source of its information by constructing
features based on provided URLs as well as user
based features like the activeness of the user and social
graph based features like the frequency of re-tweets. Our
approach to Rumour prediction also incorporates content
based features on the contingency that are available at
the time of a message’s publication.
Propagation-based features are first studied by
Mendoza et al.[13]. While studying the trustworthiness of
tweets during crises, they found that the topology of a
distrustful tweet’s propagation pattern differs from those
of news and normal tweets. These findings along with
the fact that rumours tend to more likely be questioned
by other users than news, paved the way for future research
examining propagation graphs. Shah et al.[14] was
the first to apply epidemiological models to Rumour detection
and devised the Susceptible-infected model (SI).
Later, SI has been extended into SpikeM[15] by adding
a periodical interactive function to monitor the periodical
user behaviour. This allows capturing user behaviour
such as increased activeness during daytime in comparison
with night-time. K. Wu et al.[10] in 2015 proposed
an interesting rumour propagation tree pattern for Sina
Weibo, they identified “opinion leader”, whose followers
count exceed the number of accounts they are followings.
They deem a tweet as rumour if its propagation tree root
(the original posting user) is a non-opinion leader but has
been further re-tweeted by numerous opinion leader. Unfortunately,
these features can only be harnessed retro-
spectively, thus disqualifies for Rumour prediction.
Clustering-based features are sets of posts grouped
together based on their content overlap. Zhao et al.[2]
treat the rumour detection task as finding clusters of
posts, whose topics are disputed factual claims. G. Cai
et al.[3] were among the first to apply rumour detection
in Chinese. Their approach built on harnessing crowd
responses. This work also different from others because
they emphasised the use of stop words and punctuation
character for rumour detection. We also experiment with
punctuation characters and stop words for Rumour prediction,
since they are available at publication time and
were shown to play an important role for rumour detection.
By contrast, cluster based features can only be com-
puted once other users respond to a rumour.
Sentiment analysis served two purposes in the
field of Rumour detection. First, the sentiment of rumours
themselves was shown to diverge from ordinary so-
cial media messages[7,16]. Second, sentiment analysis was
used to determine other user’s attitude towards potential
rumours[5]. Wu[10] added an overall sentiment score
to the edges of its propagation tree pattern. This allowed
representing a re-tweeter’s sentiment towards the original
#
#516 Chinese Journal of Electronics 2018
tweet. Liu et al.[6] utilized sentiment features based on
the number of support, negation, questioning and neutral
words in each tweet. Since the sentiment of a message can
be computed at publication time, we also utilize sentiment
analysis as a feature category for Rumour prediction.
Early rumour detection appeared only recently.
The motivation for Rumour detection lies in debunking
them to prevent them from spreading and causing harm.
Recent research tried to reduce the detection delay by
proposing methods for “early” or “real-time” Rumour detection,
like Liu et al.[6], Wu et al.[10], Zhao et al.[2] and
Zhou et al.[8]. Zhao et al.[2] built cluster based features
while relying on tweets containing enquiry patterns as an
indication of rumours. Although the computation of their
features is efficient, they require repeated mentions in the
form of response by other users. This results in increased
latency between a rumour’s publication and detection.
The approach with the lowest latency banks on the ‘wisdom
of the crowd’[6]. Although they claim to operate in
real-time, they require a cluster of at least 5 messages to
detect a rumour.
Rumour prediction differs from traditional and early
rumour detection because it requires determining the likelihood
of a message becoming a rumour in the future at
the time of its publication. At publication time, future
information about a message’s propagation through the
social graph or the responds of other users are not available.
Consequently, the two most useful feature categories
for Rumour detection – propagation and cluster based
features – are inapplicable to Rumour prediction. We emphasize
the importance of identifying rumours as early as
possible – preferable instantly after their publication, to
avoid them from spreading and causing harm.
III. Predicting Rumour Candidates
Rumour prediction on social media assesses the likelihood
of a message becoming a rumour in the future at
its publication time and without access to future information.
1. Problem statement
Let S be a continuous stream of messages chronologically
ordered by their publication time stamp S =
{m1, m2, m3, · · · , mn}. Further, let mt be the message m
arriving from stream S at time t. Rumour prediction is
then the task of producing a Rumour prediction score
(RPSmt) for each message mt at time t from Stream S.
RPSmt indicates the likelihood of message mt becoming a
rumour in the future. Upon arrival of document mt from
stream S at time t, we derive its corresponding feature
vector Fmt = {f1, · · · , fj}. Given the feature vector Fmt
and a previously obtained feature weight model vector
M = {wf1 , wf2 , · · · , wfj}, we computes the rumour prediction
score RPSmt = MT ×Fmt . Note that the rumour
prediction score RPSmt of message mt is tied to time t.
2. Approach overview
Fig.1 illustrated the entire procedures of our rumour
prediction approach. In particular, we base Ru-
mour prediction on a fixed threshold strategy with respect
to threshold α. Messages (doc1, · · · , doc3) arriving
from a stream are considered as rumour candidates if
their corresponding rumour prediction scores RPS exceed
α: RPSmt > α. The optimal parameter setting for
the feature weight model Fmt and detection threshold α
are learned using a Support vector machine (SVM) on a
training set, while maximising the prediction accuracy.
Fig. 1. Approach overview: illustrating the entire process of
predicting rumour candidates
3. Novelty based features
Rumours are messages circulating through the social
graph that contain wrong information. Novelty based features
focus on identifying messages containing wrong in-
formation. Rumour prediction requires decision making at
a message’s publication time. To compensate for the absence
of future information, we consult additional data in
the form of trusted resources. In particular, we build features
based on a message’s novelty with respect to news
wire articles, which are considered to be trustworthy. This
is reasonable as Petrovic et al.[1] states that in the majority
of cases, news wires lead social media for reporting
news. In a nutshell, the presence of information unconfirmed
by the official media is construed as an indication of
being a rumour. Note that this does not imply that every
message containing unconfirmed information is considered
to become a future rumour.
Novelty based feature construction can be computationally
expensive. High volume streams demand
highly efficient feature computation[17]. We explore two
approaches to novelty computation: one based on vector
proximity, the other on K -term hashing.
Computing novelty based on traditional vector proximity
alone does not yield adequate performance due
to the length discrepancy between news wire articles
and social media messages[18]. To make vector proximity
applicable, we slide a term-level based window, whose
length resembles the average social media message length,
through each of the news articles. This results in subdocuments
whose length resembles those of social media
#
#Predicting Future Rumours 517
messages. Novelty is computed using term weighted tf-idf
dot products between the social media message and all
news sub-documents. The inverse of the minimum similarity
to the nearest neighbour equates to the degree of
novelty.
The second approach relies on K -term hashing[19], a
recent advance in novelty detection known for high efficiency
and effectiveness. K -term hashing has the interest-
ing characteristic of forming a collective ’memory’ where
novelty is estimated based on the fraction of unseen K term
with respect to the memory. We introduce a variant
of K -term hashing, where instead of memorizing previous
documents, we fill the memory with the information
from the trusted resources. To determine a message’s novelty
with respect to the trusted resources we compute the
fraction of its unseen K -terms in the memory.
When K -term hashing was introduced by Wurzer[19]
for novelty detection, all K -terms were weighed equally.
We found that this does not unlock the full potential of
K -terms and additionally extract the top 10 keywords
ranked by tf.idf . This provides a separate set of K -terms
solely based on the top keywords, which we found superior
to equally weighted K -terms. In total we construct 6
novelty based features using K -term hashing – K -terms
of length 1 to 3 based on all terms and keywords.
4. Pseudo feedback feature
In addition to novelty based features we introduce a
Pseudo-feedback (PF) feature, which is conceptually related
to “Pseudo relevance feedback” found in information
retrieval. Rumours are messages circulating through the
social graph that contain wrong information. PF identifies
circulating messages without the need of retrospective
operation or future information. The concept builds upon
the idea that documents, which reveal similar characteristics
as previously detected rumours are also likely to be
a rumour. During detection, feedback about which of the
previous documents describes a rumour is not available.
Therefore, we rely on ‘pseudo’ feedback and consider all
documents whose rumour score exceeds a threshold as
true rumours. The PF feature describes the maximum
similarity, measured by vector proximity in term space,
between a new document and those documents previously
considered as rumour.
Training pseudo feedback features differs from
the standard procedure, which requires two training
phases. The first phase, uses an SVM to compute weights
for all features in the trainings set, “except” the PF features.
This provides us a incomplete model that is sub-
sequently used to determine rumour candidates for the
training of the PF feature. All documents with RPS
greater than the PF threshold α are considered to be
rumour candidates. These candidates are subsequently
used in the second training phase to determine the optimal
weight of the PF feature by computing the minimum
distance for each document with respect to the k most recent
rumour candidates. Once we obtained the values for
the PF feature, we compute its optimal weight using an
SVM. Note that the weights of all other features is based
on the initial training phase.
5. Content-based features
In addition to novelty based features and Pseudo feedback,
we also apply a range of 51 content based features
of 7 categories, listed in Table 1. These features are based
on the message text and include emotion, sentiment and
presence or usage patterns of punctuation, syntax and superlative
forms. Many of them are also applied by rumour
detection studies[2,5−7,12] which is why we don’t explain
them in detail. Our selection focuses on features that can
be computed instantly and based only on a message’s text.
The purpose of content based features is two-fold. First,
content based features aid novelty based features in determining
whether a message contains doubtful or wrong
information. Second, content based features also aid the
PF feature as they were shown to indicate, whether messages
are likely to be distributed in the future[20−22].
Table 1. Feature list
Category Description
Punctuation
Categorical feat. based on the number
of “‘!’‘?’‘.’‘,’ ”
POS
Categorical feat. based on the number of
verbs[16],nouns, adjectives, quantity and
time words
Sentiment
Categorical feat. based on the number of
strong/weak negative/positive words
Emotion
Degree of positive/negative/sad/anxious
/surprised emotion
Social media
Categorical feat. based on the number of
hash-tags and user-names
Length
Categorical feat. based on the number of
unique words
URLs
Categorical feat. based on the number of
URLs, pictures
Novelty
Novelty score based kterm length 1-3
for all and key-words
Pseudo feedback Distance to the closest previous rumour
IV. Experiments
The experiments section elaborates on the performance
of rumour prediction measured by detection ef-
fectiveness and efficiency. In a streaming setting, documents
arrive continuously one at a time. We require our
approach to Rumour prediction to compute the Rumour
prediction scores (RPS) instantaneously at the arrival of
each document, in a single-pass over the data. Messages
with high RPS are considered likely to become a rumour
in the future. The prediction decision is based on an optimal
thresholding strategy optimized on the trainings set.
1. Evaluation metrics
We evaluate rumour prediction as a detection task,
where the goal is to detect those documents likely to
become a rumour in the future. As is usual in the
#
#518 Chinese Journal of Electronics 2018
literature[8], we measure the effectiveness of the detection
task by the detection accuracy. Additionally, we show
Detection error trade-off (DET) curves based on the official
TDT3 evaluation scripts using standard settings,
which are originate from the standard TDT evaluation
procedure[23,24]. DET plots show the trade-off between
miss and false alarm probability for the full range of
thresholds and provide a more comprehensive illustration
of effectiveness than single value metrics[25]. Efficiency is
evaluated by the throughput per second, when applied to
a high number of messages.
2. Data set
Rumour prediction is a novel research field without official
data sets. Although the related research field of ru-
mour detection has gained popularity recently, it does not
provide official data sets due to license agreements that
forbid redistribution of social media data. Consequently,
no data set from previous publications or related research
fields are available to test the effectiveness of Rumour prediction.
We therefore follow previous rumour detection re-
searchers, like Liu et al.[6] and Yang et al.[7], and created
our own dataset. Our message stream is based on Sina
Weibo, a Chinese social media service with more than
200 million active users∗2 . Micro-blogs from Sina Weibo
are denoted as “weibos”.
Trusted resources Before labeling rumours, we randomly
collect 2000 news articles∗3 about broad topics
commonly reported by news wires over our target time
period. This ensures that we cannot know which rumours
we would find later on. The topics range from news about
celebrities and disasters to financial and political affairs.
Rumours Sina Weibo offers an official rumour debunking
service, operated by trained human profession-
als. Following Yang et al.[7] and Zhou et al.[8], we use
this service to obtain a high quality set of 2020 confirmed
rumours.
Non-rumours We additionally gathered 2020 nonrumours
using the public Sina Weibo API∗4 . Three human
annotators judged these weibos based on unanimous decision
making to ensure that they don’t contain rumours.
Since effectiveness is measured by accuracy, we need to
ensure that the number of rumours is the same as the
number of non-rumours.
Since we operate in a streaming environment, all weibos
are sorted based on their publication time-stamp. Af-
ter sorting the rumours and non-rumours chronologically,
we divided them in half, forming a training and test set.
We ensured that each of the sets consists of 50% rumours
and non-rumours. This is important when effectiveness is
measured by accuracy. All training and optimization uses
the trainings set. Performance is then reported based on
a single run on the test set.
3. Effectiveness of rumour prediction
In our first experiment we measure the effectiveness
of the proposed features to predict, whether a message
arriving from the stream will become a rumour in the future.
After optimizing the weights of our 57 features on
the training set, we tested their effectiveness on the test
set and measured an accuracy of 75%. The accuracy is
based on a Recall of 75% and Precision of 74%, resulting
in an F1 score of 74.5%.
4. Feature analysis
In the following section we analyse our features in detail
to gain insights on how they impact the Rumour pre-
diction accuracy. We group our 57 features into 7 featurecategories
(Table 1) and analyse their contribution using
feature ablation, as seen in Table 2. Feature ablation illustrates
the importance of a feature when measuring perfor-
mance of all except the tested feature. Table 2 shows that
novelty based features are dominant for Rumour prediction,
since removing them significantly reduces the RPS
(p < 0.05). Features based on “Sentence chars” contribute
the second most, followed by part of speech and “Extreme
word” features. “Sentiment” and “Emotion” based features
contribute the least.
Table 2. Features ablation: impact on performance when removing feature groups
Features All Sentence char POS Emotion Extreme words Sentiment PF Novelty
Accuracy 75% 69% 71% 74% 72% 74% 71% 60%
Note: Lower accuracy means higher impact; POS: Part of speech; PF: Pseudo-feedback.
Novelty based features revealed the highest impact
on detection performance. We found that K -terms formed
from the top keywords contribute the most. This is interesting,
as when K -term hashing was introduced[19], all K -
terms were considered as equally important. Interestingly,
novelty based features computed by the vector similarity
between weibos and news sub-documents perform slightly
worse (–2% absolute). When striping all but the top tfidf
weighted terms from the news sub-documents, the hit
in performance can be reduced to –1% absolute. Eventhough
K -term hashing reaches the highest accuracy than
vector similarity, we found that combining them results
in an additional accuracy boost. We conclude that vector
proximity and K -term hashing based features capture
different signals. Consequently, combining them benefits
Rumours prediction.
∗2 http://www.bbc.co.uk/news/technology-35361157 as of 2016-2-10
∗3 http://www.xinhuanet.com/; http://www.chinadaily.com.cn/; http://www.bbc.com/zhongwen/simp
∗4 http://open.weibo.com/wiki/API
#
#Predicting Future Rumours 519
Pseudo feedback revealed less impact, to further investigate
the effectiveness of PF, we manually investigate
the 100 highest false positives. Out of these 100 false positives,
16 contained wrong information but did not prop-
agate through the graph. Since the false information was
not picked up by other users, these messages did not develop
into rumours. Examples for such messages include
the death of actress Jamie Chung as well as an earthquake
in Wuhan. To demonstrate that the PF feature correlation
with a message’s spread, we alter its weight to a
sub-optimal setting. This increases the number of messages
with wrong information without later propagation
to 35. We conclude that PF successfully predicts information
spread.
5. Rumour prediction vs. Real-time rumour detection
Rumour prediction is conceptually related to real-
time rumour detection, which aims on detecting rumours
as fast as possible. Table 3 compares our ap-
proach for rumour prediction with two state-of-the-art
real-time rumour detection baselines Liu et al.[6] (dubbed
Liu) and Yang et al.[7] (dubbed Yang), which we reimplemented
according to their publications. Both apply
various message-, user-, topic- and propagation-based features
and rely on an SVM classifier. The table shows that
these state-of-the-art for Rumour detection fall short of
the accuracy of rumour prediction when forced to instant
decision making. Accuracy, although the standard metric
of detection tasks, is a single value metric describing performance
at an optimal threshold. Fig.2 shows a DET plot
comparing the effectiveness of the three algorithms for
the full range of detection threshold for an instantaneous
detection scenario. The DET plot illustrates that Liu’s
method would be preferable over Yang for any threshold
setting. Similarly, the plot reveals that our approach to
rumour prediction dominates both baselines throughout
all threshold settings and for the high-recall region in particular.
Table 3. Detection accuracy at different levels of delay
Algorithm Instant 12 hours 24 hours
Liu 62.27% 74.29% 78.4%
Liu +
Rumour prediction
75.12% * 76.9% 78.9%
Yang 60.21% 65.35% 75.82%
Yang +
Rumour prediction
75.15% * 75.35% * 75.82%
Note: Asterisk (*) indicates significance (p < 0.05)
We want to point out that this experiment does not
claim that rumour prediction is more accurate than rumour
detection in general. Instead, rumour prediction sig-
nificantly outperforms rumour detection approaches directly
or shortly after a message is published. Increas-
ing the detection delay allows rumour detection systems
to gather more evidence and subsequently improve their
detection accuracy. After 24 hours Liu reaches the highest
accuracy at 78.4%. Note that after 24 hours rumours
might have already spread far through social networks and
potentially caused harm. Since both systems apply classification
based detection using an SVM, we can easily
incorporate rumour prediction by harnessing the rumour
prediction score RPS and the features its based on as additional
features. Table 3 reveals that rumour prediction
significantly (p < 0.05) improves the detection accuracy of
both baseline systems when detecting rumours instantaneously
after their publication. The table also reveals that
the signals captured by rumour predictions are additive
to rumour detection because the accuracy increases for
both algorithms at all three measurement times stamps.
Fig. 2. DET plot, revealing superior effectiveness of our approach
for instant Rumour Detection for the full range
of thresholds
6. Efficiency and scalability
To demonstrate the high efficiency of computing the
proposed features, we apply our Rumour Prediction system
to 100k weibos, while measuring its throughput. We
implement our system in C and run it using a single
core on a 2.2GHz Intel Core i7-4702HQ and measure
the throughput on an idle machine and average the observed
performance over 5 runs. The average throughput
of our system is around 7,000 weibos per second, which
clearly exceeds the average volume of the full Twitter∗5
(5,700 tweets/sec.) and Sina Weibo∗6 (1,200 weibos/sec.)
stream. The high throughput remains constant independently
of the number of messages processed.
V. Conclusions
We introduced a new task–Rumour prediction–which
assesses the likelihood of a message becoming a rumour in
future. This allows us to identify and refute rumours before
they spread and cause harm. Our approach is based
on traditional content based features as well as two new
feature categories, Novelty based features and Pseudo
∗5 about.twitter.com/company (last updated: 2015-3-31)
∗6 http://www.techweb.com.cn/internet/2012-01-06/1139327.shtml
#
#520 Chinese Journal of Electronics 2018
feedback. Novelty based features consult additional data
sources to aid the identification of wrong factual claims.
Pseudo feedback, which we deem applicable to a general
detection/classification task on streams, is able to harness
repeated signals without the need of retrospective operation
or future information. Our experiments revealed that
we can accurately predict, whether a social media message
will become a rumour in the future. We also showed that
the proposed features can be computed efficiently enough
to scale to the average Twitter and Sina Weibo stream,
while retaining constant processing time. Furthermore, we
showed that rumour prediction can significantly increase
the accuracy of state-of-the-art real-time rumour detection
approaches.
References
[1] S. Petrovic, M. Osborne, R. McCreadie, et al., “Can Twitter replace
Newswire for breaking news?”, Proceedings of the Seventh
International AAAI Conference on Weblogs and Social Media.,
Massachusetts, USA, pp.713–716, 2013.
[2] Z. Zhao, P. Resnick and Q. Mei, “Enquiring minds: Early detection
of rumors in social media from enquiry posts”, Proceedings
of the 24th International Conference on World Wide Web, Florence,
Italy, pp.1395–1405, 2015.
[3] G. Cai, H. Wu and R. Lv, “Rumours detection in Chinese
via crowd responses”, IEEE/ACM International Conference on
Advances in Social Networks Analysis and Mining (ASONAM ),
Beijing, China, pp.912–917, 2014.
[4] S. Kwon, M. Cha, K. Jung, et al.,“Prominent features of rumor
propagation in online social media”, 13th International
Conference on Data Mining (ICDM), IEEE, Dallas, TX, USA,
pp.1103–1108, 2013.
[5] V. Qazvinian, E. Rosengren, D.R. Radev, et al., “Rumour has
it: Identifying misinformation in Microblogs”, Proceedings of
the Conference on Empirical Methods in Natural Language Processing,
Edinburgh, UK, pp.1589–1599, 2011.
[6] X. Liu, A. Nourbakhsh, Q. Li, R. Fang, et al., “Real-time rumor
debunking on twitter”, Proceedings of the 24th ACM International
Conference on Information and Knowledge Management,
ACM, Melbourne, Australia, pp.1867–1870, 2015.
[7] Fan Yang, Xiaohui Yu, Yang Liu, et al., “Automatic detection
of rumor on Sina Weibo”, Proceedings of the ACM SIGKDD
Workshop on Mining Data Semantics, Beijing, China, Page 13,
2012.
[8] X. Zhou, J. Cao, Z. Jin, et al., “Realtime news certification
system on sina weibo”, Proceedings of the 24th International
Conference on World Wide Web, Florence, Italy, pp.983–988,
2015.
[9] S. Muthukrishnan, “Data streams: Algorithms and applications”.
Foundations and Trends in Theoretical Computer Sci-
ence, Vol.1, No.2, pp.117–236, 2005.
[10] K. Wu, S. Yang and K. Zhu, “False rumours detection on Sina
Weibo by propagation structures”, 2015 IEEE 31st International
Conference on Data Engineering, Seoul, South Korea,
pp.651–662, 2015.
[11] Shihan Wang and Takao Terano, “Detecting rumour patterns
in streaming social media”, IEEE International Conference on
Big Data, Guimi, Santa Clara, CA, USA, pp.2709–2715, 2015.
[12] C. Castillo, M. Mendoza and B. Poblete, “Information credibility
on Twitter”, The 20th International Conference on World
Wide Web, Hyderabad, India, pp.675–684, 2011.
[13] M. Mendoza, Poblete B and Castillo C, “Twitter under crisis:
Can we trust what we RT?”, The 1st Workshop on Social Media
Analytics, SOMA, pp.71–79, 2010.
[14] Shah D and Zaman T, “Rumors in a network: Who’s the
culprit?”, IEEE Transactions on Information Theory, Vol.57,
No.8, pp.5163–5181, 2011.
[15] Y. Matsubara, Y. Sakurai, B. Prakash, et al., “Rise and fall patterns
of information diffusion: Model and implications”, Pro-
ceedings of the International Conference on Knowledge Discovery
and Data Mining, Beijing, China, pp.6–14, 2012.
[16] S. Sun, H. Liu, J. He, et al., “Detecting event rumours on Sina
Weibo automatically”, In Asia-Pacific Web Conference, Sydney,
NSW, Australia, pp.120–131, 2013.
[17] Yumeng Qin, Dominik Wurzer, Victor Lavrenko, et al., “Counteracting
novelty decay in first story detection”, European Con-
ference on Information Retrieval, Lecture Notes in Computer
Science, Springer, Aberdeen, UK, Vol.10193, pp.555–560, 2017.
[18] Dominik Wurzer, Victor Lavrenko and Miles Osborne, “Tracking
unbounded Topic Streams”, Proceedings of the 53rd An-
nual Meeting of the Association for Computational Linguistics,
ACL, Beijing, China, Vol.1, pp.1765–1773, 2015.
[19] Dominik Wurzer, Victor Lavrenko and Miles Osborne,
“Twitter-scale new event detection via K-term hashing”, Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, Lisbon, Portugal, pp.2584–2589, 2015.
[20] S. Petrovic, M. Osborne and V. Lavrenko, “RT to win! predicting
message propagation in twitter”, The International Confer-
ence on Weblogs and Social Media, Barcelona, Spain, pp.586–
589, 2011.
[21] B. Suh, L. Hong, P. Pirolli, et al., “Want to be retweeted? Large
scale analytics on factors impacting retweet in twitter network”,
IEEE Second International Conference on Social Computing,
Minneapolis, MN, USA, pp.177–184, 2010.
[22] T.R. Zaman, R. Herbrich, van J. Gael, et al., “Predicting information
spreading in twitter”, Workshop on Computational
Social Science and the Wisdom of Crowds, Vol.104, No.45,
pp.17599–601, 2010.
[23] James Allan, Victor Lavrenko and Hubert Jin, “First story
detection in TDT is hard”, Proceedings of the Ninth International
Conference on Information and Knowledge Management,
ACM, McLean, Virginia, USA, pp.374–381, 2000.
[24] Jonathan G. Fiscus and G R. Doddington, “Topic detection
and tracking evaluation overview”, The Information Retrieval
Series, Vol.12, pp.17–31, 2002.
[25] James Allan, Topic Detection and Tracking: Event-Based Information
Organization, Kluwer Academic Publishers, Norwell,
2002.
QIN Yumeng was born in 1991.
She received the M.S. degree in computer
science from the University of Ed-
inburgh, UK. She is a Ph.D. candidate
of Wuhan University, China. Her re-
search interests include social media data
mining and information retrieval. (Email:
yumeng.qin@whu.edu.cn)
Dominik Wurzer was born in 1989.
He received the M.S. and Ph.D. degrees in
artificial intelligence from the University of
Edinburgh, UK. His research interests include
topic tracking, event detection and
real-time data mining on massive streams.
(Email: Dominik@Wurzer.com)
RESEARCH ARTICLE
Technology and democracy: a paradox wrapped in a
contradiction inside an irony
Stephan Lewandowsky1 and Peter Pomerantsev2
1School of Psychological Science, University of Bristol, Bristol BS8 1TU, UK and 2SNF Agora Institute,
Johns Hopkins University, Baltimore, Maryland
Corresponding author: Stephan Lewandowsky, email: stephan.lewandowsky@bristol.ac.uk
(Received 5 November 2021; accepted 8 November 2021)
Abstract
Democracy is in retreat around the globe. Many commentators have blamed the Internet for this
development, whereas others have celebrated the Internet as a tool for liberation, with each opinion
being buttressed by supporting evidence. We try to resolve this paradox by reviewing some of the
pressure points that arise between human cognition and the online information architecture, and
their fallout for the well-being of democracy. We focus on the role of the attention economy,
which has monetised dwell time on platforms, and the role of algorithms that satisfy users’ presumed
preferences. We further note the inherent asymmetry in power between platforms and
users that arises from these pressure points, and we conclude by sketching out the principles of
a new Internet with democratic credentials.
Keywords: Democracy; populism; misinformation; social media; Internet
The mission of Memory, Mind & Media is to document and explore the impact of media and
technology on human, social and cultural remembering and forgetting. In this article, we
set out the key challenges for the field, and hence the core issues and ideas for the journal,
through the lens of the unique cognitive pressure points that create tension between
the online information ecology and democratic discourse and governance.
Numerous indicators suggest that democracy is in retreat globally (Freedom House
2020; Lührmann and Lindberg 2020). Even countries that had been considered stable democracies,
such as the United States (US) and the United Kingdom (UK), have recently wit-
nessed events that are incompatible with democratic governance and the rule of law, such
as the armed assault on the U.S. Capitol in 2021 and the unlawful suspension of the British
parliament in 2019.
Although the symptoms and causes of democratic backsliding are complex and difficult
to disentangle, the Internet and social media are frequently blamed in this context. For
example, social media has been identified as a tool of autocrats (Deibert 2019).
Empirical support for this assertion arises from the finding that the more committed
autocratic regimes are to prevent an independent public sphere, the more likely they are
to introduce the Internet (Rød and Weidmann 2015). In Western democracies, recent evidence
suggests that social media can cause some anti-democratic political behaviours
© The Author(s), 2021. Published by Cambridge University Press. This is an Open Access article, distributed under the terms of
the Creative Commons Attribution licence (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted re-use,
distribution and reproduction, provided the original article is properly cited.
Memory, Mind & Media (2022), 1, e5, 1–9
doi:10.1017/mem.2021.7
#
#ranging from ethnic hate crimes to voting for populist parties (Bursztyn et al 2019; Müller
and Schwarz 2019; Allcott et al 2020; Schaub and Morisi 2020). Social media have also been
blamed for increasing political polarisation (Van Bavel et al 2021). Some scholars have
openly questioned whether democracy can survive the Internet (Persily 2017).
In the opposing corner, social media has been heralded as ‘liberation technology’
(Tucker et al 2017), owing to its role in the ‘Arab Spring’, the Iranian Green Wave
Movement of 2009, and other instances in which it mobilised the public against autocratic
regimes. Similarly, protest movements in the US, Spain, Turkey, and Ukraine rely on social
media platforms for the coordination of collective action and to transmit emotional and
motivational messages (Jost et al 2018). A recent field experiment in an ethnically highly
polarised society, Bosnia and Herzegovina, found that people who continued to use
Facebook reported greater outgroup regard than a group that voluntarily deactivated
Facebook for the same time period (Asimovic et al 2021).
The fundamental paradox
This is the fundamental paradox of the Internet and social media: They erode democracy
and they expand democracy. They are the tools of autocrats and they are the tools of activists.
They make people obey and they make them protest. They provide a voice to the
marginalised and they give reach to fanatics and extremists. And all of these conflicting
views are seemingly supported by analysis or empirical evidence, rendering resolution of
this paradox difficult.
We have proposed elsewhere that to understand this basic paradox, we must examine the
unique pressure points that arise when human cognition is let loose on the Internet
(Kozyreva et al 2020; Lewandowsky et al 2020; Lorenz-Spreen et al 2020). The interaction
between fundamental human cognitive attributes and the architecture of the information
ecology have created a perfect storm for democracy. Here, we focus on a subset of these pressure
points and highlight how they, in turn, also contain intrinsic ironies and paradoxes.
The attention economy
Our attention has been commodified (Wu 2017). When we use a ‘free’ product online, we are
the product. The more time we spend watching YouTube videos or checking our Facebook
newsfeed, the more advertising revenue is generated for the platforms. This commodification
of attention is an inescapable driver of online behaviour that has several contradictory
consequences. On the positive side, the fact that dwell time online has become revenuegenerating
currency has enabled the creation of a vast array of – seemingly – free services.
YouTube is free to use and provides nearly unlimited entertainment options. Google offers a
suite of tools beyond its search engine, from email to document creation, that support
countless endeavours free of charge. Facebook permits us to stay in touch with friends
and family, and we can use WhatsApp to make video calls with people all around the
world at no cost. The array of free services available online is impressive by any measure.
But those free services are not truly free – on the contrary, they incur considerable
costs that are often external to the interactions we intentionally engage in. One implication
of the conversion of dwell time into revenue-generating currency is that the plat-
forms will naturally try to present us with captivating information to retain our
attention. This commercial incentive structure is potentially problematic because people
are known to attend to news that is predominantly negative (Soroka et al 2019) or
awe-inspiring (Berger and Milkman 2012). People also preferentially share messages
couched in moral-emotional language (Brady et al 2017). It is unsurprising, therefore,
that ‘fake news’ and misinformation have become so prevalent online because false
2 Stephan Lewandowsky and Peter Pomerantsev
#
#content – which by definition is freed from factual constraints – can exploit this attentional
bias: misinformation on Facebook during the 2016 U.S. presidential campaign
was particularly likely to provoke voter outrage (Bakir and McStay 2018) and fake news
titles have been found to be substantially more negative in tone, and display more negative
emotions such as disgust and anger, than real news titles (Paschen 2019). The flood of
disinformation and online outrage is, therefore, arguably a price we pay for the ‘free’ services
provided by the platforms.
Although human attentional biases did not suddenly change just because the Internet
was invented – the adage that ‘when it bleeds, it leads’ is probably as old as journalism
itself – web technology has turbo-charged those biases in at least two ways. First, the
sheer quantity of information online has measurable adverse consequences for our ‘collective
mind’ and societal memories. Whereas in 2013 the most popular hashtags on
Twitter remained popular for 17.5 h, by 2016 a hashtag’s life in the limelight had dropped
to 11.9 h (Lorenz-Spreen et al 2019). A similar decline in our collective attention span was
observed for Google queries and movie ticket sales (Lorenz-Spreen et al 2019). It is unsurprising
that political accountability will become more difficult in societies with a shorter
attention span: if a leader’s original transgression is forgotten in a few hours, the public
appetite for accountability is unlikely to be lasting (Giroux and Bhattacharya 2016). Even
highly consequential events can seemingly disappear without leaving much of a trace:
When British Prime Minister Boris Johnson prorogued (ie, shut down) Parliament on 24
August 2019 to escape further scrutiny of his Brexit plans, public interest was initially
intense. After this prorogation was found to be unlawful by the Supreme Court on 24
September 2019, public interest in the issue, as measured by Google Trends, dissipated
by 93% within 5 days. Within 2 months, public interest in prorogation returned to the
near-zero level observed before the prorogation, when hardly anyone in Britain even
knew the term ‘prorogation’ existed.1 Johnson went on to win an election a few months
later by a landslide.
The problems arising from a shortened attention span are compounded by the fact that
information overload generally makes it harder for people to make good decisions about
what to look at, what to spend time on, what to believe, and what to share (Hills et al 2013;
Hills 2019). Choosing a paper to purchase at a newsstand requires a single decision. Our
Twitter feed or Facebook newsfeed confronts us with the need for a multitude of microdecisions
for every article or post. Although these repeated micro-decisions open the door
to greater diversity in our news diet, they also increase the probability that at least some
of our chosen sources fail to be trustworthy. Worse yet, information overload can also
contribute to polarisation and dysfunctional disagreement between well-meaning and
rational actors (Pothos et al 2021). That is, despite their good-faith efforts, overload
may prevent actors from forming compatible mental representations of complex problems.
Excessive complexity mandates a simplification of representations, and this, in
turn, necessarily introduces potential incompatibilities between actors that may result
in irresolvable disagreement (Pothos et al 2021).
The second turbocharger of human cognitive biases by online technologies relies on
the exact measurement of our responses to information. Facebook has access to our
every click while we are on the platform, and Facebook can use that information for continual
personalised refinement of our information diet through the platform’s algorithms.
The Jekyll and Hyde of the algorithm
Most of the information we consume online is shaped and curated by algorithms.
YouTube, by default, keeps playing videos we are presumed to like based on inferences
by its recommender system. Facebook’s newsfeed is curated by a sophisticated algorithm,
Memory, Mind & Media 3
#
#and Google’s search results are customised according to numerous parameters.
Algorithms are an essential tool to harness the abundance of information on the web:
Googling ‘Georgia’ should return different results in Atlanta than in Tbilisi, and without
such intelligent filtering, useful information would most likely remain inaccessible.
Algorithms can also help us satisfy our preferences, for example, when recommender systems
help us find movies, books, or restaurants that we are likely to enjoy (Ricci et al
2015). It is unsurprising, therefore, that the public is mainly appreciative of algorithms
and customisation in those contexts (Kozyreva et al 2021).
There are, however, several darker sides to algorithms. The first problem is that algorithms
ultimately serve the interests of the platforms rather than the users. An ironic con-
sequence of this is that in the relentless pursuit of increasing dwell time, algorithms may
eagerly satisfy our presumed momentary preferences even if that reduces our long-term
well-being. In the same way that strategically placed junk food in the supermarket can satisfy
our cravings while also propelling an obesity epidemic, algorithms may satisfy our
momentary desire for emotional engagement while contributing to the formation of sealed
anti-democratic communities (Kaiser and Rauchfleisch 2020). Unconstrained preference
satisfaction may ironically create fractionated and polarised societies (Pariser 2011).
The second problem with algorithms is that their design and operation are proprietary
and not readily subject to public scrutiny. Most algorithms operate as ‘black boxes’ where
neither individual users nor society, in general, know why search results or social media
feeds are curated in a particular way (Pasquale 2015). At present, knowledge about the
algorithms can only be obtained by ‘reverse engineering’ (Diakopoulos 2015), that is, by
seeking to infer an algorithm’s design based upon its observable behaviour.
Reverse engineering can range from the relatively simple (eg, examining which words
are excluded from auto-correct on the iPhone; Keller 2013) to the highly complex (eg, an
analysis of how political ads are delivered on Facebook; Ali et al 2019). Reverse engineering
has uncovered several problematic aspects of algorithms, such as discriminatory
advertising practices and stereotypical representations of Black Americans in Google
Search (Sweeney 2013; Noble 2018) and in the autocomplete suggestions that Google provides
when entering search terms (Baker and Potts 2013). At the time of this writing, a
Facebook whistle-blower revealed further information about how content is being highlighted
on the platform. It transpired that any content that made people angry –
which was disproportionately likely to include misinformation, toxicity, and low-quality
news – was given particular prominence in people’s newsfeed. Facebook thus ‘systematically
amped up some of the worst of its platform, making it more prominent in users’ feeds
and spreading it to a much wider audience’ (Merrill and Oremus 2021).
The opacity of algorithms allows platforms to drench users in information that may be
detrimental to democratic health. Even ignoring the specifics of content, algorithmic opacity
also contributes to a general imbalance of power between platforms and users that
can only be unhealthy in a democracy.
The asymmetry of power
The platforms know much about their users – and even about people who are not on their
platforms (Garcia 2017) – and deploy that knowledge to shape our information diets. By
contrast, citizens know little about what data the platforms hold and how these data are
used (Lorenz-Spreen et al 2020). For example, Facebook ‘likes’ can be used to infer our personality
through machine learning with considerable accuracy (Youyou et al 2015).
Knowledge of just a few likes raises machine-learning performance above that of work colleagues,
and with knowledge of 300 likes, the performance of the machine exceeds that of
one’s spouse (Youyou et al 2015). In stark contrast to the power of machine learning, a
4 Stephan Lewandowsky and Peter Pomerantsev
#
#substantial share of people does not even know that their Facebook newsfeed is curated
based on personal data (Eslami et al 2015; Rader and Gray 2015; Powers 2017), with estimates
of this lack of awareness ranging from 27 to 62.5 per cent.
Asymmetry in knowledge translates into an asymmetry of power: To keep others under
surveillance while avoiding equal scrutiny oneself is the most important form of authoritarian
political power (Balkin 2008; Zuboff 2019). Similarly, to know others while revealing
little about oneself is the most important form of commercial power in an attention economy.
When Facebook recently shut down the accounts of researchers who were studying
how misinformation spreads and how users are targeted on the platform (Edelson and
McCoy 2021), it did not do so to preserve users’ privacy as it claimed. That claim was
quickly and thoroughly rejected by the Federal Trade Commission. Facebook shut down
the researchers to preserve its asymmetrical power advantage by preventing an examination
of how it operates. It is this power asymmetry that renders the freedom and choice
offered by the Internet largely illusory.
The illusion of freedom and choice
Everyone gets a voice on the Internet. On the positive side of the ledger, there is evidence
that access to the Internet leads to enhanced transparency and reduction of corruption. In
a cross-national analysis of 157 countries, Starke et al (2016) showed that Internet access
was associated with a significant reduction in official corruption. On the more negative
side of the ledger, a single tweet can trigger a cascade of adverse events. The ‘pizzagate’
affair of 2016 was triggered by a baseless accusation that the Democratic party was operating
a paedophilia ring out of the basement of a pizza parlour in Washington, D.C. This
conspiracy theory was eventually picked up by mainstream media, and ultimately an
armed individual entered the pizza parlour and fired shots inside in search of a (nonexistent)
basement (Fisher et al 2016).
The ambivalent consequences of unfettered access to the Internet are amplified by the
opportunities offered for manipulation through targeted advertising. All advertising and
political speech seek to persuade. Manipulation differs from persuasion by furtively exploiting
a target’s weaknesses and vulnerabilities to steer their behaviour in a desired direction
(Susser et al 2019). The fact that Facebook ‘likes’ permit inferences about a user’s personality
(Youyouet al 2015), combinedwith the fact that advertisers can select target audiences based
on those likes (coded as users’ interests), offers an opportunity for targetedmanipulation on
a global scale and without any transparency. Research suggests that single individuals or
households can be targeted with messages using Facebook’s ad delivery services
(Faizullabhoy and Korolova 2018). Although the effectiveness of such ‘microtargeting’ of
messages is subject to debate (eg, Matz et al 2017 vs. Eckles et al 2018), there is no question
that targetingofpoliticalmessages at individuals (or small numbersof individuals) facilitates
the dissemination of disinformation because political opponents cannot knowwhat is being
said and hence cannot rebut false information (Heawood 2018). Similarly, microtargeting
allows politicians to make multiple incompatible promises to different audiences without
anyone being able to track and point out those incompatibilities (Heawood 2018). A recent
pertinent example arose during the German parliamentary election in September 2021.
The Liberal Democratic Party (FDP) was found to target Facebook users with ‘green’ interests
with amessage that identified the party with ‘more climate protection’ through a regulatory
upper limit on CO2 emissions. At the same time, the FDP targeted frequent travellers on
Facebook with an ad that promised ‘no state intervention or restrictions of freedom or prohibitions’
to address climate change.2
Unsurprisingly, the public overwhelmingly rejects this type of manipulative targeting
(Kozyreva et al 2021).3
Memory, Mind & Media 5
#
#Everyone may get a voice on the Internet. But everyone is also exposed to a cacophony
of voices whose origin may be obscured and that may seek to manipulate rather than
inform. The power to design and deliver manipulative messages that form our society’s
collective memory rests with advertisers and platforms rather than citizens. For now at
least, the freedom and choice offered by the Internet, therefore, remains largely illusory.
Building a better Internet
Our preceding analysis illustrates the fundamental paradox of the online media environment:
On the one hand, there is more information than ever before, but we know less than
ever about how that information is produced, targeted, organised, and distributed.
Citizens do not know why algorithms show them one thing and not another, or which
of their own data are being used to target them and why. Citizens have little way of knowing
about the vast social engineering experiments tech companies play with as they fiddle
with their algorithms. Citizens do not even know if their basic rights are being infringed
by manipulative algorithms and advertisers. We believe that democratic societies would
never have consented to any of those consequences of the Internet if they had been
known ahead of time or if the Internet had been designed with those attributes in
mind. It is only because the Internet evolved, one technological innovation and one
tweak to an algorithm at a time, that democracies are only now realising what they are
confronting.
What, then, should the online experience be like for a person in a democracy? How can
be design and build a better Internet? We have both been involved in developing specific
recommendations for a better Internet (eg, Kozyreva et al 2020; Lewandowsky et al 2020;
Lorenz-Spreen et al 2020; Applebaum and Pomerantsev 2021). Here, we focus on one
aspect only, namely the power asymmetry between platforms and users and how it
might be redressed.
In an Internet with democratic credentials, users would be able to understand which of
their own data have been used to target them and why. Users would know why algorithms
show them one thing and not another. During elections, people would immediately
understand how different campaigns target different people with different messages,
who is behind campaigns, and how much they spend.
Online anonymity is a basic right. People should be allowed to ‘wear a mask’ online for
reasons of safety and many others. But the receiver of information should also have the
right to understand whether they are being targeted by a real person (whether anonymous
or not), or by a political campaign, a corporation, or a state that is pretending to be a
real person. ‘Troll farms’, bot nets, and other forms of mass coordinated inauthentic activity
should be clearly identified as such.
An empowered online citizen would also have far greater control over their own data
and would be able to regulate how others use it. There may be instances where, for
example, one might be comfortable with sharing one’s data with a national health service.
But there should be strict guardrails that do not allow user data to be used further by
data brokers.
And as individuals should have more oversight and control over the information environment
all around them, so should the public have greater oversight and control over
tech companies in general. The public need to be able to understand what social engineering
experiments the companies tinker with, what their impacts are, and how the tech
companies track the consequences of these experiments.
Likewise, algorithmic transparency is essential. This does not mean that companies
have to reveal their proprietary source code. They do, however, need to explain the purpose
of adjustments they make to their algorithms, and the changes these bring about. If
6 Stephan Lewandowsky and Peter Pomerantsev
#
#algorithms infringe on people’s rights, such as in cases where algorithms produce advertising
that disadvantages minorities, the public need to have oversight over what the
companies are doing to rectify these discriminatory practices. Such algorithmic transparency
needs to be backed up with regulatory teeth: regulators should have the right to
spot-check how companies are continually analysing and mitigating negative effects of
their own design decisions.
But regulation needs to go beyond just mitigating the bad and setting standards. It needs
to encourage ‘the good’ too. We must design regulations that encourage the development of
‘civic tech’; that is, technology that is meant to benefit individuals and strengthen democratic
processes. Such technology would be created in the public interest, and not driven
by short-term profit motives to extract people’s personal data and then sell it on.
As Ethan Zuckerman of the University of Massachusetts argues4, we are in a similar
place in the development of the Internet as we were with radio at the start of the 20th century.
Back in the 1920s, in the UK, Lord Reith fought for the existence of public interest
broadcasting to balance the polarising impact of press barons and the rising power of radioenhanced
dictatorships. The result was the creation of the BBC. What would be the online
equivalent of that today? We do not know. This illustrates the magnitude of the task ahead.
It may be daunting, but that should concern us less than the conflict between current technologies
and democracy that is driven, in part, by known limitations of human attention,
memory, and cognition. The mission of Memory, Mind & Media is precisely aimed at those
limitations and conflicts, and the journal is, therefore, poised to make a contribution to
what we consider the defining political battle of the 21st century – the battle between
technological hegemony and survival of democracy.
Funding. The first author was supported by funding from the Humboldt Foundation in Germany through a
research award, and by an ERC Advanced Grant (PRODEMINFO). The preparation of this paper was also facilitated
by a grant from the Volkswagen Foundation for the project ‘Reclaiming individual autonomy and democratic discourse
online’.
Conflict of Interest. The authors declare no competing interests.
Notes
1 https://trends.google.com/trends/explore/TIMESERIES/1635669000?hl=en-GB\&tz=0\&date=today+5-y\&geo=GB
\&q=prorogation\&sni=3.
2 https://targetleaks.de/.
3 Recent transparency measures (eg, Facebook’s ‘ad library’) are insufficient to analyze parties’ expenditure on
microtargeting and what content has been shown (Dommett and Power 2019). The ad library is also missing more
than 100,000 political ads (Edelson and McCoy 2021). This difficulty is likely to persist because ads on Facebook
are delivered by a continually evolving algorithm, known as AdTech, that auctions off ads on a second-to-second
basis based on live analysis of user data (Ali et al 2019).
4 https://www.umass.edu/spp/news/case-digital-public-infrastructure.
References
Ali M, Sapiezynski P, Korolova A, Mislove A and Rieke A (2019) Ad delivery algorithms: the hidden arbiters of political
messaging. Tech. Rep. Available at https://arxiv.org/pdf/1912.04255.pdf, accessed 19 April 2020.
Allcott H, Braghieri L, Eichmeyer S and Gentzkow M (2020) The welfare effects of social media. American
Economic Review 110, 629–676. doi:10.1257/aer.20190658
Applebaum A and Pomerantsev P (2021) How to put out democracy’s dumpster fire. Available at https://www.theatlantic.com/magazine/archive/2021/04/the-internet-doesnt-have-to-be-awful/618079/,
accessed 4 August 2021.
Asimovic N, Nagler J, Bonneau R and Tucker JA (2021) Testing the effects of Facebook usage in an ethnically
polarized setting. Proceedings of the National Academy of Sciences 118, e2022819118. doi:10.1073/pnas.2022819118
Baker P and Potts A (2013) ‘Why do white people have thin lips?’ Google and the perpetuation of stereotypes via
auto-complete search forms. Critical Discourse Studies 10, 187–204. doi:10.1080/17405904.2012.744320
Memory, Mind & Media 7
#
#Bakir V and McStay A (2018) Fake news and the economy of emotions. Digital Journalism 6, 154–175. doi:10.1080/
21670811.2017.1345645
Balkin JM (2008) The constitution in the national surveillance state. Minnesota Law Review 93, 1–25.
Berger J and Milkman KL (2012) What makes online content viral? Journal of Marketing Research 49, 192–205.
Brady WJ, Wills JA, Jost JT, Tucker JA and Van Bavel JJ (2017) Emotion shapes the diffusion of moralized content
in social networks. Proceedings of the National Academy of Sciences 114, 7313–7318. doi:10.1073/
pnas.1618923114
Bursztyn L, Egorov G, Enikolopov R and Petrova M (2019) Social media and xenophobia: evidence from Russia. Tech.
Rep. National Bureau of Economic Research.
Deibert RJ (2019) Three painful truths about social media. Journal of Democracy 30, 25–39. doi:10.1353/
jod.2019.0002
Diakopoulos N (2015) Algorithmic accountability. Digital Journalism 3, 398–415. doi:10.1080/21670811.2014.976411
Dommett K and Power S (2019) The political economy of Facebook advertising: election spending, regulation
and targeting online. The Political Quarterly. doi:10.1111/1467-923x.12687
Eckles D, Gordon BR and Johnson GA (2018) Field studies of psychologically targeted ads face threats to internal
validity. Proceedings of the National Academy of Sciences 115, E5254–E5255. doi:10.1073/pnas.1805363115
Edelson L and McCoy D (2021) We research misinformation on Facebook. It just disabled our accounts. Available at
https://www.nytimes.com/2021/08/10/opinion/facebook-misinformation.html, accessed 4 August 2021.
Eslami M, Rickman A, Vaccaro K, Aleyasen A, Vuong A, Karahalios K, Hamilton K and Sandvig C (2015) "I
always assumed that I wasn’t really that close to [her]” reasoning about invisible algorithms in news feeds. In
Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, 153–162.
Faizullabhoy I and Korolova A (2018) Facebook’s advertising platform: new attack vectors and the need for interventions.
CoRR, abs/1803.10099. Available at http://arxiv.org/abs/1803.10099
Fisher M, Cox JW and Hermann P (2016) Pizzagate: from rumor, to hashtag, to gunfire in DC. Available at https://
www.washingtonpost.com/local/pizzagate-from-rumor-to-hashtag-to-gunfire-in-dc/2016/12/06/4c7def50-bb
d4-11e6-94ac-3d324840106c_story.html, accessed 13 April 2020.
Freedom House. (2020). Freedom in the world 2020. A leaderless struggle for democracy. Tech. Rep.
Garcia D (2017) Leaking privacy and shadow profiles in online social networks. Science Advances 3, e1701172.
doi:10.1126/sciadv.1701172
Giroux HA and Bhattacharya D (2016) Anti-politics and the scourge of authoritarianism. Social Identities.
doi:10.1080/13504630.2016.1219145
Heawood J (2018) Pseudo-public political speech: Democratic implications of the Cambridge Analytica scandal.
Information Polity 23, 429–434. doi:10.3233/IP-180009
Hills TT (2019) The dark side of information proliferation. Perspectives on Psychological Science 14, 323–330.
doi:10.1177/1745691618803647
Hills TT, Noguchi T and Gibbert M (2013) Information overload or search-amplified risk? Set size and order effects
on decisions from experience. Psychonomic Bulletin & Review 20, 1023–1031. doi:10.3758/s13423-013-0422-3
Jost JT, Barberá P, Bonneau R, Langer M, Metzger M, Nagler J, Sterling J and Tucker JA (2018) How social
media facilitates political protest: information, motivation, and social networks. Political Psychology 39, 85–118.
doi:10.1111/pops.12478
Kaiser J and Rauchfleisch A (2020) Birds of a feather get recommended together: algorithmic homophily in
YouTube’s channel recommendations in the United States and Germany. Social Media+Society 6. doi:10.1177/
2056305120969914
Keller M (2013) The Apple ‘kill list’: what your iPhone doesn’t want you to type. Available at https://www.thedailybeast.com/the-apple-kill-list-what-your-iphone-doesnt-want-you-to-type,
accessed 20 April 2020.
Kozyreva A, Lewandowsky S and Hertwig R (2020) Citizens versus the Internet: confronting digital challenges
with cognitive tools. Psychological Science in the Public Interest 21, 103–156. doi:10.1177/1529100620946707
Kozyreva A, Lorenz-Spreen P, Hertwig R, Lewandowsky S and Herzog SM (2021) Public attitudes towards algorithmic
personalization and use of personal data online: evidence from Germany, Great Britain, and the United
States. Humanities and Social Sciences Communications 8. doi:10.1057/s41599-021-00787-w
Lewandowsky S, Smillie L, Garcia D, Hertwig R, Weatherall J, Egidy S, Robertson RE, O’Connor C, Kozyreva
A, Lorenz-Spreen P, Blaschke Y and Leiser M (2020) Technology and democracy: understanding the influence of
online technologies on political behaviour and decision making. Tech. Rep. doi:10.2760/709177
Lorenz-Spreen P, Mønsted BM, Hövel P and Lehmann S (2019) Accelerating dynamics of collective attention.
Nature Communications 10, 1759. doi:10.1038/s41467-019-09311-w
Lorenz-Spreen P, Lewandowsky S, Sunstein CR and Hertwig R (2020) How behavioural sciences can promote
truth and, autonomy and democratic discourse online. Nature Human Behaviour 4, 1102–1109. doi:10.1038/
s41562-020-0889-7
8 Stephan Lewandowsky and Peter Pomerantsev
#
#Lührmann A and Lindberg SI (2020) Autocratization surges – resistance grows. Democracy Report 2020 (Tech. Rep.).
Gothenburg: V-Dem Institute.
Matz SC, Kosinski M, Nave G and Stillwell DJ (2017) Psychological targeting as an effective approach to digital
mass persuasion. Proceedings of the National Academy of Sciences 48, 12714–12719. doi:10.1073/pnas.1710966114
Merrill JB and Oremus W (2021) Five points for anger, one for a ‘like’: how Facebook’s formula fostered rage and misinformation.
Available at https://www.washingtonpost.com/technology/2021/10/26/facebook-angry-emoji-
algorithm/, accessed 31 October 2021.
Müller K and Schwarz C (2019) Fanning the flames of hate: social media and hate crime. SSRN Electronic Journal.
doi:10.2139/ssrn.3082972
Noble SU (2018) Algorithms of Oppression: How Search Engines Reinforce Racism. New York, NY: New York University
Press.
Pariser E (2011) The Filter Bubble: What the Internet Is Hiding from You. New York: Penguin Press.
Paschen J (2019) Investigating the emotional appeal of fake news using artificial intelligence and human contributions.
Journal of Product & Brand Management 29, 223–233. doi:10.1108/jpbm-12-2018-2179
Pasquale F (2015) The Black Box Society. Cambridge, MA: Harvard University Press.
Persily N (2017) Can democracy survive the Internet? Journal of Democracy 28, 63–76.
Pothos EM, Lewandowsky S, Basieva I, Barque-Duran A, Tapper K and Khrennikov A (2021) Information overload
for (bounded) rational agents. Proceedings of the Royal Society B: Biological Sciences 288, 20202957.
doi:10.1098/rspb.2020.2957
Powers E (2017) My news feed is filtered? Digital Journalism 5, 1315–1335. doi:10.1080/21670811.2017.1286943
Rader E and Gray R (2015) Understanding user beliefs about algorithmic curation in the Facebook news feed. In
Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. doi:10.1145/
2702123.2702174
Ricci F, Rokach L and Shapira B (2015) Recommender Systems: Introduction and Challenges. New York, NY: Springer.
Rød EG and Weidmann NB (2015) Empowering activists or autocrats? The Internet in authoritarian regimes.
Journal of Peace Research 52(3), 338–351. doi:10.1177/0022343314555782.
Schaub M and Morisi D (2020) Voter mobilisation in the echo chamber: broadband Internet and the rise of
populism in Europe. European Journal of Political Research. doi:10.1111/1475-6765.12373
Soroka S, Fournier P and Nir L (2019) Cross-national evidence of a negativity bias in psychophysiological reactions
to news. Proceedings of the National Academy of Sciences 116, 18888–18892. doi:10.1073/pnas.1908369116
Starke C, Naab T and Scherer H (2016) Free to expose corruption: the impact of media freedom, Internet access
and governmental online service delivery on corruption. International Journal of Communication 10, 4702–4722.
Susser D, Roessler B and Nissenbaum H (2019) Online manipulation: hidden influences in a digital world.
Georgetown Law Technology Review 4, 1–45.
Sweeney L (2013) Discrimination in online ad delivery. Queue 11, 1–19. doi:10.1145/2460276.2460278
Tucker JA, Theocharis Y, Roberts ME and Barberá P (2017) From liberation to turmoil: social media and democracy.
Journal of Democracy 28, 46–59. doi:10.1353/jod.2017.0064
Van Bavel JJ, Rathje S, Harris E, Robertson C and Sternisko A (2021) How social media shapes polarization.
Trends in Cognitive Sciences. doi:10.1016/j.tics.2021.07.013
Wu T (2017) The Attention Merchants. London, UK: Atlantic Books.
Youyou W, Kosinski M and Stillwell D (2015) Computer-based personality judgments are more accurate than
those made by humans. Proceedings of the National Academy of Sciences 112, 1036–1040. doi:10.1073/
pnas.1418680112
Zuboff S (2019) Surveillance capitalism and the challenge of collective action. New Labor Forum 28, 10–29.
doi:10.1177/1095796018819461
Stephan Lewandowsky is a cognitive scientist at the University of Bristol. His research focuses on people’s
responses to misinformation and the potential tension between online technology and democracy.
Peter Pomerantsev is a senior fellow at the SNF Agora Institute at Johns Hopkins University where he co-directs
the Arena Initiative, a research project dedicated to overcoming the challenges of digital era disinformation and
polarisation.
Cite this article: Lewandowsky S, Pomerantsev P (2022). Technology and democracy: a paradox wrapped in a
contradiction inside an irony. Memory, Mind & Media 1, e5, 1–9. https://doi.org/10.1017/mem.2021.7
Memory, Mind & Media 9
Research Article
Spread of Misinformation in Social Networks: Analysis Based on
Weibo Tweets
Han Luo ,1 Meng Cai ,1 and Ying Cui 2
1School of Humanities and Social Sciences, Xi’an Jiaotong University, Xi’an 710049, China
2School of Mechano-Electronic Engineering, Xidian University, Xi’an 710071, China
Correspondence should be addressed to Meng Cai; mengcai@mail.xjtu.edu.cn
Received 28 August 2021; Revised 19 November 2021; Accepted 26 November 2021; Published 16 December 2021
Academic Editor: Chenquan Gan
Copyright © 2021Han Luo et al.+is is an open access article distributed under the Creative CommonsAttribution License, which
permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.
Social networks are filled with a large amount of misinformation, which often misleads the public to make wrong decisions,
stimulates negative public emotions, and poses serious threats to public safety and social order. +e spread of misinformation in
social networks has also become a widespread concern among scholars. In the study, we took the misinformation spread on social
media as the research object and compared it with true information to better understand the characteristics of the spread of
misinformation in social networks.+is study adopts a deep learningmethod to perform content analysis and emotion analysis on
misinformation dataset and true information dataset and adopts an analytic network process to analyze the differences between
misinformation and true information in terms of network diffusion characteristics. +e research findings reveal that the spread of
misinformation on social media is influenced by content features and different emotions and consequently produces different
changes. +e related research findings enrich the existing research and make a certain contribution to the governance of
misinformation and the maintenance of network order.
1. Introduction
Misinformation is an objective social phenomenon that
appears in the social operation environment. It usually refers
to the information that is widely circulated intentionally or
unintentionally without a factual basis and confirmation or
clarification [1]. It has been a concern not only in the social
sciences such as sociology and journalism [2] but also in
computer science and other research fields [3]. With the
development of Internet technology and social media
platforms, the misinformation spread by word-of-mouth is
rapidly spread through social media platforms and has the
characteristics of fission diffusion, fast propagation speed, a
wide range of influence, and deep impact. A large amount of
false information and the spread of rumors and misleading
information on social media platforms not only cause public
concern and pose a threat to the public’s physical and
psychological health but also bring serious challenges to the
governance and stability of social order [4].
+e destructive nature of misinformation has also
made the concept of “information epidemic” known to the
public. “Information epidemic” refers to a series of
physical and psychological reactions formed by the public
when they are faced with misinformation because it is
difficult to identify the veracity of the information, and the
spread of misinformation infiltrates into everyone’s life
[5]. For example, during the COVID-19 outbreak, the
World Health Organization considered fighting against
the “information epidemic” as an important part of its
work [6]. With the influence of social media, the “information
epidemic” expanded in scope and magnified the
threat posed by misinformation. For example, when faced
with misinformation, the uncertainty of the future and the
lack of access to information will increase the psychological
pressure of the public, causing anxiety and panic
among the public [7]. At this time, under the influence of
rumors and misleading information, the public is very
likely to be trapped by the group, amplifying mass panic,
triggering collective social crisis, and even leading to
various social tragedies [8]. It has been shown that the
harm caused by misinformation spread on social media is
more serious due to the characteristics of social media such
Hindawi
Security and Communication Networks
Volume 2021, Article ID 7999760, 23 pages
https://doi.org/10.1155/2021/7999760
#
#as fast propagation speed, a wide range of influence, and
deep impact. On this basis, it is urgent to understand the
propagation process of misinformation on social media
and to govern misinformation [9].
In addition, with the convenience of communication
and the timeliness of information dissemination, social
media often becomes an important communication
channel for two-way information exchange. On the one
hand, considering the influence of “information cocoon,”
weak ties can provide us with more diverse information,
based on which social media becomes a better choice for
people to communicate. With the help of social media, we
can get in touch with people from different regions and
industries more easily, and communicate more frequently
with people who different from us. +erefore, the public
can also learn about relevant situations and specific information
about different events in different regions from
social media [10]. On the other hand, influenced by the
interaction and fusion of massive true information and
misinformation on social media platforms, the public is
often prone to emotional fluctuations and tends to publish
their views and emotions on social media platforms and
receive different responses depending on the type, progress,
and content of events [11]. At the same time, social
networks formed on social media will spread public
opinions or emotions and form new communication
networks [12].
In recent years, the spread of misinformation on social
media platforms has caused public concern, not only because
misinformation can easily confuse people, causing
them to make wrong decisions, resulting in economic and
material losses, but also because misinformation can have an
impact on health, medical, and other fields, spreading wrong
treatments which even further damages the public’s physical
and mental health. At the same time, unconstrained misinformation
will lead to social disorder and the prevalence of
negative emotions, ultimately causing a huge impact on
society. +erefore, it is especially important to understand
the spread process and diffusion characteristics of misinformation
on social media platforms [13]. Considering this,
the study is based on large-scale social media datasets, takes
true information andmisinformation as the research objects,
and conducts correlation analysis on the content and
emotion of true information and misinformation for their
dissemination. In addition, we use the social network
analysis method to further compare the network structure of
true information and misinformation in the process of
dissemination, expecting to reveal the evolution law of false
information to realize the public opinion governance of
misinformation and reduce the negative impact brought by
misinformation.
+e results for the rest of this article are as follows. In the
second part, we introduce relevant studies. In the third part,
we present the methodology used in the study. In the fourth
part, we show the analysis results and discussions, mainly
involving the analysis of the content and emotions spread by
true information andmisinformation on social media. In the
fifth part, we present the conclusion of the research and
further put forward future ideas for the research.
2. Related Research
2.1. Misinformation in Social Network. Unverified or unclarified
messages are very common on social media, and
studies have attempted to conceptualize unverified messages
from different perspectives. Common names include
“misinformation,” “disinformation,” “fake news,” and “rumor”
[14]. Among them, misinformation, disinformation,
and fake news all emphasize the false nature of information
and describe the object of information that has been falsified.
+e difference lies in the fact that misinformation often
appears in a random form with unknown intention and
motivation, and is often used by researchers to describe false
information in a broad sense [15]. Disinformation is usually
the deliberate tampering of correct information to obtain
benefits or advantages and then spread [16]. Similar to
disinformation, fake news is used to disseminate false information
and stories in the guise of reliable sources for
economic or political gain [17]. Rumor is quite different
from the previous three concepts. Although a rumor is also
unconfirmed information, it need not always be false information,
as it can also be correct information in some
cases, and the dissemination motivations and intentions of
rumor are often unknown [18].
+e spread of misinformation on social media has always
been an important research topic. But before we can understand
why misinformation can spread on social media,
we need to have a clearer understanding of the misinformation.
One of the reasons why misinformation is con-
cerned as important by researchers lies in the misleading
nature of misinformation. Misinformation often misleads
the public to make decisions, causes them to form corresponding
actions, and generates emotional and psycho-
logical fluctuations [19]. At this time, the public forms an
adaptive response under the stimulus and influence of
misinformation, and tends to interact with the outside
world, and then magnifies the scope and degree of the influence
of misinformation [20]. On the one hand, misin-
formation, as an adaptive form of stimulus-response, has a
warning function, indicating that the public’s emotions are
dictated by external forces in a tense situation. On the other
hand, misinformation also reflects the psychological state
behind public emotions in the social environment. For
example, when a social crisis occurs, the public is more
susceptible to the influence of other public emotions under
the misdirection of misinformation, forming a large-scale
emotion cluster phenomenon, which will then impact the
social order and easily cause a negative effect on society [21].
In addition, some studies have analyzed factors that
influence the spread of misinformation on social media.
Studies have pointed out that true information and misinformation
are often mixed and difficult to identify, and
misperception of misinformation as true information is the
main reason for the public to share and spread misinformation
on social media [22]. Social media users tend to
support the spread of unverified information on social media
but generally do not spread information that has been
proven to be false. +e problem is that social media users
often lack the ability to recognize misinformation before
2 Security and Communication Networks
#
#sharing it [23]. Studies have been conducted to explain the
public’s deficiency in misinformation recognition on this
basis, with related studies mainly focusing on political,
psychological, and media literacy perspectives. For example,
from the political perspective, it has been argued that the
public’s political orientation affects the human brain’s
processing of information, and an identity-based model of
political orientation has been proposed to explain how
political orientation causes the public to form information
bias and thus choose to trust false information [24]. Other
studies have combined web browsing data and online survey
data to analyze how people share fake news in political
elections. +e study found that the public’s behavioral
preferences have a significant impact on the trust in misinformation
and that the public is more willing to believe
stories about politicians they support, even if the stories are
false and full of implausible elements [25]. In the field of
psychology, some studies measured the accuracy of respondents’
perception of misinformation by conducting the
Cognitive Reflection Test (CRT) for the respondents, and it
was found that the better the respondents performed in the
CRT test, the more likely they were to detect misinformation
and the more accurate they were. +e findings of the study
prove the importance of psychological factors. When the
public is more inclined to use analytical thinking rather than
lazy thinking when identifying information, they are more
likely to recognize the difference between true and false
information [26]. Studies have also paired Twitter data with
public accounts during elections to analyze the process of
spreading fake news, and they found that psychologically
conservative people are more likely to be exposed to fake
news and less able to spot it. Related studies have confirmed
the influence of psychological factors on the identification of
misinformation [27]. In the field of media literacy, some
scholars believe that the public’s general trust in the misinformation
spread on social media mainly stems from the
lack of digital media literacy. +e study has used survey data
from the US and India during elections to assess the effectiveness
of media literacy campaign interventions, and
the findings confirm that digital media literacy plays an
important role in the identification of misinformation [28].
Some scholars have conducted empirical studies to evaluate
whether individuals with higher media literacy are more
capable of identifying fake news and compared different
media literacy aspects such as information literacy and news
literacy. +e results verified the effectiveness of information
literacy, which can effectively enhance the public’s ability to
identify misinformation [29]. +ese studies also provide
references for us to understand the spread of misinformation
on social media from multiple perspectives, and help us
further understand the reasons why Internet users share and
spread misinformation on social media.
2.2. Misinformation and Content Analysis. Social media has
become an important mode for the public to communicate
and obtain relevant information by virtue of its unlimited
access. However, due to the lack of online supervision and
user anonymity, the line between true information and
misinformation is not always easy to distinguish, which
makes the public often face the risk of being misled when
accessing relevant information. And, the unique echo
chamber design of social media platforms allows people with
a common need for information to gather together. Although
this facilitates communication among people in the
same situation and increases the possibility that the public
will have access to the true information they need, it also
amplifies the negative impact of misinformation [30].
Moreover, the interaction and fusion of true information
and misinformation on social media platforms have also
attracted many scholars to study the dissemination of
misinformation and conduct quantitative research on it. For
example, some scholars extracted Ebola-related tweets from
social media and coded them by using professionals with
knowledge background to evaluate the proportion of the
extracted tweets that contained specific content. It was found
that ten percent of the relevant tweets contained misinformation
[31]. Another study conducted a content analysis
on the misinformation about COVID-19 and found that
only thirty-eight percent of the misinformation about
COVID-19 on social media platforms was completely fabricated,
and most of the misinformation was formed by
distorting and falsifying the true information [32]. +e
relevant findings confirm the current situation of integration
and fusion of misinformation and true information on social
media, which can help us understand the evolution and
diffusion pattern of misinformation on social media.
In addition to using content analysis to distinguish true
information frommisinformation, relevant studies have also
focused on the characteristics of the spread of misinformation
on social media, especially how misinformation
spreads relative to true information [33]. For example, by
examining the differential distribution of large-scale true
information and misinformation on social media platforms,
a study found that misinformation had the characteristics of
faster propagation speed, longer propagation path, and
wider propagation range than true information. Moreover,
the information content of misinformation was often more
novel than true information, so it was more likely to be
shared by the public on social media. It was also found that
the content of misinformation was more likely to stimulate
emotions such as fear, surprise, and disgust than the
emotions such as anticipation, sadness, and joy evoked by
true information [34]. Some studies also pointed out that the
misinformation can be spread more quickly when the
content of misinformation was related to major public crisis
events. For example, during COVID-19 in 2019, the amount
of misinformation exploded and spread worldwide in a very
short spread. According to relevant agencies, during the
spread of the epidemic, about 46,000 instances of epidemicrelated
misinformation were spread on social media every
day and spread rapidly with retweets from Internet users
[35]. In addition to these studies, some scholars have also
tried to understand the sources and evolutionary patterns of
misinformation by means of content analysis. Based on the
analysis of massive amounts of Twitter data, it was found
that most of the misinformation on social media was generated
by general accounts, but the content often contained
Security and Communication Networks 3
#
#links to websites that lacked credible sources. It was also
found that the spread of misinformation on social media was
a dynamic process in which the content undergoes dynamic
changes, i.e., it was constantly modified by network users to
spread at the next step in the process of spreading [36].
+e content analysis method based on misinformation is
important. On the one hand, misinformation content
conveys bias and misunderstanding which may affect the
public’s trust in experts, institutions, and governments,
leading to the spread of unnecessary fear and suspicion. For
example, studies have pointed out that the majority of the
public relies on online comments to evaluate and judge
enterprises and institutions, and maintains their trust in
relevant institutions under the influence of online comments.
However, online comments containing misinfor-
mation can exacerbate public prejudice and
misunderstanding [37]. On the other hand, misinformation
can also reverse the public’s behavioral response to natural
disasters, accidents, public health, and social security
emergencies. For example, studies have pointed out that
with the development of the Internet, social media has
become an important channel for the public to seek health
information. However, the authenticity of health information
will significantly affect the treatment of patients and
threaten their lives [38]. +erefore, the content analysis
method is used to analyze the misinformation on social
media to understand the content characteristics of the
misinformation. It is not only beneficial to identify misinformation
on social media, but it also can contribute to the
governance of misinformation and the maintenance of
network security.
2.3. Misinformation and Emotion Analysis. +e spread of
misinformation on social media implies different emotions
of the public, and the emotions also change with the spread
of misinformation. On the one hand, in addition to the
influence of specific social conditions, the psychological state
is an important factor in the production of misinformation,
which is formed as an additional product of public events
under the adaptive condition of public emotions [39]. On
the other hand, emotions also play an important role in
spreading misinformation on social media. It is because they
can meet some psychological needs of the public that
misinformation keeps forming and spreading on social
media. Related studies point out that the spread of misinformation
is inseparable from emotions, and misinforma-
tion can spread rapidly when a group falls into negative
emotions such as anxiety [40]. +erefore, it is particularly
important to analyze the misinformation on social media
based on emotion analysis.
Among the research on misinformation using the
emotion analysis method, exploring the influencing factors
of emotion changes in misinformation is an important
object of research. For example, some researchers explored
the factors influencing the emotional dynamics in misinformation
by quantitatively analyzing the emotional be-
haviors of Internet users on social media. Studies have found
that misinformation contains far more negative emotions
than other kinds of information, while factors such as user
engagement, number of comments, and time of discussion
all had an impact on the change of emotions in misinformation.
Generally, the more active the users are, the more
comments they make, the longer the discussion takes, and
the more dominant negative emotions become [41]. Studies
have also been conducted to study misinformation active on
social media during public crises as the research object and
to identify emotions of Internet users in relation to comments
under the misinformation. It was found that both the
gender of Internet users and the subject category of the
content had an impact on the change in emotions [42]. Some
studies conducted sentiment analysis on popular misinformation
on social media based on content analysis
methods and found that as the content, form, and linguistics
of misinformation changed, so did the public’s sentiment. In
general, the more conflicting the content of the misinformation,
the more attractive the font (e.g., colorful fonts are
more attractive than regular fonts), and the more exaggerated
and extreme the use of language, the more intense
the change in public sentiment would be [43].
In addition to analyzing the influencing factors of emotions,
it is also an important topic to understand the evolution
pattern of emotions triggered by misinformation on social
media. On the one hand, social media exists as a disseminator
of misinformation, and when the public has emotional
fluctuations under the influence of misinformation, social
media often magnifies the impact of emotions and makes
emotions spread rapidly in social networks. On the other
hand, the spread of emotions on social media often exists in
some regular form. By analyzing a certain amount of text data,
we can try to reveal the evolution law of emotions on social
media. For example, a study combined with network science
to analyze the sentiment of misinformation about the political
election in Twitter, and found that posts with negative sentiments
last longer than those with positive or neutral sen-
timents. At the same time, misinformation about the election
winners had a broader diffusion network in the social media,
and more positive sentiments representing likes and support
among them [44]. Some studies used big data-driven approaches
to conduct sentiment analysis on the “viral” spread
of false and true information about natural disasters on social
media based on massive data. +e study found that tweets
with negative emotions spread faster than tweets with positive
or neutral emotions. It was also found that emotions had an
opposite effect on the “viral” spread of true and false information
on social media [45]. Based on data from social media
chat platforms, some studies have also attempted to map the
trends in public sentiments over time related to misinformation.
+e study not only confirmed that the proportion of
negative emotions triggered by misinformation was more
than positive emotions but also found that negative emotions
were affected by event information in an inverted U-shaped
curve over time. When event information was further
revealed, when handling measures and coping methods were
introduced or falsified information appears, the proportion of
negative emotions caused by misinformation would decrease
and the negative emotions brought by misinformation would
be weakened [46].
4 Security and Communication Networks
#
#To sum up, the active misinformation on social media
brings mainly negative emotions, which not only affects the
public’s psychology and behavior and increases the difficulty
of network public opinion governance but also has an
impact on the stability of social order and the maintenance
of social security. +erefore, combining with network
analysis methods, and analyzing the law of spreading
emotions of misinformation in social networks is not only
beneficial to provide suggestions for emotion management
when online public opinions occur but also to better achieve
guidance and prevention and control of online public
opinions, and thus eliminate the negative effects brought
about by the changes in group emotions.
3. Research Methods
+is paper selects misinformation as the research object and
compares the network structure of misinformation and true
information on social media to better reveal the diffusion and
evolution law ofmisinformation on social media, and provides
a reference for the public opinion management of misinformation.
+e frame structure diagram of the study is shown in
Figure 1. Firstly, we obtained misinformation, true information,
and the corresponding public opinion datasets on social
media, and completed the data pretreatment. Secondly, we
combined deep learning methods to conduct content analysis
and emotion analysis onmisinformation and true information
data and tried to compare the diffusion patterns of misinformation
and true information on social media using net-
work analysis methods. Finally, we drew a correlation graph to
show the disturbance of misinformation and true information
on the network structure and revealed the evolution pattern of
misinformation and true information in the network.
3.1. Network Analysis Method. As a research method for
analyzing interpersonal relationships, the network analysis
method has been widely used in various disciplines. One of
the most typical and well-known network analysis methods
belongs to the social network analysis method, which is a
combination of methods and tools for studying interpersonal
relationships, interactions, and communication, and
has been adopted by a large number of researchers and
research fields [47]. In the social network analysis method,
we describe the relationships between people in the social
network through the concepts of nodes and edges. Nodes
which are defined as individuals or groups are connected in a
certain direction by the edge representing the relationship to
form a social network graph [48]. With the social network
analysis method, we can understand the sparse relationship
between participants in social networks. At the same time,
we are able to analyze the importance of the location of
participants in the social network by measuring network
centrality and identifying important and isolated participants
in the network [49]. In short, the social network
analysis method is able to analyze not only the breadth
(scope) but also the depth of the relationship between
participants, providing support for exploring the interpersonal
interaction and the degree of communication.
With the development of network technology, the online
social network formed on the basis of social media has
become an important part of interpersonal activities.
Compared with traditional social networks, online social
networks are characterized by higher participation, larger
network scale, faster changes in network structure, and
wider network influence [50]. At the same time, the online
social network has a rapid growth rate, and has penetrated
people’s daily lives, and provides everyone with a convenient
way to communicate. Unlike traditional social network
analysis methods, online social network analysis methods
mainly focus on the flow and dissemination of information
on social media and have become an important research
method in the field of machine learning, data mining, and
complex network systems with the support of massive social
media data [51, 52]. In the analysis process of online social
network analysis methods, on the one hand, social media
users act as individual nodes of the network, and the forwarding
of information among users constitutes a propa-
gation relationship, and the network structure is shown by
visual methods. On the other hand, relevant information
continues to evolve and spread in social networks through
user interactions such as comments, favorites, and reposts.
+rough the online social network analysis method, we can
not only analyze the structure of communication and interaction
among network users but also explore the prop-
agation evolution pattern of a certain event with the support
of social media data. +erefore, we use the network analysis
method to analyze the propagation pattern of misinformation
on social media.
In this paper, we used the network users involved in the
spread of misinformation as nodes and the forwarding
relationship between users as edges to map out the network
structure of misinformation. In addition, in combination
with the content analysis method and emotion analysis
method, we analyzed the network diffusion characteristics
of misinformation, such as the number of reposts, favorites,
and comments of misinformation, and then explored the
evolution pattern of misinformation in social networks. At
the same time, considering that only misinformation was
not enough to investigate the evolution of misinformation
on social media, we also selected the corresponding true
information as the comparison object. +rough the comparison
of the spread of misinformation and true infor-
mation on social media, we can gain an insight into the
evolutionary diffusion characteristics of misinformation on
social media.
3.2. Content Analysis Method. In this paper, we used the
content analysis method to analyze the data and summarize
the topics of misinformation and true information. Firstly,
after data preprocessing, to reduce noise interference and the
need for further filtering the data in this paper, we used
the TF-IDF method to extract text keywords based on the
importance of the words. According to the frequency of
words in the text and corpus, we extracted 20 keywords most
important in the text using the TF-IDF method based on the
weighted processing.
Security and Communication Networks 5
#
#Secondly, we need to extract semantic features according
to the meaning of words in the language environment and
the relationship with other words and then transform semantic
features into feature vectors. In general, discrete
representation and distributed representation are commonly
used for word representation in the computer. +e difference
is that the former represents each word as a long vector,
while the latter represents each word as a dense continuous
vector of fixed length. Compared with the former, the latter
can not only save the vector space but also better represent
the relationship between words. +erefore, in existing
studies, distributed representation has often been adopted by
researchers to represent word vectors [53]. In this study, we
used theWord2vec method introduced by Google to convert
text data into word vectors and mapped text content into
vector space to calculate the similarity between words [54].
+eWord2vec model is mainly composed of the Continuous
Bag-of-Wordsmodel, which predicts the current word based
on the upper and lower words, and the Continuous Skipgram
model, which predicts the context word based on the
current word. In the analysis of large sample datasets, the
effectiveness of the CBOW model in text analysis has been
confirmed by previous studies. Compared with the Skipgram
model, the CBOW model has greater advantages in
analysis efficiency and analysis speed [55]. +erefore, we
used the CBOW model to complete the training of word
vectors and convert text data into word vectors.
Finally, we analyzed the topics of misinformation and
true information based on the K-means clustering method.
By using the K-means method, the study divided the processed
sample set into K categories and tried to minimize the
distance between samples in each category andmaximize the
distance between categories, which in turn could be achieved
based on setting the clustering category as ω1,ω2,ω3, . . . ,ωk.
+e calculation process of the least square error is shown in
equation (1). +e minimum squared error is denoted by β,
and the different clustering categories are denoted byωn.+e
documents and mean vectors in the clustering category are
represented by w and φn, respectively.
β � 􏽘
k
n−1
􏽘
w∈ωn
w ∈ φ2
n
����
����. (1)
As a common unsupervised clustering method, K-means
mainly achieves the classification of different categories
through similarity, where K and means, respectively, represent
the number of clustering categories and the mean
value of clustering vectors. A common method used to
measure the size of K is the Elbow method, and the distance
is measured by the cosine distance or Euclidean distance
formula [56]. Using the Elbow method, we selected different
K values to cluster the sample set and calculated the square
sum of the error between the document vector and the
clustering mean vector, based on which the curve of the sum
of the square error is plotted. By observing the comprehensive
curve of the square error, we could find that the
square error varies with the value of K. Accordingly, we
could infer the appropriate value size of K, and then classify
the misinformation and true information topics into K
categories.
3.3. Sentiment Analysis Methods. With the expansion of
social network corpora built on social media platforms,
sentiment analysis has become an important research field in
natural language processing and text mining. Sentiment
identification and analysis of comments, opinions, and other
texts in the social network corpus through sentiment
analysis not only provide data support for our understanding
of the dissemination pattern of information on
social media and the change of public sentiment but also
enable further explanation of the behavior logic behind
social networks of Internet users [57]. At present, common
sentiment analysis methods are mainly divided into two
categories. One is by building sentiment dictionaries and
combining semantics to compare the similarity between the
text keywords in the sample data and the sentiment words in
the dictionary and then calculating the sentiment intensity of
the text keywords to assign the corresponding sentiment
True information 
Misinformation
Remove stop words
Text cleaning
Word Segmentation
Network analysis  
Content analysis
Emotion analysis
Data
visualization
Data collection Data preprocessing Data analysis
Figure 1: Research frame diagram.
6 Security and Communication Networks
#
#labels to the text [58]. +e other is to use machine learning
methods such as SVM, Naive Bayes, or deep learning
methods based on neural networks to conduct bias analysis
of sentiments in a supervised way and assign corresponding
sentiment labels to texts [59]. However, in the sentiment
analysis based on the sentence types of social media platforms,
the approach based on sentiment dictionaries often
has an impact on the final sentiment classification results
due to the neglect of context. +erefore, machine learning
and deep learning methods are often used in sentiment
analysis of social network data. However, taking sentiment
analysis as an example, SVM, CRF, and other machine
learning methods excessively rely on manual labeling data
and have limitations in processing natural data in the
original form. Deep learning methods combine simple but
nonlinear modules to learn complex functions and the selflearning
feature of deep learning greatly improves its ap-
plication ability in the field of natural language and shows
better effect in sentiment analysis [60, 61].
In sentiment analysis of sentence-level objects, long- and
short-term memory models that can learn word vectors of
different lengths have shown better performance in sentiment
classification and have been widely used by researchers [62].
As a result of the further development of the recurrent neural
network, the LSTM model retains the flexibility of recurrent
neural networks in learning contextual information of text
sequences and addresses the difficulty of traditional recurrent
neural networks in storing information for long periods of
time. Similarly, the LSTM model remains structurally consistent
with recurrent neural networks, but the internal
structure is composed of the cell circulation state C, input gate
I, forgetting gate F, output gate O, and hidden state output θ,
which differs from that of the recurrent neural network.
Among them, the final output of themodel is completed by tan
transformation of the current state. +e calculation process is
shown in Equation 2, where θT represents the output of the
hidden state at time T, and OT and CT, respectively, represent
the state of the model output gate cell unit at time T.
θT � OT ∗ tanθ CT( 􏼁. (2)
+e output gate of the model determines what should be
output by calculating the sigmoid layer, and the calculation
process is shown in equation (3), where OT denotes the state
of the output gate at the moment of T, θT−1 denotes the
output state at the previous moment, φT denotes the input
state of the information at the current moment, ωO and bO
denote the weight and deviation of the output gate, and the
activation function is denoted by σ.
OT � σ ωOθT−1 + ωOφT + bO( 􏼁. (3)
As an important stage of model operation, the cell renewal
is an intermediate stage in which input information
passes through the input gate, the forgetting gate, and finally
enters the output gate. +e cell state update is updated as
shown in equation (4),CT, FT , and IT denote the state of the
cell unit, the forgetting gate, and the output gate at the
moment of T, respectively, and the weights and deviations
are denoted by ωC and bC.
CT � FT ∗CT−1 + IT ∗ tanθ ωCθT−1 + ωCφT + bC( 􏼁. (4)
+e forgetting gate determines what information should
be discarded and the calculation process is shown in
equation (5), where σ denotes the activation function, ωF
and bF denote the weights and biases of the forgetting gate,
respectively.
FT � σ ωFθT−1 + ωFφT + bF( 􏼁. (5)
+emain work of the input gate in the model structure is
to decide which information should be input, as shown in
equation (6). In this case, the weight and deviation of the
input gate are denoted by ωI and bI, respectively.
IT � σ ωIθT−1 + ωIφT + bI( 􏼁. (6)
Although the LSTM model performs well in processing
the input context sequence in this paper, the model also has
some shortcomings in that it cannot consider the direction
of input and can only process text sequence context in one
direction. +erefore, we adopted a bidirectional long- and
short-term memory model, which can process text sequences
from left to right and from right to left by two
parallel long- and short-term memory models. Compared
with the long- and short-term memory model, the bidirectional
long- and short-term memory model can obtain
past and future information and concatenate the hidden
states in the backward and backward directions, and output
contextual information through the same output layer. It not
only improves the scope of text processing but also improves
the efficiency of text processing. +e calculation process of
the final output Z of the model is shown in the following
equation:
ZT � ωZθT + bZ. (7)
In this paper, we used the training and test sets from the
NLP&CC dataset, and the emotion labels in the dataset were
“like,” “surprise,” “disgust,” “sadness,” “happiness,” “anger,”
and “fear,” respectively [63, 64]. In our study, we used a
bidirectional LSTM model to assign emotion labels to text
data. +e test accuracy of the model met our research needs
and was expressed as 0.71. Meanwhile, to further improve
the accuracy and efficiency of the model emotion, we randomly
grabbed 200,000Weibo tweets for pre-training. Based
on the pre-training, we implemented the analysis of both
misinformation and true information datasets. Finally, we
assigned emotion tags to each Weibo tweet in the misinformation
dataset and the true information dataset to study
the pattern of emotional evolution in social networks.
4. Results and Discussion
4.1. Data Collection and Preprocessing. In this paper, we
chose the large open-source dataset of the Sina Weibo
platform as our analysis object [65]. As one of the most
popular social media platforms in China, Sina Weibo has
about 530 million active users and has established a huge
online social network, which also provides rich data sources
to investigate the spread of misinformation in social
Security and Communication Networks 7
#
#networks. Datasets are open-source datasets from 2015 and
2016, mainly composed of misinformation and true information
datasets. We obtained the relevant content and
forwarding information of each original microblog in the
dataset and constructed the dissemination network of
misinformation and true information on social media.
Among them, the misinformation in the microblog dataset
came from the information that has been falsified by the Sina
Community Management Center. At the same time, the
dataset also collected a similar amount of true information
for comparative studies. True information and misinformation
were present at the same time, which attracted ex-
tensive public attention and the content had been proved to
be true. +e details of the misinformation dataset and the
true information dataset are shown in Table 1.
Table 1 shows the statistics of the dataset. Among them,
there were 2351 true information tweets and 1717154
retweets. +e original Weibo tweets were retweeted 52,158
times at most and 12 times at least, with an average of 730
retweets. +e number of misinformation tweets was 2313,
and the number of retweets was 2093056. +e original
Weibo tweets were retweeted 59,319 times at most, 11 times
at least, and 905 times on average.
4.2. Network Structure of Misinformation. +rough the
microblog ID and forwarding relationship in the dataset, we
can understand the diffusion of true information and
misinformation in the social network. Meanwhile, based on
the Mid of microblog, we obtained the secondary and tertiary
forwarding information of some true information and
misinformation in the process of forwarding, so as to
construct the dissemination network of true information
and misinformation on social media. Based on this, we used
the Fruchterman Reingold layout to draw the graph of the
forwarding network. +e spread graph of true information
and misinformation on social media is shown in Figure 2.
In Figure 2, we can observe the network built by true
information and misinformation on social media. Considering
the limitation of space, we randomly selected and
showed the spread of some true information and misinformation
on social media. Among them, Figure 2(a) shows
the forwarding network built by true information and
misinformation on social media, with the purple node
representing misinformation events and network users who
forward misinformation, and the green node representing
true information events and network users who forward true
information. Figures 2(b) and 2(c) show the network diffusion
structures of misinformation and true information,
respectively.
Firstly, in the network graph composed of misinformation
and true information, the network structure of true
information or misinformation is sparse, the number of
edges is much smaller than the number of nodes, and there
are many isolated nodes.+is may be related to the nature of
the content of the dataset.+e dataset we adopted consists of
different misinformation events and true information
events, and the connection between the events is not very
close. Although there are crossover relationships between
some events, most of them are differentiated and exist in an
isolated form during the forwarding process. Combined
with text materials, most of the related events in the original
data collected were related to the topic. For example, when
referring to food safety events, users often enumerate previous
similar events for comparative analysis, which was also
an important reason for the network connection between
different events. Secondly, in the dataset adopted in this
paper, although the number of events of misinformation is
smaller than that of true information, the forwarding relationship
of misinformation is much more than that of true
information. It can also be intuitively observed from Figure 2
that the network graph of misinformation is denser and the
nodes are more closely connected to each other than the
network graph of true information. Compared with true
information, misinformation has more advantages in both
the scope and depth of diffusion and is more likely to attract
the attention of the public and be spread by the public in
social networks. Combined with text materials, misinformation
in social networks often exists with exaggerated titles
and contents, which are more likely to be noticed and
retweeted by the public. +is also tentatively confirms the
conclusion of existing studies that misinformation is more
likely to be disseminated on social media than true
information.
4.3. Topic Categories ofMisinformation andTrue Information.
Based on the obtained dataset of misinformation and true
information, we divided the data into seven categories by
using the clustering method and determined the topic tag
and interpretation content of each category in conjunction
with the information content. However, considering the
information content embedded in the event microblog was
not always easy to distinguish, there are often situations
involving multiple topics. +erefore, researchers extracted
the top keywords of each topic and matched the extracted
keywords with the text information of the event microblog.
If the text information of the event microblog conforms to
multiple keywords, the Weibo tweet would be given the
corresponding topic tag, while the microblog that did not
match the keyword would be given the corresponding topic
tag according to the content by manual annotation. +e
topic categories for true information andmisinformation are
shown in Table 2.
It can be observed from Table 2 that the distribution of
the number of microblogs is in an unbalanced state among
the seven topics divided into true information and misinformation.
Firstly, both true information and misinforma-
tion microblog posts are distributed in the largest number in
the field of food and product safety. Food and product safety
involves everyone’s daily life, so it is most likely to breed
misinformation and true information. Existing studies have
also revealed the causes of misinformation and true information
related to food and product safety from multiple
perspectives such as education level, gender, age, and media
reports [66].
Secondly, true information and misinformation are
distributed in public safety and crime topics only second to
8 Security and Communication Networks
#
#Table 1: Descriptive analysis of the dataset.
Descriptive analysis True information Misinformation
Tweets (i.e., original posts by Sina Weibo users) 2351 2313
Retweets (i.e., reposted messaged by Sina Weibo users) 1717264 2093056
Maximum forwarding relationship 52158 59319
Minimum forwarding relationship 12 11
Average forwarding relationship 730 905
(a)
Figure 2: Continued.
Security and Communication Networks 9
#
#(b)
Figure 2: Continued.
10 Security and Communication Networks
#
#food and product safety areas. +e difference between the
public safety theme and the crime theme lies in that the
former has a wider scope, involving national security, social
security, and personal security, while the latter is mainly for
acts that violate the law generated by individuals or small
groups. +e reason why the two topics rank high in terms of
(c)
Figure 2: An illustration of the forwarding network. In the figure, the node represents the microblog ID and the edge represents the
forwarding relationship between the microblogs. (a) True information and misinformation. (b) Misinformation. (c) True information.
Table 2: Topic categories of true information and misinformation.
Topic type Topic content True information Misinformation
Public security Information related to public safety and social security incidents 145 259
Food and product Related to the safety of the diet or certain type of product 1618 1430
Politics Information related to political events, politicians, or major policies 74 87
Celebrity Celebrity information such as anecdotes 26 9
Crime Information related to criminal events and people 269 358
Disaster Information related to natural disasters or accidents 79 49
Social events Involving a strong social response, biased towards folk tales 78 84
Security and Communication Networks 11
#
#the number of topic distribution is not only because such
incidents are most likely to trigger public anger, sympathy,
and other emotions but also because related events are easy
to be favored and extensively reported by the media.
+erefore, both true information and misinformation have a
large number of microblogs in public safety and crime
topics.
+e political topic mainly involves information about
political events, politicians, and major policies. On the one
hand, the public tends to pay more attention to politics and
is keen on discussing and paying attention to events and
information related to politics. On the other hand, it is also
because the major policies issued by the government often
affect every field of society and everyone’s life. However,
there are also some individuals or groups who misinterpret
policies for personal gain, allowing misinformation to
spread.
+e disaster topic mainly involves natural disasters and
accidents that cause economic and property losses to the
public and threaten the safety of the public. When natural
disasters or accidents occur, social media, as a convenient
way of communication, becomes an important platform for
the public to share disaster information and seek help.
+erefore, there often is a wide public discussion under
related topics. However, social media’s weak falsification
capabilities have also led to the widespread dissemination of
misinformation about events such as natural disasters and
accidents. For example, disaster-related information is often
distorted on social media by exaggerating the number of
people, distorting the real situation of events, conspiracy
theories, and other ways.
Social events and celebrity topics are more targeted, with
the former mainly targeting events that arouse social responses
and discussions and concerns about the public, and
the latter mainly responding to celebrities and other celebrity-related
events. However, these two types of events
tend to cover only some groups; for example, the former is
more geographically differentiated, the latter is more distinguished
by demographic attributes such as fans. +e
distribution of true information and misinformation is less
compared to other topics.
4.4. Diffusion Characteristics of Misinformation and True
Information. To further compare the evolution pattern of
misinformation and true information in social networks and
understand the diffusion characteristics of misinformation
and true information, we tracked the forwarding relationship
of each original microblog and measured the diffusion
characteristics of datasets of misinformation events and true
information events by using the three indicators: the number
of retweets, number of comments, and number of favorites
of microblog. +e number of retweets mainly refers to the
accumulation of the diffusion index of posts in social networks
constructed by social media, which measures the
diffusion degree of microblog posts. +e number of comments
refers to the accumulation of user interaction index
for posts in the social network, which measures user engagement
of microblog posts. +e number of favorites
mainly refers to the accumulation of the number of times
that posts are collected by users in social networks and
measures the user acceptance and recognition degree of
microblog posts. +e three indicators involve several diffusion
characteristics, such as the diffusion range, diffusion
participation, user acceptance, etc., which can give us a more
comprehensive understanding of the diffusion patterns of
misinformation and true information in social networks.
Table 3 shows the difference between true information and
misinformation in the number of retweets, favorites, and
comments on the microblog.
It can be observed from Table 3 that each true information
event is forwarded about 924 times (SD 2706.101).
Among them, a maximum of one true information was
forwarded 81,776 times, and at least one true information
was forwarded 30 times. From the point of view of the
number of favorites, the average of each true information
was about 648 times (SD 1961.269), true information was
saved 40,873 times at most, and at least one was not saved. In
terms of the number of comments, there were about 387
comments (SD 1061.163) on average for each true information
event on Weibo, with a maximum number of 26,275
comments and a minimum number of no comments.
Skewness and kurtosis represent the asymmetry and
steepness of variables, respectively. +rough the observation
of skewness and kurtosis, we found that the number of
retweets, favorites, and comments of true information was
all rightward and steeper, with a sharp peak. Among them,
the rightward deviation and peak degree of the forwarding
number was the deepest.
Based on the data in Table 3, we also found that each
misinformation was forwarded about 2138 times (SD
6299.091), with the maximum number of forwards being
103682 and the minimum number of forwards being 88. In
terms of the number of favorites, each misinformation was
saved 521 times (SD 2308.874) on average, with the maximum
number of favorites being 48889 times and the
minimum number of favorites being 0 times. In terms of the
number of comments, there were about 540 comments (SD
2261.956) for each misinformation on average, with the
maximum number of comments being 38,103, and the
minimum number of comments being 0. In terms of
skewness and kurtosis, the number of forwards, favorites,
and comments of misinformation all showed rightward
deviation and peak state. Among them, the spikes of the
number of collections were the most obvious, and the right
skew of the number of comments was deeper.
At the same time, we also took the independent sample
T-test to further compare the difference in diffusion characteristics
between true information and the misinforma-
tion. +e results showed that there were significant
differences between true information and misinformation in
the number of retweets, favorites, and comments (P< 0.01),
and themean value of the number of retweets and comments
of misinformation was greater than the mean value of the
true information, which means that the spread feature of
misinformation would have been easier to be spread on
social media had it been further verified. Compared with the
true information, the public was more inclined to interact
12 Security and Communication Networks
#
#and communicate around the misinformation in the social
network, and it was more likely to forward and spread the
misinformation, which makes the misinformation spread
rapidly in the social network. At the same time, we also
found that the number of true information events was
greater than that of misinformation, which means that
despite the rapid spread of misinformation on social media,
people had higher levels of recognition and support for true
information. Combined with text materials, this was related
to the content characteristics of true information. True
information usually contained more knowledge and had
more detailed arguments, and was more likely to be collected
by network users. As for misinformation, on the one hand, it
often attracted users’ attention through exaggerated titles,
and then was retweeted by Internet users. On the other hand,
with the help of misleading and controversial content, users
often argued with each other, resulting in a large number of
comments under misinformation posts. However, as misinformation
can be easily proved to be forged and the
polarizing debate also made users tend not to collect posts,
the number of misinformation collected was smaller than
that of true information.
In addition, we used the General Linear Model (GLM) to
examine the differences in diffusion characteristics between
different categories of topics in true information and misinformation
based on setting the number of original Weibo
users’ followers as the control variable. GLM allows for
multivariate analysis of variance and is often used to
compare differences in means between two or more variables.
First of all, we established the model according to the
research needs and decided the model to be a univariate or
multivariate analysis of variance. +e difference between the
two lies in the number of dependent variables. Secondly, we
estimated the model through the test and got the value of the
model test. And, we used statistical inference to calculate
significance and values. Finally, after obtaining significant
values, the meaning of the relationship needed to be
explained. GLM statistics showed a significant interaction
between information veracity and topic category (F� 2.264,
Wilks’ Lambda� 0.991, P< 0.01). +e marginal estimates
and confidence intervals on the number of retweets for true
information and misinformation on different topic categories
are shown in Figure 3. +e marginal estimates and
confidence intervals on the number of favorites are shown in
Figure 4.+emarginal estimates and confidence intervals for
the number of comments are shown in Figure 5. At the same
time, to further conform to the normal distribution hypothesis,
we carried out the logarithmic transformation on
the number of retweets, the number of favorites, and the
number of comments.
Combining marginal estimates and significance tests, we
analyzed the diffusion characteristics of true information
and misinformation in each topic category. For the topic
categories of public security and politics, the estimated mean
value showed that the diffusion index of misinformation in
retweets and comments was higher than those of true information,
and the diffusion index in favorites was lower
than that of true information. However, there was a significant
difference between misinformation and true in-
formation only for favorites. For the topic category of food
and product, the diffusion index of misinformation in
retweets and comments was also higher than that of true
information, and smaller than that of true information in
favorites, and there was a significant difference between
misinformation and true information in all the three aspects.
For the topic category of celebrity, there was no significant
difference between misinformation and true information in
the number of retweets, favorites, and comments, whichmay
be related to the small sample size of the topic category of
celebrity. For the topic category of crime, there was a significant
difference between misinformation and true in-
formation in the number of retweets and favorites, and in the
diffusion index of retweets, misinformation was larger than
true information. In the diffusion index of the favorites,
misinformation was less than true information. For the topic
category of disaster, misinformation was larger than true
information in the diffusion indexes of retweets and favorites,
and there was a significant difference, which may be
related to the occurrence of disaster information often accompanied
by information of first aid measures. As for the
topic category of social events, contrary to the previous topic
categories, the diffusion index of misinformation was
smaller than that of true information in both retweets and
favorites, and there was a significant difference, which may
be related to the fact that the true information of social
events was easier to be identified, and it was more likely to
arouse the emotions of Internet users.
To sum up, we further confirmed that in social networks,
misinformation spread faster than true information in most
cases, and was more likely to be forwarded and spread by
network users. In terms of user participation represented by
comments, misinformation was also in an advantageous
Table 3: Diffusion characteristics of true information and misinformation.
Retweets Favorites Comments
T M T M T M
Mean 923.78 2138.09 648.23 521.28 386.97 540.36
Std. deviation 2706.101 6299.091 1961.269 2308.874 1061.163 2261.956
Maximum 81776 103682 40873 48889 26275 38103
Minimum 30 88 0 0 0 0
Skewness 17.022 7.913 8.716 10.581 10.751 11.033
Kurtosis 421.234 86.120 113.224 153.908 188.846 145.921
T value −9.750∗∗ 16.139∗∗ −1.816∗∗
Note. ∗∗P< 0.01, T denotes true information, and M denotes misinformation.
Security and Communication Networks 13
#
#10
8
6
4
2
0
Public security Food and
product
Politics Celebrity Crime Disaster Social events
Topic category
Es
tim
at
ed
 m
ea
n
True information
Misinformation
Figure 3: Estimated diffusion characteristics of true information andmisinformation based on the topic category by the number of retweets.
10
8
6
4
2
0
Public security Food and
product
Politics Celebrity Crime Disaster Social events
Topic category
Es
tim
at
ed
 m
ea
n
True information
Misinformation
Figure 4: Estimated diffusion characteristics of true information and misinformation based on the topic category by the number of
favorites.
14 Security and Communication Networks
#
#position compared with true information. However, in
terms of the number of favorites, misinformation was
generally less than true information, and true information
was more likely to be recognized by the public for its more
detailed content characteristics, and thus showed an advantage
in terms of the number of favorites.
4.5. Emotion Changes of Misinformation in Topic
Classification. After assigning emotion labels corresponding
to misinformation and true information events, we first used
the Chi-square test to analyze whether there was a significant
difference in emotion distribution between misinformation
and true information. +e results confirmed the existence of
significant differences in the distribution of information
veracity across emotions (Pearson Chi-Square� 17.705,
P< 0.01). Second, we compared specific differences in the
distribution of emotions between misinformation and true
information. +e details are shown in Table 4.
It can be observed from Table 4 that “like” emotion
dominates both true information and misinformation,
which was also the reason why both true information and
misinformation can get many retweets. Combined with text
materials, both true information and misinformation contained
information that the public wanted to obtain, such as
diet collocation, health knowledge, anecdotes, etc., which
attracted the interest of the public. +erefore, in the distribution
of emotions between true information and
misinformation, the “like” emotion became the dominant
emotion.
Secondly, in contrast, in addition to the emotion of
“like,” negative emotions dominated by “disgust” and
“sadness” occupied the mainstream in both misinformation
and true information, and the number was much larger than
other emotion types. +e analysis of textual materials
showed that the public tended to form emotions of “disgust”
and “sadness” in the face of disasters, crimes, and other kinds
of news. +is finding was also in line with our expectations
that when faced with natural disasters, accidents, or other
similar events, in reality, we are more likely to form “disgust”
and “sadness” emotions under the influence of factors such
as compassion and sense of justice.
At the same time, we also divided true information and
misinformation into different topic categories and compared
10
8
6
4
2
0
Public security Food and
product
Politics Celebrity Crime Disaster Social events
Topic category
Es
tim
at
ed
 m
ea
n
True information
Misinformation
Figure 5: Estimated diffusion characteristics of true information and misinformation based on the topic category by the number of
comments.
Table 4: Distribution of information veracity in different emotions.
Emotion type
Information veracity
True information Misinformation
Like 1715 1729
Disgust 300 277
Surprise 31 21
Happiness 29 56
Fear 24 23
Sadness 241 203
Anger 11 4
Security and Communication Networks 15
#
#the distribution of true information and misinformation on
emotions under different topic categories. +e specific
distribution information of information veracity on emotion
under different topic categories is shown in Table 5.
Based on the observation in Table 5, we found that there
was a relationship between misinformation and true information
in terms of the content type and sentiment type.
In the topics of public security, the most common expressions
of emotion in true information and misinformation
were the emotions of like, disgust, and sadness. Among
them, the emotion of like came from the emergency management
measures embodied in information and timely
response to events; the emotion of disgust came from the
harm brought about by public safety events to the country,
society, and individuals; and the emotion of sadness mainly
came from sympathy for victims. In the topic category of
food and product, emotions in true information and misinformation
were most reflected as like and disgust. Com-
bined with the text materials, the emotion of like lied in the
knowledge of food safety and products contained in the
microblog information, and the emotion of disgust lied in
the conflict and hatred of fake and shoddy food and
products. In the topic category of politics, like and disgust
were often reflected in true information, while like and
happy were usually reflected in misinformation. In combination
with the text materials, the emotion of like was
reflected in the information that satisfies the public’s curiosity
about the political field, the emotion of disgust was
reflected in the dissatisfaction with some political events and
policies, and the emotion of happiness was often reflected in
the support for political figures and the approval of some
policies. In the topic category of celebrity, the emotions of
like and disgust became the mainstream in true information
and misinformation. +e emotion of like came from the
public’s love for some stars and celebrities, while the
emotion of disgust also came from the public’s resistance to
some stars and celebrities. In the topic category of crime,
emotions were mainly embodied in two types of emotions:
like and disgust. As the most direct source of information,
the public’s emotion of like is more obvious when they see
images of criminals being arrested and punished in media
reports, while the emotion of disgust was mainly from the
resistance to the crime. In the topic category of disaster, the
emotions of like and sadness were often reflected. Among
them, the emotion of like came from the solidarity and
common support of the public in the face of disaster, while
the emotion of sadness was mainly reflected in the loss of
personnel and property. In the topic category of social
events, like and disgust often occupied the dominant position.
Among them, the emotion of like came from the
public’s curiosity about strange and usual stories, while the
emotion of disgust came from the fact that some social
events often contained contents that go against the social
conscience.
In addition, we also verified whether there was a significant
difference in the effect of subject category on
emotion between true information and misinformation. +e
results showed that compared with true information
(Pearson Chi-Square� 38.707, P> 0.05), the topic category
of misinformation had a more significant impact on the
emotion types (Pearson Chi-Square� 178.625, P< 0.01), and
there were significant differences in the emotional distribution
of different topics in misinformation. +erefore, it is
important to divide misinformation into different topics and
understand the emotional distribution of misinformation
from different topic categories for us to better understand
the diffusion pattern of misinformation in social networks.
4.6. Emotion Analysis of Network Diffusion Characteristics of
Misinformation. Related studies have confirmed that
emotion plays an important role in the spread of misinformation.
On the one hand, under the influence of emotion,
the public will interact and communicate with each other
about different events, which will cause heated discussions.
On the other hand, information is also embedded with
public emotions in the process of information sharing and
spreading, and the spread of misinformation in social
networks is often promoted by different emotions. +erefore,
this paper compares the network diffusion character-
istics of misinformation and true information through
emotion analysis to further reveal the network diffusion
pattern of misinformation. Combined with the available
data, to further determine the specific correlation of each
topic in different emotions, we used Pearson’s Point Biserial
correlation coefficient to analyze the correlation between
information veracity and the corresponding network diffusion
characteristics. Among them, the direction of cor-
relation represents whether it had a dominant position in
network diffusion. Positive correlation meant that misinformation
was more likely to be retweeted, favored, and
commented by network users in social networks than true
information, and negative correlation meant that true information
was more likely to be retweeted, favored, and
commented by network users in social networks than
misinformation. +e stronger the correlation, the closer the
relationship between information veracity and network
diffusion. +e correlation between information veracity and
the number of retweets is shown in Figure 6. +e correlation
between information veracity and the number of favorites is
shown in Figure 7. +e correlation between information
veracity and the number of comments is shown in Figure 8.
From the perspective of topics, in the topic of public
security, misinformation containing like and sadness was
more likely to be retweeted by Internet users, while misinformation
containing disgust and fear was more difficult to
be retweeted by Internet users. Among them, the information
containing the emotion of like had statistical sig-
nificance in the correlation between veracity and the number
of retweets (P � 0.073< 0.1). In terms of the number of
favorites, misinformation with sadness was more likely to be
collected by Internet users, while misinformation with like,
disgust, and fear was less likely to be collected by Internet
users. Among them, the information containing the emotion
of disgust had statistical significance in the correlation between
veracity and the number of favorites (P � 0.066< 0.1).
In terms of the number of comments, misinformation with
emotions of like and sadness was more likely to be
16 Security and Communication Networks
#
#commented on by Internet users, while misinformation with
emotions of fear and disgust was more difficult to be
commented on by Internet users.
In the topic of food and product, the veracity of information
under different emotional types was positively
correlated with the number of retweets, and misinformation
was more likely to be retweeted in social networks. Among
them, the information containing the emotions of like
(P � 0.000< 0.001), disgust (P � 0.001< 0.01), and sadness
(P � 0.02< 0.05) had statistical significance in the
Table 5: Distribution of topic types of information veracity in different emotions.
Topic type
Emotion type
Like Disgust Surprise Happiness Fear Sadness Anger
True information
Public security 107 16 2 1 4 15 0
Food and product 1144 218 27 20 18 185 6
Politics 56 11 0 1 0 5 1
Celebrity 20 4 0 0 0 2 0
Crime 215 30 1 6 1 14 2
Disaster 61 6 1 0 1 10 0
Social events 54 14 0 1 0 9 0
Misinformation
Public security 207 25 0 1 1 24 1
Food and product 1035 194 16 29 16 137 3
Politics 58 6 0 19 1 3 0
Celebrity 7 2 0 0 0 0 0
Crime 292 32 3 5 4 22 0
Disaster 36 5 2 1 0 5 0
Social events 70 11 0 1 1 11 0
Public security
Retweets map
Food and product
Politics
Celebrity
Crime
Disaster
Social events
Li
ke
D
isg
us
t
Su
rp
ris
e
H
ap
pi
ne
ss
Fe
ar
Sa
dn
es
s
A
ng
er
0.10
0.12
0.16
-0.02
-0.02
-0.32 0.360.10
0.13
0.14
0.14
0.270.24
0.20
0.21
0.55
0.45
0.16
-0.09 0.00
0.00
0.00
0.00
0.00 0.00 0.00 0.00
0.000.000.00
0.00
0.00
0.00 -0.80 0.00
0.00
0.00 0.00 0.00
0.00
0.00-0.48
0.23 0.15 0.19 0.13 0.42
0.17
0.17
0.16
0.50
0.25
-0.25
-0.50
-0.75
Figure 6: Correlation between information veracity and retweets’ volume.
Security and Communication Networks 17
#
#correlation between veracity and the number of retweets. In
terms of the number of favorites, the veracity of information
under different emotional types was negatively correlated
with the number of favorites, and true information was more
likely to be collected by network users. Among them, the
information containing the emotions of like
(P � 0.024< 0.05), fear (P � 0.03< 0.05), and sadness
(P � 0.004< 0.01) had statistical significance in the correlation
between veracity and the number of favorites. In terms
of the number of comments, the veracity of information
under different emotional types was positively correlated
with Internet users, and misinformation was more likely to
be commented on by Internet users. Among them, the information
containing the emotions of like
(P � 0.000< 0.001) and disgust (P � 0.04< 0.05) had statistical
significance in the correlation between veracity and
the number of comments.
In the topic of politics, misinformation in the emotions
of like, disgust, and sadness was more likely to be retweeted
by Internet users, while true information with happy
emotion was more likely to be retweeted by Internet users.
Among them, the information containing the emotions of
like (P � 0.09< 0.1) and disgust (P � 0.082< 0.1) had statistical
significance in the correlation between veracity and
the number of retweets. In terms of the number of favorites,
misinformation with the emotions of like, disgust, and
sadness were more likely to be collected by Internet users,
while true information with happy emotion was more likely
to be collected by Internet users. In terms of the number of
comments, misinformation with like, disgust, happiness,
and sadness was more likely to elicit comments from Internet
users.
In the topic of celebrity, misinformation containing the
emotion of like was more difficult to be retweeted by Internet
users, and misinformation in the emotion of disgust was
more likely to be retweeted by Internet users. In the number
of favorites, misinformation in the emotions of like and
disgust was easier to be collected by Internet users. In terms
of the number of comments, the misinformation in the
emotion of like was less likely to be commented by users,
while the misinformation in the emotion of disgust was
more likely to be discussed by users.
In the topic of crime, misinformation containing
emotions of like, disgust, happiness, fear, and sadness was
more easily retweeted by Internet users, while misinformation
under the emotion of surprise was more difficult to
be retweeted by Internet users. Among them, the information
containing the emotion of like (P � 0.023< 0.1) had
statistical significance in the correlation between veracity
and the number of retweets. In terms of the number of
favorites, misinformation under the emotions of like, fear,
and sadness was more likely to be collected by Internet users,
while misinformation under the emotions of disgust, surprise,
happiness, and sadness was more difficult to be
Public security
Food and product
Politics
Celebrity
Crime
Disaster
Social events
Li
ke
D
isg
us
t
Su
rp
ris
e
H
ap
pi
ne
ss
Fe
ar
Sa
dn
es
s
A
ng
er
0.00
0.50
0.25
-0.25
-0.50
-0.03
-0.05
0.05
0.11
0.06
-0.14
-0.15
0.37
0.21
-0.02
0.61
0.16
-0.01
-0.29 0.00
0.00
0.00
0.00
0.00 0.00 0.00 0.00
0.00
0.00
0.00
0.00
0.00
0.00 0.00
0.00 0.00
0.00
0.00
0.00
-0.24
-0.63
-0.21
-0.21
-0.22
0.68
-0.38
-0.61 0.03
-0.16
0.06
0.05
0.07
-0.49
Favorites map
0.22
Figure 7: Correlation between information veracity and favorites’ volume.
18 Security and Communication Networks
#
#collected by Internet users. In terms of the number of
comments, the misinformation containing emotions of like,
disgust, happiness, fear, and sadness was more likely to be
commented on by Internet users, while the misinformation
containing the emotions of surprise was more difficult to be
discussed and communicated by Internet users.
Among the topics of disaster and social events, misinformation
containing the emotions of like, disgust, and
sadness was more likely to be retweeted by Internet users. In
terms of the number of favorites, misinformation containing
the emotion of like was more difficult to be collected by
network users, and misinformation containing the emotions
of disgust and sadness was more likely to be collected by
network users. Among the topic of social events, information
containing the emotion of disgust (P � 0.078< 0.1) had
statistical significance in the correlation between veracity
and the number of favorites. In terms of the number of
comments, misinformation containing the emotions of like,
disgust, and sadness was more likely to be discussed and
communicated by Internet users.
From the perspective of emotion, in the topic category
under the like emotion, misinformation under the topic of
celebrity was more difficult to be retweeted by Internet users,
and misinformation under the rest of the topic categories
was more likely to be retweeted by Internet users than true
information. Among them, the information containing the
topics of public security (P � 0.073< 0.1), food and product
(P � 0.000< 0.001), politics (P � 0.09< 0.1), and crime
(P � 0.023< 0.05) had statistical significance in the correlation
between veracity and the number of retweets. In terms
of the number of favorites, true information under the topics
of public security, food and product, disaster, and social
events were more likely to be collected by network users.
Misinformation under the topics of politics, celebrity, and
crime was more likely to be collected by Internet users.
Among them, the information containing the topic of food
and product (P � 0.024< 0.05) had statistical significance in
the correlation between veracity and the number of favorites.
In terms of the number of comments, misinformation under
the topic of celebrity was more difficult to be discussed by
online users, and misinformation under the remaining topic
categories was more likely to be discussed by online users
than true information. Among them, the information
containing the topic of food and product (P � 0.000< 0.001)
had statistical significance in the correlation between veracity
and the number of comments.
In the topic category under disgust emotion, true information
under the topic of public security was more likely
to be retweeted by Internet users than misinformation.
Misinformation under the other types of topic categories was
more likely to be retweeted by network users. Among them,
information containing the topics of food and product
Public security
Food and product
Politics
Celebrity
Crime
Disaster
Social events
Li
ke
D
isg
us
t
Su
rp
ris
e
H
ap
pi
ne
ss
Fe
ar
Sa
dn
es
s
A
ng
er
Comments map
0.06
0.07
0.01
-0.22
0.06
0.11
0.09 0.13
0.31
0.17
0.52
0.20
0.10
-0.16 0.00
0.00
0.00 0.00
0.00 0.00
0.01
0.00
0.00 0.00
0.00 0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.21
-0.32 0.30
0.05
-0.59
0.10
0.91
0.23
0.41
0.18
0.21
0.08
0.17
0.34
0.9
0.6
0.3
0.0
-0.3
Figure 8: Correlation between information veracity and comments’ volume.
Security and Communication Networks 19
#
#(P � 0.001< 0.05) and politics (P � 0.082< 0.1) had statistical
significance in the correlation between veracity and the
number of retweets. In terms of the number of favorites, true
information under the topics of food and product, public
security, and crime were more likely to be collected by
Internet users, while misinformation under the topics of
politics, celebrity, disaster, and social events was more likely
to be collected by Internet users. Among them, information
containing the topics of public security (P � 0.066< 0.1) and
social events (P � 0.078< 0.1) had statistical significance in
the correlation between veracity and the number of favorites.
In terms of the number of comments, misinformation under
the topic categories of food and product, politics, celebrity,
crime, disaster, and social events was more likely to be
discussed by Internet users. Compared withmisinformation,
true information containing the topic of public security is
more likely to be commented on by network users. Among
them, the information containing the topic of food and
product (P � 0.04< 0.05) had statistical significance in the
correlation between veracity and the number of comments.
In the topic category under surprise emotion, misinformation
under the topic of food and product was more
likely to be retweeted by Internet users. In the topic category
of crime, true information wasmore likely to be retweeted by
network users than misinformation. In terms of the number
of favorites, true information under the topics of food and
product, and crime was more likely to be collected by Internet
users. In terms of the number of comments, misin-
formation under the topic of food and product was more
likely to be discussed by Internet users. In the topic category
of crime, true information was more likely to be discussed by
Internet users than misinformation.
In the topic category under happiness emotion, misinformation
under the topics of food and product, and crime
was more likely to be retweeted in social networks. In the
topic category of politics, true information was more likely
to be retweeted on the network than misinformation. In
terms of the number of favorites, true information under the
topics of food and product, politics, and crime was more
likely to be collected by Internet users. In terms of the
number of comments, misinformation under the topics of
food and product, politics, and crime was more likely to be
discussed by Internet users.
In the topic category under fear emotion, misinformation
in the topic categories of food and product, and crime
was more likely to be retweeted in social networks than true
information. In the topic category of public security, true
information was more likely to be retweeted over the network
than misinformation. In terms of the number of fa-
vorites, true information under the topics of public security
and food and product was more likely to be collected by
Internet users, while misinformation under crime was more
likely to be collected by Internet users. Among them, the
information containing the topic of food and product
(P � 0.03< 0.05) had statistical significance in the correlation
between veracity and the number of favorites. In terms
of the number of comments, misinformation wasmore likely
to arouse users’ discussion in social networks in the topic
categories of food and product, and crime. In the topic
category of public security, true information was more likely
to be discussed by users on the network.
In the topic category under sadness, except for the topic
category of celebrity where there was no significant correlation,
misinformation in the rest of the topic categories was
more likely to be retweeted on the network than true information.
Among them, information containing the topic
of food and product (P � 0.02< 0.05) had statistical significance
in the correlation between veracity and the number
of retweets. In terms of the number of favorites, true information
under the topic of food and product was more
likely to be collected by network users, while misinformation
under the topics of public security, politics, crime, disaster,
and social events was more likely to be collected by network
users. Among them, information containing the topic of
food and product (P � 0.004< 0.01) had statistical significance
in the correlation between veracity and the number of
favorites. In terms of the number of comments, except for
the topic of celebrity where there was no obvious correlation,
misinformation under other topics was more likely to be
discussed by users on the Internet than true information.
In the topic category under anger, misinformation in the
topic of food and product was more likely to be retweeted
and commented on the network than true information. True
information on the topic of food and product was more
likely to be collected by Internet users. +ere was no significant
correlation among the rest of the topic categories.
To sum up, by comparing the correlation between information
veracity and network diffusion characteristics of
each topic under different emotions, we can further provide
corresponding suggestions for public opinion governance
and prevention assessment of misinformation. When misinformation
spreads in social networks, its threat can be
effectively reduced by guiding the discussion content and
emotions, and the governance goal of maintaining network
order and social stability can be achieved.
5. Conclusions
As an important channel of information dissemination,
social media has not only become an important platform for
the communication of true information but has also contributed
to the spread of misinformation in social networks.
On the one hand, the spread of misinformation on social
media misleads the public and prompts them to make wrong
decisions. On the other hand, it also brings about great
threats to the public’s physical and mental health and
economic properties. +erefore, it is important to understand
the diffusion characteristics of misinformation on
social media. It can not only provide reference and basis for
the governance of misinformation in social media but also
maintain the order of network security. And, it is also
possible to prevent and control misinformation in advance
by understanding the diffusion rules of misinformation, to
reduce the spread of misinformation on social media more
effectively, and thereby reduce the harm caused by misinformation
from the source. In this paper, we took the
misinformation spread on social media as the research
object, used the deep learning method to analyze the content
20 Security and Communication Networks
#
#characteristics and emotional characteristics of misinformation,
and combined it with the network analysis method
to make a targeted analysis of the network diffusion characteristics
of misinformation in the social network. In ad-
dition, the study also introduced corresponding true
information for comparative study on the basis of the
analysis of misinformation to further reveal the propagation
law of misinformation in social networks.
Related studies have found different characteristics of
misinformation and true information dissemination in
social networks. In content analysis, there were differences
in the network distribution and diffusion characteristics of
misinformation and true information of different topics. In
emotion analysis, we also found that emotion was an
important factor influencing the spread of misinformation,
and the process of spread of misinformation in social
networks showed different changes due to the influence of
different emotions. On the one hand, these findings supplement
the research content with misinformation as the
object and broaden the research boundary of misinformation.
On the other hand, they also provide a reference
for the public opinion management of misinformation. By
guiding the public to discuss different topics and contents,
and guiding the public to generate corresponding emotions,
it helps to achieve the management of misinfor-
mation, reduce the harm caused by misinformation, and
maintain the stability of social order. In addition, with the
help of the distribution of topics and emotions of misinformation
on social media, we also have a deeper under-
standing of the propagation rules of misinformation on
social media. And, it also makes a deeper analysis of the
evolution characteristics of misinformation on social media
and the driving factors of the social network to misinformation.
+is provides a basis for us to further explore
the propagation mechanism of misinformation on social
media. Meanwhile, in the future, we will further deepen the
research based on this paper and adopt different datasets
and methods to better reveal the propagation pattern of
misinformation in social networks.
Data Availability
+e data used to support the findings of this study are
available from the corresponding author upon request.
Conflicts of Interest
+e authors declare that they have no conflicts of interest.
Acknowledgments
+is research was funded by the National Natural Science
Foundation of China under Grant no. 71501153, the Innovation
Capability Support Project of Shaanxi Province of
China under Grant no. 2021KRM135, the Research Fund of
Grand +eory and Practical Problem in Philosophy and
Social Science of Shaanxi Province of China under Grant no.
2021ND0221, the Research Fund of the Education Department
of Shaanxi Province of China under Grant no.
20JG020, and the Natural Science Foundation of Shaanxi
Province of China under Grant no. 2019JM-572.
References
[1] S. O. Søe, “Algorithmic detection of misinformation and
disinformation: gricean perspectives,” Journal of Documentation,
vol. 74, no. 2, pp. 309–332, 2017.
[2] L. Guo and Y. Zhang, “Information flow within and across
online media platforms: an agenda-setting analysis of rumor
diffusion on news websites, Weibo, and WeChat in China,”
Journalism Studies, vol. 21, no. 15, pp. 2176–2195, 2020.
[3] L. Li, H. Xia, R. Zhang, and Y. Li, “DDSEIR: a dynamic rumor
spreading model in online social networks,” in Proceedings of
the International Conference on Wireless Algorithms, Systems,
and Applications, pp. 596–604, Springer, Honolulu, HI, USA,
June 2019.
[4] D. M. J. Lazer, M. A. Baum, Y. Benkler et al., “+e science of
fake news,” Science, vol. 359, no. 6380, pp. 1094–1096, 2018.
[5] T. Caulfield, “Pseudoscience and COVID-19 - we’ve had
enough already,” Nature, 2020.
[6] J. Zarocostas, “How to fight an infodemic,” <e Lancet,
vol. 395, no. 10225, p. 676, 2020.
[7] K. M. Malecki, J. A. Keating, and N. Safdar, “Crisis communication
and public perception of COVID-19 risk in the
era of social media,” Clinical Infectious Diseases, vol. 72, no. 4,
pp. 697–702, 2021.
[8] C. M. Pulido, B. Villarejo-Carballido, G. Redondo-Sama, and
A. Gómez, “COVID-19 infodemic: more retweets for sciencebased
information on coronavirus than for false information,”
International Sociology, vol. 35, no. 4, pp. 377–392, 2020.
[9] C. Huimin, J. Sichen, L. Wei et al., “Quantitative analysis on
the communication of COVID-19 related social media rumors,”
Journal of Computer Research and Development,
vol. 58, no. 7, pp. 1366–1384, 2021.
[10] M. S. Granovetter, “+e strength of weak ties,” American
Journal of Sociology, vol. 78, no. 6, pp. 1360–1380, 1973.
[11] L. Zhang, L. Xu, andW. Zhang, “Social media as amplification
station: factors that influence the speed of online public response
to health emergencies,” Asian Journal of Communi-
cation, vol. 27, no. 3, pp. 322–338, 2017.
[12] G. Pennycook and D. G. Rand, “Fighting misinformation on
social media using crowdsourced judgments of news source
quality,” Proceedings of the National Academy of Sciences,
vol. 116, no. 7, pp. 2521–2526, 2019.
[13] A. Ghenai and Y. Mejova, “Fake cures: user-centric modeling
of health misinformation in social media,” Proceedings of the
ACM on human-computer interaction, vol. 2, pp. 1–20, 2018.
[14] J. Shin, L. Jian, K. Driscoll, and F. Bar, “+e diffusion of
misinformation on social media: temporal pattern, message,
and source,” Computers in Human Behavior, vol. 83,
pp. 278–287, 2018.
[15] S. Lewandowsky, U. K. Ecker, C. M. Seifert, N. Schwarz, and
J. Cook, “Misinformation and its correction: continued influence
and successful debiasing,” Psychological Science in the
Public Interest, vol. 13, no. 3, pp. 106–131, 2012.
[16] R. Faris, H. Roberts, B. Etling, N. Bourassa, E. Zuckerman, and
Y. Benkler, “Partisanship, propaganda, and disinformation:
online media and the 2016 US presidential election,” Berkman
Klein Center Research Publication, vol. 6, 2017.
[17] C. J. Vargo, L. Guo, and M. A. Amazeen, “+e agenda-setting
power of fake news: a big data analysis of the online media
landscape from 2014 to 2016,” New Media & Society, vol. 20,
no. 5, pp. 2028–2049, 2018.
Security and Communication Networks 21
#
#[18] A. Rojecki and S. Meraz, “Rumors and factitious informational
blends: the role of the web in speculative politics,” New
Media & Society, vol. 18, no. 1, pp. 25–43, 2016.
[19] S. Zannettou, M. Sirivianos, J. Blackburn, and N. Kourtellis,
“+e web of false information: rumors, fake news, hoaxes,
clickbait, and various other shenanigans,” Journal of Data and
Information Quality (JDIQ), vol. 11, no. 3, pp. 1–37, 2019.
[20] T. Xiaorui, W. Danchen, and D. Anbang, “Research on
dissemination mechanism of public crisis information under
the influence of psychological stress,” Library and Information
Service, vol. 58, no. 02, pp. 59–65, 2014.
[21] M. T. +ai, W. Wu, and H. Xiong, Big Data in Complex and
Social Networks, CRC Press, Boca Raton, FL, United States,
2016.
[22] G. Pennycook, Z. Epstein, M. Mosleh, A. A. Arechar,
D. Eckles, and D. G. Rand, “Shifting attention to accuracy can
reduce misinformation online,” Nature, vol. 592, no. 7855,
pp. 590–595, 2021.
[23] A. Zubiaga, M. Liakata, R. Procter, G. Wong Sak Hoi, and
P. Tolmie, “Analysing how people orient to and spread rumours
in social media by looking at conversational threads,”
PloS one, vol. 11, no. 3, Article ID e0150989, 2016.
[24] J. J. Van Bavel and A. Pereira, “+e partisan brain: an identitybased
model of political belief,” Trends in Cognitive Sciences,
vol. 22, no. 3, pp. 213–224, 2018.
[25] H. Allcott and M. Gentzkow, “Social media and fake news in
the 2016 election,” <e Journal of Economic Perspectives,
vol. 31, no. 2, pp. 211–236, 2017.
[26] G. Pennycook and D. G. Rand, “Lazy, not biased: susceptibility
to partisan fake news is better explained by lack of
reasoning than by motivated reasoning,” Cognition, vol. 188,
pp. 39–50, 2019.
[27] N. Grinberg, K. Joseph, L. Friedland, B. Swire-+ompson, and
D. Lazer, “Fake news on Twitter during the 2016 US presidential
election,” Science, vol. 363, no. 6425, pp. 374–378,
2019.
[28] A. M. Guess, M. Lerner, B. Lyons et al., “A digital media
literacy intervention increases discernment between mainstream
and false news in the United States and India,” Pro-
ceedings of the National Academy of Sciences, vol. 117, no. 27,
pp. 15536–15545, 2020.
[29] S. M. Jones-Jang, T. Mortensen, and J. Liu, “Does media
literacy help identification of fake news? Information literacy
helps, but other literacies don’t,” American Behavioral Scientist,
vol. 65, no. 2, pp. 371–388, 2021.
[30] Y. Zhao, J. Da, and J. Yan, “Detecting health misinformation
in online health communities: incorporating behavioral features
into machine learning based approaches,” Information
Processing & Management, vol. 58, no. 1, Article ID 102390,
2021.
[31] T. K. Sell, D. Hosangadi, and M. Trotochaud, “Misinformation
and the US Ebola communication crisis: analyzing the
veracity and content of social mediamessages related to a fearinducing
infectious disease outbreak,” BMC Public Health,
vol. 20, no. 1, pp. 1–10, 2020.
[32] J. S. Brennen, F. Simon, P. N. Howard, and R. K. Nielsen,
“Types, sources, and claims of COVID-19 misinformation,”
2020, https://reutersinstitute.politics.ox.ac.uk/types-sourcesand-claims-covid-19-misinformation.
[33] L. Chen, X. Wang, and T.-Q. Peng, “Nature and diffusion of
gynecologic cancer–related misinformation on social media:
analysis of tweets,” Journal of Medical Internet Research,
vol. 20, no. 10, Article ID e11515, 2018.
[34] S. Vosoughi, D. Roy, and S. Aral, “+e spread of true and false
news online,” Science, vol. 359, no. 6380, pp. 1146–1151, 2018.
[35] E. Hollowood and A. Mostrous, “Fake news in the time of
C-19,” 2020, https://members.tortoisemedia.com/2020/03/
23/the-infodemic-fake-news-coronavirus/content.html.
[36] S. M. Jang, T. Geng, J.-Y. Q. Li et al., “A computational
approach for examining the roots and spreading patterns of
fake news: evolution tree analysis,” Computers in Human
Behavior, vol. 84, pp. 103–113, 2018.
[37] H. Ahmed, “Detecting opinion spam and fake news using
n-gram analysis and semantic similarity,” 2017, https://
dspace.library.uvic.ca//handle/1828/8796.
[38] Y. Zhao and J. Zhang, “Consumer health information seeking
in social media: a literature review,” Health Information and
Libraries Journal, vol. 34, no. 4, pp. 268–283, 2017.
[39] R. Ma, “Spread of SARS and war-related rumors through new
media in China,” Communication Quarterly, vol. 56, no. 4,
pp. 376–391, 2008.
[40] O. Oh, M. Agrawal, and H. R. Rao, “Community intelligence
and social media services: a rumor theoretic analysis of tweets
during social crises,” MIS Quarterly, vol. 37, pp. 407–426,
2013.
[41] F. Zollo, P. K. Novak, M. Del Vicario et al., “Emotional
dynamics in the age of misinformation,” PloS one, vol. 10,
no. 9, Article ID e0138740, 2015.
[42] K. Klimiuk, A. Czoska, K. Biernacka, and Ł. Balwicki,
“Vaccine misinformation on social media–topic-based content
and sentiment analysis of Polish vaccine-deniers’ com-
ments on Facebook,” Human Vaccines &
Immunotherapeutics, vol. 17, no. 7, pp. 2026–2035, 2021.
[43] Y. Leng, Y. Zhai, S. Sun et al., “Analysis of misinformation
during the COVID-19 outbreak in china: cultural, social and
political ntanglements,” 2020, https://arxiv.org/abs/2005.
10414.
[44] E. Kušen and M. Strembeck, “Politics, sentiments, and misinformation:
an analysis of the Twitter discussion on the 2016
austrian presidential elections,” Online Social Networks and
Media, vol. 5, pp. 37–50, 2018.
[45] K. K. King and B. Wang, “Diffusion of real versus misinformation
during a crisis event: a big data-driven approach,”
International Journal of Information Management, Article ID
102390, 2021.
[46] L. H. X. Ng and J. Y. Loke, “Analyzing public opinion and
misinformation in a COVID-19 telegram group chat,” IEEE
Internet Computing, vol. 25, no. 2, pp. 84–91, 2020.
[47] M. Saqr and A. Alamro, “+e role of social network analysis as
a learning analytics tool in online problem based learning,”
BMC Medical Education, vol. 19, no. 1, pp. 1–11, 2019.
[48] S. P. Borgatti, A. Mehra, D. J. Brass, and G. Labianca,
“Network analysis in the social sciences,” Science, vol. 323,
no. 5916, pp. 892–895, 2009.
[49] L. F. Bringmann, T. Elmer, S. Epskamp et al., “What do
centrality measures measure in psychological networks?”
Journal of Abnormal Psychology, vol. 128, no. 8, pp. 892–903,
2019.
[50] D. Camacho, Á. Panizo-LLedot, G. Bello-Orgaz, A. GonzalezPardo,
and E. Cambria, “+e four dimensions of social net-
work analysis: an overview of research methods, applications,
and software tools,” Information Fusion, vol. 63, pp. 88–120,
2020.
[51] P. Wang, B. Xu, Y. Wu, and X. Zhou, “Link prediction in
social networks: the state-of-the-art,” Science China Information
Sciences, vol. 58, no. 1, pp. 1–38, 2015.
22 Security and Communication Networks
#
#[52] R. Pastor-Satorras, C. Castellano, P. Van Mieghem, and
A. Vespignani, “Epidemic processes in complex networks,”
Reviews of Modern Physics, vol. 87, no. 3, pp. 925–979, 2015.
[53] M. Liu, B. Lang, Z. Gu, and A. Zeeshan, “Measuring similarity
of academic articles with semantic profile and joint word
embedding,” Tsinghua Science and Technology, vol. 22, no. 6,
pp. 619–632, 2017.
[54] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient
estimation of word representations in vector space,” 2013,
https://arxiv.org/abs/1301.3781.
[55] B. Liu, “Text sentiment analysis based on CBOW model and
deep learning in big data environment,” Journal of Ambient
Intelligence and Humanized Computing, vol. 11, no. 2,
pp. 451–458, 2020.
[56] T. M. Kodinariya and P. R. Makwana, “Review on determining
number of cluster in K-means clustering,” Interna-
tional Journal, vol. 1, no. 6, pp. 90–95, 2013.
[57] N. K. Nguyen, A.-C. Le, and H. T. Pham, Deep Bi-directional
Long Short-Term Memory Neural Networks for Sentiment
Analysis of Social Data, International Symposium on Integrated
Uncertainty in Knowledge Modelling and Decision
Making, Springer, New York, NY, USA, 2016.
[58] V. Loia and S. Senatore, “A fuzzy-oriented sentic analysis to
capture the human emotion in Web-based content,”
Knowledge-Based Systems, vol. 58, pp. 75–85, 2014.
[59] S. Poria, H. Peng, A. Hussain, N. Howard, and E. Cambria,
“Ensemble application of convolutional neural networks and
multiple kernel learning for multimodal sentiment analysis,”
Neurocomputing, vol. 261, pp. 217–230, 2017.
[60] F. Luo, C. Li, and Z. Cao, “Affective-feature-based sentiment
analysis using SVM classifier,” in Proceedings of the 2016 IEEE
20th International Conference on Computer Supported Cooperative
Work in Design (CSCWD), pp. 276–281, IEEE,
Nanchang, China, May 2016.
[61] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,”Nature,
vol. 521, no. 7553, pp. 436–444, 2015.
[62] G. Xu, D. Zhou, and J. Liu, “Social network spam detection
based on ALBERT and combination of Bi-LSTM with selfattention,”
Security and Communication Networks, vol. 2021,
Article ID 5567991, 11 pages, 2021.
[63] Y. Lai, L. Zhang, D. Han, R. Zhou, and G. Wang, “Finegrained
emotion classification of Chinese microblogs based
on graph convolution networks,” World Wide Web, vol. 23,
no. 5, pp. 2771–2787, 2020.
[64] K. Cheng, Y. Yue, and Z. Song, “Sentiment classification based
on Part-of-Speech and self-attention mechanism,” IEEE Access,
vol. 8, pp. 16387–16396, 2020.
[65] J. Ma, W. Gao, P. Mitra et al., “Detecting rumors from
microblogs with recurrent neural networks,” in Proceedings of
the Twenty-Fifth International Joint Conference on Artificial
Intelligence, pp. 3818–3824, AAAI Press, New York, NY, USA,
July 2016.
[66] P. Liu and L. Ma, “Food scandals, media exposure, and citizens’
safety concerns: a multilevel analysis across Chinese
cities,” Food Policy, vol. 63, pp. 102–111, 2016.
Security and Communication Networks 23
RESEARCH ARTICLE
The role of artificial intelligence in disinformation
Noémi Bontridder* and Yves Poullet
Research Centre in Information, Law and Society, University of Namur, Namur, Belgium
*Corresponding author. E-mail: noemi.bontridder@unamur.be
Received: 02 March 2021; Revised: 04 July 2021; Accepted: 19 July 2021
Key words: artificial intelligence; content regulation; digital ecosystem; disinformation; online platforms
Abbreviations: AI, artificial intelligence; EU, European Union; ICT, information and communication technologies.
Abstract
Artificial intelligence (AI) systems are playing an overarching role in the disinformation phenomenon our
world is currently facing. Such systems boost the problem not only by increasing opportunities to create
realistic AI-generated fake content, but also, and essentially, by facilitating the dissemination of disinformation
to a targeted audience and at scale by malicious stakeholders. This situation entails multiple ethical
and human rights concerns, in particular regarding human dignity, autonomy, democracy, and peace. In
reaction, other AI systems are developed to detect and moderate disinformation online. Such systems do not
escape from ethical and human rights concerns either, especially regarding freedom of expression and
information. Having originally started with ascending co-regulation, the European Union (EU) is now
heading toward descending co-regulation of the phenomenon. In particular, the Digital Services Act proposal
provides for transparency obligations and external audit for very large online platforms’ recommender
systems and content moderation. While with this proposal, the Commission focusses on the regulation of
content considered as problematic, the EU Parliament and the EU Council call for enhancing access to
trustworthy content. In light of our study, we stress that the disinformation problem is mainly caused by the
business model of the web that is based on advertising revenues, and that adapting this model would reduce
the problem considerably. We also observe that while AI systems are inappropriate to moderate disinformation
content online, and even to detect such content, they may be more appropriate to counter the
manipulation of the digital ecosystem.
Policy Significance Statement
This study aims at identifying the right approach to tackle the disinformation problem online with due
consideration for ethical values, fundamental rights and freedoms, and democracy. While moderating content
as such and using AI systems to that end may be particularly problematic regarding freedom of expression and
information, we recommend countering the malicious use of technologies online to manipulate individuals. As
considering the main cause of the effective manipulation of individuals online is paramount, the business model
of the web should be on the radar screen of public regulation more than content moderation. Furthermore, we do
support a vibrant, independent, and pluralistic media landscape with investigative journalists following ethical
rules.
©TheAuthor(s), 2021. Published byCambridgeUniversity Press. This is anOpenAccess article, distributed under the terms of the Creative Commons
Attribution licence (http://creativecommons.org/licenses/by/4.0), which permits unrestricted re-use, distribution and reproduction, provided the
original article is properly cited.
Data & Policy (2021), 3: e32
doi:10.1017/dap.2021.20
#
#1. Introduction
Manipulation of truth is a recurring phenomenon throughout history.1Damnatio memoriae, namely the
attempted erasure of people from history, is an example of purposive distortion of reality that was
already practiced in Ancient Egypt. Nevertheless, owing to the rapid advances in information and
communication technologies (ICT) as well as their increasing pervasiveness, disingenuous information
can now be produced easily and in a realistic format, and its dissemination to a targeted audience occurs
at an unparalleled speed and scale, including through artificial intelligence (AI) techniques. The
consequences are serious with far-reaching implications. For instance, the media ecosystem has been
leveraged to influence citizens’ opinion and voting decisions related to the 2016 US presidential
election2 and the 2016 UK referendum on leaving the European Union (EU) (Howard and Kollanyi,
2016). In Myanmar, Facebook has been a useful instrument for those seeking to spread hate against
RohingyaMuslims (Human Rights Council, 2018, para 74).3 In India, rumors onWhatsApp resulted in
several murders (Dixit and Mac, 2018). In France, a virulent online campaign on social media against a
professor ended up with him being murdered (Bindner and Gluck, 2020). Conspiracy theories are
currently prospering.4 And presently in the context of the Covid-19, we are facing what has been called
an infodemic5 by the World Health Organization (WHO), with multiple adverse effects on individuals
and society at large.
As commonly understood, disinformation is false, inaccurate or misleading information that is
shared with the intent to deceive the recipient,6 as opposed to misinformation that refers to false,
inaccurate, or misleading information that is shared without any intent to deceive. Whereas new digital
technology and social media have amplified the creation and spread of both mis- and disinformation,
only disinformation has been considered by the EU institutions as a threat that must be tackled by
legislative and technical means.7 This choice of focus has to do with the manipulative character of
disinformation, along with the importance of protecting fundamental rights and freedoms, especially
freedom of expression and information.8 Indeed, if anyone or any entity was allowed to decide whose
1 For an historical review of the phenomenon in the EU, see Mork (2020).
2 For an in-depth study on the disinformation phenomenon in the US, see Lance Bennett and Livingston (2020). During the 2016
US election campaign, not only has Cambridge Analytica played an important role in influencing voter’s decisions, but also the
writing of sensationalist stories to earn money from advertising. See for instance the example of the “fake news” farm inMacedonia,
reported by Kirby (2016).
3 The report indicates that “[t]he extent to which Facebook posts andmessages have led to real-world discrimination and violence
must be independently and thoroughly examined.”
As a contributing vector of the possibly large extent to which Facebook was used to spread hate effectively, it is important to note
that in Myanmar, Facebook is considered as the internet for most people. This situation is the result of various factors: the social
network is already installed when buying a phone; in 2016 it released its “Free Basics” program that offers users creating a Facebook
account free access to some pre-selected websites, with the aim to conquer the developing countries; and since people in Myanmar
have been silenced for long, they may have particularly reveled in being able to express themselves and share information on social
media. These elements were discussed in an article of The Economist (2020a). See also the analysis of the situation conducted by the
anthropologist Christina Fink, such as exposed in her article, Fink (2018).
4 QAnon is an example of such conspiracy theory, see Hannah (2021).
5 As defined by the WHO, “infodemics are an excessive amount of information about a problem, which makes it difficult to
identify a solution. They can spreadmisinformation, disinformation, and rumors during a health emergency. Infodemics can hamper
an effective public health response and create confusion and distrust among people.” WHO (2020a). See also WHO (2020b).
6 The European Commission defines disinformation as “verifiably false or misleading information that is created, presented, and
disseminated for economic gain or to intentionally deceive the public, and may cause public harm. Public harm comprises threats to
democratic political and policy-making processes as well as public goods such as the protection of EU citizens’ health, the
environment or security. Disinformation does not include reporting errors, satire and parody, or clearly identified partisan news and
commentary.” European Commission (2018d, pp. 3–4). By disseminating disinformation online, malicious stakeholders may for
instance seek to discredit political leaders, tarnish the reputation of a competing firm, or negate facts of public interest with
antidemocratic purposes.
7 See for instance European Commission (2018b). It will be revised and strengthened as announced in the European Democracy
Action Plan, which addresses the fight against disinformation as one of its three pillars. See European Commission (2020b).
8 Freedom of expression and freedom to receive information are set out in article 19 of theUniversal Declaration ofHumanRights
(adopted 10 December 1948) UNGA Res 217 A(III) (UDHR). They are protected in Europe by article 10 of the Convention for the
e32-2 Noémi Bontridder and Yves Poullet
#
#truth should be considered as false and was enabled to regulate content accordingly, freedom of
expression and information would be seriously impaired. The disinformation problem is particular in
the sense that, firstly, the shared information is intentionally deceptive to manipulate people and,
secondly, for achieving his or her goal, its author takes benefit from the modern techniques of
communication and information. For these reasons, our analysis stays on the beaten path, hence the
title of this article referring solely to the disinformation problem. It is also worth specifying that unlike
“fake news,” a term that has been used by politicians and their supporters to dismiss coverage that they
find disagreeable, the disinformation problem encompasses various fabricated information and practices
going beyond anything resembling “news.”9
As indicated, advances in ICT have changed the way information can be produced and disseminated.
What must be noted is the decisive role of AI techniques used in this field. Not only do they facilitate the
creation and dissemination of disinformation bymalicious stakeholders, they are also used contrariwise to
tackle disinformation online. In the present study, we first analyse the different AI techniques that amplify
the disinformation problem. Second, we focus on AI techniques developed in response to this exact same
issue. Ethical implications arise in both cases, which we consider respectively. Third, we discuss the EU
regulation of the phenomenon, which started with ascending co-regulation but is presently heading
toward descending co-regulation. And finally, we conclude our study and recommend future directions to
address the problem ethically, with due consideration for fundamental rights and freedoms.
2. AI Techniques Boost the Creation and Dissemination of Disinformation
AI techniques boost the disinformation phenomenon online in two ways. First, AI techniques are
generating new opportunities to create or manipulate texts and image, audio or video content. Second,
AI systems developed and deployed by online platforms to enhance their users’ engagement significantly
contribute to the effective and rapid dissemination of disinformation online. These latter techniques
constitute the main contributing factor of the problem. Multiple ethical implications arise from this
situation, which should be thoroughly examined.
2.1. AI techniques facilitate the creation of fake content
When AI techniques are used to create fake content, the product is called a deepfake. As highlighted in a
report dealing with technology-enabled disinformation, “[f]alse media has existed for as long as there has
been media to falsify: forgers have faked documents or works of art, teenagers have faked driver’s
licenses, etc. With the advent of digital media, the problem has been amplified, with tools like Photoshop
making it easy for relatively unskilled actors to perform sophisticated alterations to photographs” (Akers
et al., 2018, p. 4).More recently, developments inAI have further expanded the possibilities tomanipulate
texts, images, audios and videos, with the two latter types of content becoming increasingly realistic. The
following definition explains clearly what deepfakes are:
Deepfakes (a portmanteau of deep learning and fake) are the product of two AI algorithms working
together in a so-called Generative Adversarial Network (GAN). GANs are best described as a way
to algorithmically generate new types of data from existing datasets. For example, a GAN could
analyse thousands of pictures of Donald Trump and then generate a new picture that is similar to the
analysed images but not an exact copy of any of them. This technology can be applied to various
types of content—images, moving images, sound, and text. The term deepfake is primarily used for
audio and video content (Walorska, 2020, p. 9).
Protection of Human Rights and Fundamental Freedoms (European Convention on Human Rights, as amended) (ECHR) and in the
EU by article 11 of the Charter of Fundamental Rights of the European Union (CFR).
9 These are the two reasonswhy the EuropeanCommission has decided to avoid the term “fake news.”SeeEuropeanCommission
(2018a, p. 10).
Data & Policy e32-3
#
#While the manipulation of texts and images was already feasible with less sophisticated tools, the
advent of deepfakes renders the creation of fake videos and audios highly accessible as well. Indeed,
“only a few hundred pictures or audio recordings are required as training data to achieve credible
results. For just under $3, anybody can order a fake video of a person of their choice, provided that they
have at least 250 pictures of that person—but this is unlikely to be an obstacle for any person that uses
Instagram or Facebook. Synthetic voice recordings can also be generated for just $10 per 50 words”
(Walorska, 2020, p. 15). Apart from their entertainment value, the involvement of deepfakes in the
media can adversely affect society. Not only do they fuel the spread of false information, they are also
prone to undermine the credibility of legitimate information, creating doubts about any information
encountered, including when it is given by the traditional press or by governmental administrations. In
the disinformation context, it is therefore possible for anyone willing to deceive or mislead individuals,
to manipulate the truth in two effective ways: fake content can be passed off as real and authentic
information can be passed off as fake.
2.2. AI techniques present on the web boost the dissemination of disinformation
To ensure dissemination of disinformation to a maximum of selected people, micro-targeting is
especially effective. Owing to the economic model of the web based on advertising, tracking methods
using AI technologies were developed by information and communication platforms to enable the
targeting of advertisements to specific consumers for the sake of efficiency. The traditional browser
cookie-based tracking or third-party tracking “is the practice bywhich companies embed content—like
advertising networks, social media widgets, and website analytics scripts—in the first party sites that
users visit directly. This embedding relationship allows the third party to re-identify users across
different websites and build a browsing history” (Akers et al., 2018, p. 4). Browser fingerprinting is
another tracking method, “which relies on browser-specific and OS-specific features and quirks to get a
fingerprint of a user’s browser that can be used to correlate the user’s visits across websites” (Akers
et al., 2018, p. 4). As just said, these methods were developed to target advertisements at potential
consumers. In this respect, “[t]he advertiser usually sets the targeting parameters (such as demographics
and presumed interests), but the platform’s algorithmic systems pick the specific individuals who will
see the ad and determine the ad’s placement within the platform” (Maréchal and Biddle, 2020, p. 13).
Tracking methods are now often used to, more broadly, target particular content at each user. Indeed,
according to the economic model adopted by the major online platforms and websites, which is based on
“the economics of attention” (Festré and Garrouste, 2015), the financing of investments is not done
directly by visitors but rather through the remuneration received from the advertising companies.10
Therefore, more time spent by a user on a platform means more economic gain for the latter. Algorithms
are thus recording every action we take online (which can be active or passive) in order to propose content
that will optimize the time we spend using the platform and to propose products we are most likely to
purchase. Therefore, as outlined by the Council of the EU in its recent conclusions on safeguarding a free
and pluralistic media system, algorithms are “affecting the results that users are actively searching for
(findability) and the media content that users are passively exposed to (discoverability)” (Council of the
European Union, 2020, para 21). This is the business model of the web we are usually not completely
aware of when surfing online.
Since data analysis and algorithms are having an increasing impact on the information shown to
individuals, each of us going online sees a different version of reality. Facebook’s News feed, Twitter’s
Timeline, and YouTube’s recommender system are only some examples of content shaping algorithms
that determine what individual users see online (Maréchal and Biddle, 2020, p. 13). We can also mention
10 For instance, in 2020, Google’s advertising revenue amounted to 146.92 billion US dollars. See https://www.statista.com/
statistics/266249/advertising-revenue-of-google/ (accessed 01March 2021). And Facebook’s advertising revenue amounted to 84.2
billion U.S. dollars. See https://www.statista.com/statistics/267031/facebooks-annual-revenue-by-segment/ (accessed 01 March
2021).
e32-4 Noémi Bontridder and Yves Poullet
#
#Netflix’smovie recommender system, Spotify’s recommender system and,more surreptitiously, Google’s
ranking system. Besides privacy concerns raised by the tracking and targeting technologies as well as
concerns regarding people’s autonomy and right to information, this ecosystem can be directly leveraged
by disinformation campaigns to target specific vulnerable users and to create and exploit filter bubbles
(Akers et al., 2018 , p. 5). For instance, there existed until recently on Facebook a pseudoscience interest
category that advertisers could purchase and target (Sankin, 2020), and since AI systems analyse the
unique psychographic and behavioral user profiles, micro-targeting of voters has been boosted by the use
of such technology (Bergamini, 2020, p. 10; Marsden and Meyer, 2019, p. 15).
The design of algorithms based onmicro-targeting can also directly amplify the spread of bothmis- and
disinformation. For example, on YouTube, more than a billion hours of video are viewed every day, 70%
of which by automated systems in order to provide recommendations on what video to watch next for
human users. Since there aremore than two billion users onYouTube, this has a significant impact onwhat
the worldwatches. Yet, as the platform operates for profit, which is optimized by viewing time, the quality
of the recommended content is far from being prioritized. The project AlgoTransparency, which aims at
exposing the impact of the most influential algorithms,11 demonstrates YouTube’s vicious feedback loop.
It can be described as follows: (1) Divisive content performs better! (2) Algorithmic systems promote
this content in order to optimize viewing time ! (3) This kind of content being more viewed, content
creators create more of it.12
The deployment of social bots by malicious stakeholders also contributes to the effective dissemination
of disinformation. These bots (an abbreviation for software robots) are fully or semiautomated user-
accounts operating on social media platforms, which are designed for communication and the imitation of
human online-behavior. There are different types of social bots, ranging “from very simple bots that
automate single elements of the communication process (e.g., liking or sharing), over partially humansteered
accounts with automated elements (so-called hybrid bots, or ‘cyborgs’) to autonomously acting
agents equipped with [AI] and learning skills such asMicrosofts’ Zo1 or Replika.ai” (Assenmacher et al.,
2020). These “artificial speakers,” which are consistently present on social media,13 have a real impact:
they “foment political strife, skew online discourse, and manipulate the marketplace” (Lamo and Calo,
2018). Indeed, such bots can be designed to post context-relevant content based on the community they
are attempting to blend into, and once they have gained a credible profile and seem trustable, they are
capable of disseminating disinformation efficiently (Akers et al., 2018, p. 5).14
2.3. Ethical implications
Disinformation is well a long-standing problem, but given that AI techniques present in the digital
ecosystem create new possibilities to manipulate individuals effectively and at scale, multiple ethical
concerns arise or are exacerbated. These should be carefully considered.
A first ethical value that is challenged by the current digital ecosystem is human dignity.15 Following
this fundamental value, “human beings are to be understood as ends in themselves and never as a means
alone” (EDPS Ethics Advisory Group, 2018). Yet, when algorithms are programmed to adapt what is
shown to individuals based on their profile created through datafication16 in order to optimize
11 See https://www.algotransparency.org/ (accessed 01 March 2021).
12 For a presentation of the project AlgoTransparency and its main findings by the founder Guillaume Chaslot, see the following
video: The Toxic Potential of YouTube’s Feedback Loop (CADETech PolicyWorkshop, 17 November 2019). https://www.youtube.
com/watch?v=Et2n0J0OeQ8&feature=youtu.be (accessed 01 March 2021).
13 The average presence of bots was estimated to range between 9 and 15% of all Twitter accounts in 2017, and to amount to
approximately 11% of all Facebook accounts in 2019. See Varol et al. (2017) and Zago et al. (2019).
14 For an analysis of which users may be the target of bots’ activities, see Balestrucci (2020).
15 Human dignity is set out in article 1 of theUniversal Declaration ofHumanRights and in article 2 of the Charter of Fundamental
Rights of the European Union.
16 Following the definition provided on Wikipedia, “Datafication is a technological trend turning many aspects of our life into
data which is subsequently transferred into information realised as a new form of value.” https://en.wikipedia.org/wiki/Datafication.
Data & Policy e32-5
#
#engagement, regardless of the content’s quality, those individuals are considered as mere means for
economic purposes. Furthermore, those same algorithms can be leveraged by malicious stakeholders to
effectively manipulate their opinion and to reach specific goals thereupon. As explained by the EDPS
Ethics Advisory Group, “[w]hen individuals are treated not as persons but as mere temporary aggregates
of data processed at an industrial scale so as to optimize through algorithmic profiling, administrative,
financial, educational, judicial, commercial, and other interactions with them, they are arguably, not fully
respected, neither in their dignity nor in their humanity” (EDPS Ethics Advisory Group, 2018, p. 17). In
addition, AI techniques present in the digital ecosystem change reality in most cases unbeknownst to the
individuals, expanding opportunities for effective manipulation of their opinion. Indeed, targeted
individuals are rarely aware of the current digital ecosystem, and they usually think that the (dis)
information they see online is objective and universally encountered by other users.
Secondly, the difficulty to access information alongside the pervasiveness of disinformation online
drastically impairs the individuals’ capacity to make free and informed decisions, which is an essential
prerequisite for their autonomy (Poullet, 2020a). The value of autonomy “refers to the capacity of
individuals to construct their own identity, to determine their own ‘good’, their own vision of a good
life in respect of others’ similar capacity, and therefore to contribute fully to collective deliberation” (Free
translation; Poullet, 2020b, p. 93). As observed by the European Court of Human Rights in the Pretty
case,17 this notion of personal autonomy underlies the right to privacy.18 As algorithms make use of
personal data to determine the content that will be shown to each individual and distort their capacity to
decide freely, the right to privacy and data protection19 may be violated.20
Information provides individuals with the capacity to make informed decisions by enabling them to
acquaint themselves with facts and societal challenges, and understand those (Hanot and Michel, 2020,
p. 162). It is therefore a key element of individuals’ autonomy. Yet, when individuals encounter realistic
fake content, when they are enclosed unconsciously or consciously in filter bubbles and “echo chambers,”
and when they are the target of disinformation campaigns that leverage the current digital ecosystem to
effectively manipulate their opinion, their access to information is definitely made harder, which impedes
or at least limits their right to information. The decrease of the average level of trust in the news
worldwide,21 to which the invasiveness of disinformation online has participated, also contributes to
the lack of information that individuals have to deal with.
Since citizens’ capacity to make free and informed decisions is impaired by the digital ecosystem, their
ability to participate in the democratic discourse is also affected. Yet, when individuals’ capacity to
participate fully in public debate is impaired, including through the effectivemanipulation of their opinion
and voting decisions, democracy is seriously jeopardized. Democracy indeed “implies that people with
different views come together to find common solutions through dialogue” (Bergamini, 2020, p. 15).
It is important to note that in order to form their own opinion, individuals need to access diversified
information, namely to be informed in a contradictory way. Receiving information only oriented toward
one point of view is not enough. Alongside disinformation campaigns, echo chambers therefore play a key
role in the threat toward the democratic process. They are generated in consequence of the filter bubbles
created bymicro-targetingmethods, namely content shaping algorithms. An echo chamber can be defined
as “an environment in which individuals encounter only beliefs or opinions that coincide with their own,
17 ECtHR, Pretty v. United Kingdom (Judgment) [2002] Application n°2346/02, para 61.
18 The right to privacy is set out in article 8 of the European Convention on Human Rights and in article 7 of the Charter of
Fundamental Rights of the European Union.
19 The right to protection of personal data is included in the right to privacy, but it is also explicitly set out in article 8 of the Charter
of Fundamental Rights of the European Union. In the EU, the GDPR sets out rules aimed at ensuring this right. Regulation
(EU) 2016/679 of the European Parliament and of the Council on the protection of natural persons with regard to the processing of
personal data and on the free movement of such data, and repealing Directive 95/46/EC [2016] OJ L 119/1 (General Data Protection
Regulation).
20 For further developments regarding the right to privacy and data protection as guarantees of the individuals’ autonomy, see
Poullet (2020b, pp. 114–131).
21 For an analysis of the current level of trust in the media, see Newman et al. (2020).
e32-6 Noémi Bontridder and Yves Poullet
#
#so that their existing views are reinforced and alternative ideas are not considered” (Bergamini, 2020,
note 18). Individuals are thus led “into a state of intellectual isolation where there is no place for dialogue
[…] [and,] by prioritizing the news and information which users like, algorithms tend to reinforce their
opinions, tastes and habits, and limiting access to diverging views” (Bergamini, 2020, p. 10). As outlined
by Vincent De Coorebyter, “if we define democracy as a mechanism of construction of compromises
taking account of a large diversity of opinions and demagogy as a method to flatter predetermined ideas,
we can fear that the Internet fosters the secondmore than the first” (free translation; de Coorebyter, 2020).
In 2001, Tim Berners-Lee had already expressed similar concerns regarding the web’s evolution to a
journalist, who reported the related part of the interview as follows: “Berners-Lee, standing at a
blackboard, draws a graph, as he’s prone to do. It arrays social groups by size. Families, workplace
groups, schools, towns, companies, the nation, the planet. The Web could in theory make things work
smoothly at all of these levels, as well as between them. That, indeed, was the original idea—an organic
expanse of collaboration. But the Web can pull the other way. And Berners-Lee worries about whether it
will ‘allow cranks and nut cases to find in the world 20 or 30 other cranks and nut cases who are absolutely
convinced of the same things. Allow them to set up filters around themselves… and develop a pothole of
culture out of which they cannot climb.’Will we ‘end up with a world which is full of very, very disparate
cultures which do not talk to each other?’” (Wright, 2001). In 2019, he added that “we have to make sure
that the web is serving humanity. Not just by keeping it free and open, but by making sure that the things
that people build in this permissionless space are actually helping democracy” (Perrigo, 2019).
Enclosing individuals in filter bubbles and echo chambers is definitely dangerous for peace, since
living together requires knowledge of and tolerance for different views and cultures. As individuals
increasingly interact solely with groups of people with their own views, sometimes inflated by social bots,
and are manipulated with a biased version of reality, their ability to accept the presence of other cultures
and to understand them is made harder. This situation nurtures radicalisation, thus a lack of tolerance for
each other essential for peace and stability. Mahatma Gandhi, well-known for his philosophy of
nonviolence, was giving attention to openness as he said: “I do not want my house to be walled in on
all sides and my windows to be stuffed. I want the cultures of all lands to be blown about my house as
freely as possible.”22
3. AI Techniques As a Way to Tackle Disinformation Online
In reaction to the alarming consequences of disinformation online, social media platforms and search
engines are increasingly requested to respond to the phenomenon, especially now in the context of the
“infodemic.”23 Consequently, various technological methods are being developed. In this regard, AI
techniques are explored both to detect false, inaccurate ormisleading content, and to regulate such content
online. An important point to consider from the outset is the inability, or inappropriateness, of AI systems
to differentiate misinformation from disinformation, which is particularly problematic regarding freedom
of expression and information, as we explain infra.
3.1. AI techniques are developed to detect disinformation online
To detect articles containing false information, which are not necessarily deepfakes, the already
mentioned technical report dealing with technology-enabled disinformation presents and analyses four
techniques (Akers et al., 2018, p. 6). First, it is possible with machine learning to train end-to-end
models using labeled data, namely articles containing false information and articles containing accurate
information. The system is then able to directly differentiate between these two types of articles.
However, besides the large amounts of labeled data necessary for such a model to be operational, which
can be tricky to obtain, its output would lack explainability and would be affected by biases in the
22Mahatma Gandhi, Young India, 1921.
23 See infra. The term “infodemic” is defined in note 5.
Data & Policy e32-7
#
#datasets. Second, the detection of factual inaccuracies may be more effective. It can be done by
comparing the content of the article with external evidence, but this task is better performed by human
fact checkers because the nuances of natural language are difficult to formalize. It can nonetheless also
be done by verifying if a claim is backed up by given sources, or by using unstructured external web
information. Third, the detection of misleading style—inferring the intent of an article by analyzing its
style—can be done by trained human experts or through machine learning. However, the presence or
absence of misleading style is not always correlated with the inaccuracy or veracity of the provided
information. Fourth, the analysis of metadata (e.g., the sharer’s profile or attributes …) instead of the
article’s content as such is also investigated.
As highlighted by Chris Marsden and Trisha Meyer, textual analysis programs trained to identify
potential disinformation material are prone to false negatives or positives “due to the difficulty of parsing
multiple, complex and possibly conflictingmeanings emerging from the text” (Marsden andMeyer, 2019,
pp. 1–2). It has also been noted that as of 2018, Facebook’s AI systems did not have enough training data
to be highly effective outside of English and Portuguese (Marsden and Meyer, 2019, p. 17). Accurate
detection of articles containing false, inaccurate or misleading information therefore still requires the
intervention of human agents.
To detect deepfakes in particular, different techniques have been envisaged as well. A first one is to
develop technologies capable of distinguishing between fake content and real content. To this end, it is
possible to use algorithms similar to those which generated the deepfake.24 The use of forensic tools can
also be considered. Researchers observed in 2018 that actors in AI-generated videos did not blink due to
the lack of faces with closed eyes in most training datasets. Technologies looking for abnormal patterns of
eyelid movement were thus developed. However, once this method became public, new deepfake videos
were adapted by featuring blinking people (Kertysova, 2018, p. 71; Walorska, 2020, p. 24). A second
technique that has been envisaged to detect deepfakes is the authentication of content before it spreads: if
images, audios, and videos can be digitally labeled at themoment of their creation, this label could be used
as a reference to comparewith suspected fake content (Kertysova, 2018, p. 71). However, deepfakes could
be created with pre-existing unlabelled content, which would make this approach inefficient in our
opinion. For the record, the “authenticated alibi service” is a third technological approach that would
monitor and store all individual’s locations, movements, and actions in order to prove where each
individual was and what he or she was saying or doing at any given moment (Kertysova, 2018, p. 71).
This approach is particularly undesirable as it would generatemultiple human rights’ violations, including
regarding privacy.
Given the lack of efficient means to detect deepfakes and their potential impact on the legitimacy of
online information, the Partnership on AI (PAI)25 created the AI andMedia Integrity Steering Committee
in late 2019 for developing and advising projects that strengthen mis/disinformation solutions, including
detection of manipulated and synthetic content. Its first project is the Deepfake Detection Challenge,
24 For instance, “[u]sing GLTR, a model based on the GPT-2 system […], researchers from the MIT-IBM Watson AI Lab and
HarvardNLP investigated whether the same technology used to write independently fabricated articles can be used to recognize text
passages that were generated by AI. When a text passage is generated in the test application, its words are highlighted in green,
yellow, red, or purple to indicate their predictability, in decreasing order. The higher the proportion of words with low predictability,
namely sections marked in red and purple, the greater the likelihood that the passage was written by a human author. The more
predictable the words (and the ‘greener’ the text), the more likely the text was automatically generated.” Walorska (2020, p. 24).
25 The Partnership on AI “was formally established in late 2016, led by a group of AI researchers representing six of the world’s
largest technology companies: Apple, Amazon, DeepMind and Google, Facebook, IBM, and Microsoft. In 2017 the addition of six
not-for-profit Board members expanded the Partnership into a multi-stakeholder organization—which now represents a community
of 50þ member organizations.” This multistakeholder organization now “brings together academics, researchers, civil society
organizations, companies building and utilizing AI technology, and other groups working to better understand AI’s impacts. The
Partnership was established to study and formulate best practices on AI technologies, to advance the public’s understanding of AI,
and to serve as an open platform for discussion and engagement about AI and its influences on people and society.” See https://
www.partnershiponai.org/ (accessed 01 March 2021).
e32-8 Noémi Bontridder and Yves Poullet
#
#which aims at promoting the development of technical solutions that detect AI-generated andmanipulated
videos (for more information, see, Partnership on AI, 2020).
To detect social bots, various efforts have been made as well. As exposed in the report dealing with
technology-enabled disinformation, “[d]etection approaches commonly leverage machine learning to
find differences between human users and bot accounts. Detection models make use of the social network
graph structure, account data and posting metrics, as well as natural language processing techniques to
analyze the text content from profiles. Crowdsourcing techniques have also been attempted. From the
platforms provider’s vantage point, a promising approach relies on monitoring account behavior such as
time spent viewing posts and number of friend requests sent” (Akers et al., 2018, p. 5). It can be observed
that AI techniques have been more successful to detect fake accounts, including bot accounts. Indeed,
99.6% of Facebook’s fake accounts actioned in the fourth quarter of 2020 were found and flagged by the
company before users reported them.26
3.2. AI techniques are developed to regulate content online
AI techniques can also be used to aid regulation of content and accounts online. The following methods
are discussed in the report prepared by Marsden and Meyer at the request of the STOA (Marsden and
Meyer, 2019, pp. 38–41). Filtering of content is a measure undertaken by technical providers to prevent
the upload or posting of content, and removal of content is undertaken upon awareness, request, or order.
Filtering or removal of content is a particularly effective method to tackle disinformation, but also the
most invasive one as it prevents content sharing. Blocking of content is a method by which the user’s
access to the content is blocked. However, this method can be circumvented by employing a virtual
private network (VPN). (De)prioritization of content can either be done by the user who opts to see less
content related to particular persons or subjects, or by utilizing automated ranking through algorithms and
network-based solutions. Disabling and suspension of accounts is another method that can be used to
tackle disinformation online. Technology providers take these measures when their users abuse the terms
of service or do not respect legislation. As is well-known and disputable, Twitter and Facebook have for
instance suspended Donald Trump’s account for having violated the platforms’ terms and conditions.27
Other innovative technologies that may prevent disinformation are being developed, such as decentralized
web (Dweb) technologies. Indeed, Dweb technologies “would enable us to break down the
immense databases that are currently held centrally by internet companies rather than users (hence the
decentralized web). In principle, this would also better protect users from private and government
surveillance as data would no longer be stored in a way that was easy for third parties to access. This
actually harks back to the original philosophy behind the internet, which was first created to decentralize
US communications during the Cold War to make them less vulnerable to attack.”28
3.3. Ethical implications
Even if the intended purpose behind the development of AI techniques in response to the ambient
disinformation may be legitimate, their use does not escape from multiple ethical concerns either. Taking
them into account is of paramount importance before deploying such techniques in the digital ecosystem.
26 Facebook, “Community Standards Enforcement Report,” https://transparency.facebook.com/community-standards-enforce
ment#fake-accounts (accessed 01 March 2021).
27 Twitter, “Permanent suspension of @realDonaldTrump” (8 January 2021). https://blog.twitter.com/en_us/topics/company/
2020/suspension.html (accessed 01 March 2021); Facebook, “Referring Former President Trump’s Suspension From Facebook to
the Oversight Board” (21 January 2021). https://about.fb.com/news/2021/01/referring-trump-suspension-to-oversight-board/
(accessed 01 March 2021).
28 Harbinja andKaragiannopoulos (2019); “Whenwe currently access the web, our computers use the HTTP protocol in the form
of web addresses to find information stored at a fixed location, usually on a single server. In contrast, the DWeb would find
information based on its content, meaning it could be stored inmultiple places at once. As a result, this form of the web also involves
all computers providing services as well as accessing them, known as peer-to-peer connectivity.”
Data & Policy e32-9
#
#First, the use of AI systems to detect disinformation material requires an agreed formal definition of
what disinformation encompasses. Yet, defining the problem is not always straightforward (Hanot and
Michel, 2020, pp. 157–161; Poullet, 2020c). and raises questions regarding who is the judge in
determining what is legal or illegal and desirable or undesirable in society. Distinguishing satire,
propaganda, and hoaxes from disinformation may be problematic, and some definitions include these
kinds of content in the disinformation problem.As outlined by the EuropeanCourt ofHumanRights in the
leading caseHandyside v. UnitedKingdom, freedomof expression “is applicable not only to ‘information’
or ‘ideas’ that are favorably received or regarded as inoffensive or as a matter of indifference, but also to
those that offend, shock or disturb the State or any sector of the population. Such are the demands of that
pluralism, tolerance and broadmindedness without which there is no ‘democratic society.’ This means,
amongst other things that every ‘formality,’ ‘condition,’ ‘restriction’ or ‘penalty’ imposed in this sphere
must be proportionate to the legitimate aim pursued.”29 Furthermore, the scope of the problem is
questionable. As a matter of fact, the lack of access to accurate information goes beyond the disinformation
problem. Misinformation also affects society as a whole, and most people sharing content are
unaware of the initial intention of the source’s publisher. However, tackling misinformation would
dramatically limit freedom of expression and information, as we have already mentioned.
Yet, difficulty particularly arises when it comes to distinguish misinformation from disinformation, as
the intent of the sharer needs to be determined to that end. As explained in this section, AI techniques
developed to tackle disinformation online detect all false, inaccurate or misleading information with no
distinction related to the intent of the sharer. Permitting AI systems to regulate content automatically
would therefore seriously affect freedom of expression and information. This is the case even when
humans are involved in the process, since they may rely on the output given by the AI system. We can
pursue our reflection by questioning the possibility to enable AI systems to assess the malicious intent of
the sharer. Well, since the intention of a person creating or spreading content can be unclear, it would be
particularly difficult to enable a machine to assess the intent behind a shared content with the degree of
certainty that the delicate action of moderation should require.
To express false, inaccurate or misleading information is indeed not per se condemnable. What is
reprehensible is by no means related to the quality of the shared content but rather the malicious use of
technology to expand voluntarily false, inaccurate, or misleading information in order to manipulate
people. AI techniques might thus be used precisely to detect this malicious use of technology (e.g., AIgenerated
content, the use of recommender systems to target content at specific individuals with a
malicious intent, the use of social bots) but not to assess the information’s quality. The Council of the
EU indeed concluded that “with regard to the importance of freedom of speech, states and administrative
regulatory authorities as well as private platform providers should abstain from defining quality content or
the reliability of content itself.”30
In addition, Marsden and Meyer underline that “it is not clear how often and under which circumstances
ex ante filtering or blocking take place on the platforms. Some is machine-driven, but it is unclear
how the illegality of the content or its violation with the community guidelines is determined, nor what
safeguards are in place to prevent over-censoring of content.”31 Yet, transparency of content regulation is
primordial in order to assess its legality, especially regarding freedom of expression. Explainability of any
content regulation is therefore required, and the complexity of AI systems’ functioning is problematic in
this regard, as it may lead to opaque outputs.
The Information for All Programme32 of UNESCO had already highlighted a decade ago that while
technologies “can open channels by which information may be shared and opinions expressed; it can also
29 ECtHR, Handyside v. United Kingdom (Judgment) [1976] Application n°5493/72, para 49.
30 Council of the European Union (2020, para 39). It adds that “[t]his should not prevent platforms from promoting public
communications and announcements in case of crisis or emergency situations.”
31 SeeMarsden andMeyer (2019, p. 45). In this regard, we specify that algorithms deployed by platforms can be quite explainable
(machine learning) or, by contrast, quite inexplainable (so-called “black box AI”).
32 https://en.unesco.org/programme/ifap.
e32-10 Noémi Bontridder and Yves Poullet
#
#be used to restrict the information available and to identify and interfere with people expressing alternative
opinions” (Rundle and Conley, 2007, p. 15). They specified that “the right to freedom of opinion and
expression loses valuewithout the ability to communicate one’s views to others. ICTs can be used to create
a public forumwhere this communication can take place, or it can restrict expression by placing limits on a
person’s ability to communicate with others” (Rundle and Conley, 2007, pp. 15–16). Albeit the current
digital ecosystem generates the necessity to place some limits on one’s opportunities to manipulate
individuals, programming algorithms to tackle false, inaccurate ormisleading informationwould limit the
individuals’ ability to express freely their opinion, which is inadmissible, how factually questionable
those opinions may be, regarding the fundamental rights to freedom of expression and information
necessary for democracy and without which we would readily evolve toward a totalitarian society
resembling to the one described by George Orwell in his well-known dystopian novel.
Furthermore, as we have already mentioned, AI systems trained to detect false, inaccurate, or
misleading information are prone to false positives and false negatives. False positives, namely the
wrongful detection of false, inaccurate, or misleading content, affect freedom of expression. Indeed, they
“could lead to over-censorship of legitimate content that is machine-labeled incorrectly as
disinformation” (Marsden andMeyer, 2019, p. 17). On the other hand, false positives and false negatives
would both generate discriminations, therefore impacting equality and nondiscrimination.33 These can be
generated by bias in the algorithms. For instance, we indicated above that Facebook’s algorithms lack
training data outside of English and Portuguese. Therefore, we can assume that content shared in any other
language is less prone to be detected by AI systems. This would generate an asymmetrical approach to the
disinformation problem.
Other concerns emerge when AI techniques are used to automatically regulate content. For instance,
the use of content ranking algorithms can pose problems regarding media pluralism when it relies on the
prioritization of “authoritative sources.” Indeed, such a method makes it particularly difficult for new
brands to emerge, and it can affect the plurality of voices (Marsden and Meyer, 2019, p. 45). These
concerns jibe with the ones exposed above regarding the importance of protecting freedom of expression
and information by abstaining from defining the quality or reliability of content.
While all ethical implications regarding AI techniques developed to tackle disinformation online
must be carefully considered, it is still important to bear in mind that human moderators need to cope
with a very hard job. In May 2020, Facebook agreed to pay 52 million dollars to 11,250 moderators
who had developed post-traumatic stress disorder from looking at the worst of the internet (The
Economist, 2020b). Moreover, human review is prone to error or ambiguous results as well, and biases
also affect the human mind. The right not to be subject to a decision solely based on automated
processing.34 is however relevant in this regard, and AI techniques used to tackle disinformation may
put a publisher in this situation.
In parallel, the disinformation problem goes beyond search engines and platforms. Indeed, instant
messaging platforms are also efficient means to spread disinformation “since they enable the sharing of
content within closed groupswith large numbers of users as well as the transfer of content from one closed
group to another.”35 The murders in India mentioned above illustrate this issue. However, tackling
disinformation in private messages would also raise significant concerns regarding the right to privacy.
33 Equality before the law and the prohibition of discriminations are set out in articles 1 and 7 of the Universal Declaration of
Human Rights, in article 14 of the European Convention on Human Rights and in article 21 of the Charter of Fundamental Rights of
the European Union.
34 This right is protected in the EU under article 22 of the GDPR.
35 European Commission (2020a). The European Regulators Group for Audiovisual Media Services noted that “[e]ven though
theymay be defined as ‘instant messaging tools’more than ‘platforms’,WhatsApp andMessenger allow the users to share content in
closed groups that may contain an enormous amount of people, and offer simple functions to transfer messages/content from one
group to another, thus making the content very easily viral. From this point of view, therefore, instant messaging tools may become
very efficient tools to spread disinformation.” ERGA (2020, p. 44, note 56).
Data & Policy e32-11
#
#4. The EU Regulation of Disinformation
The alarming consequences of the disinformation phenomenon online have prompted the EU institutions
to initiate a regulatory framework to address the problem. That framework first represented a form of
ascending (or “bottom-up”) co-regulation, but it is now being reshaped into descending (or “top-down”)
co-regulation, predominantly through the Digital Services Act proposed by the Commission. While this
new proposal focuses on themoderation of content circulating on online platforms, the EU parliament and
the EU Council advocate for more positive measures aiming at enhancing media literacy and access to
accurate information.
4.1. From ascending co-regulation …
Following a public consultation launched in 2017 revealing that over 97% of nearly 3,000
respondents were considering that they had already faced fake news (European Commission,
2018c), the European Commission set up in January 2018 a high-level group of experts (HLEG)
to advise on policy initiatives to counter fake news and disinformation spread online. The report
delivered by the HLEG (European Commission, 2018a). recommends the adoption of a controlled
self-regulatory approach to address the problem. The European Commission Communication
“Tackling online disinformation: a European approach” (European Commission, 2018d). does not
deviate from this solution, and provides four principles and objectives that should guide action to
tackle disinformation: “First, to improve transparency regarding the origin of information and the
way it is produced, sponsored, disseminated, and targeted in order to enable citizens to assess the
content they access online and to reveal possible attempts to manipulate opinion. Second, to promote
diversity of information, in order to enable citizens to make informed decisions based on critical
thinking, through support to high quality journalism, media literacy, and the rebalancing of the
relation between information creators and distributors. Third, to foster credibility of information by
providing an indication of its trustworthiness, notably with the help of trusted flaggers, and by
improving traceability of information and authentication of influential information providers. Fourth,
to fashion inclusive solutions. Effective long-term solutions require awareness-raising, more media
literacy, broad stakeholder involvement and the cooperation of public authorities, online platforms,
advertisers, trusted flaggers, journalists, and media groups” (European Commission, 2018d, p. 6).
In reaction to this EU position and following strictly the self-regulatory approach, representatives of the
largest online platforms (Google, Facebook, Twitter, and Mozilla) and trade associations of advertisers
presented aCode of Practice onDisinformation. The Commission, which had limited choice for alternative
action, endorsed it in September 2018 (European Commission, 2018b). Microsoft and TikTok have also
signed the code since.36 It gathers commitments that the signatories can individually undertake and will
address in good faith. These commitments relate to: 1° scrutiny of ad placements; 2° identification of
political advertising and of issue-based advertising; 3° integrity of services through policies regarding bots
and the use of automated systems; 4° empowerment of consumers by prioritizing authentic information,
facilitating their evaluation of content via tools like indicators of trustworthiness, improving the findability
of diverse perspectives about topics of public interest, and supporting efforts aimed at improving critical
thinking and media literacy; 5° empowerment of research aimed at understanding the disinformation
phenomenon and its impact. The Code also includes an annex containing best practices that signatories
will apply to implement their commitments. The signatories further commit to publish annually a report on
the different measures they undertake related to their specific commitments. This report must include, inter
alia, indicators providing transparency regarding the scale of the disinformation problem, the submitted
complaints, and the provided solutions.
The Code of Practice on disinformation represents a form of co-regulation that we name “ascending”
since the initiative comes from private actors, the content has been decided by signatories and the execution
36Microsoft signed the Code in May 2019 and TikTok in June 2020.
e32-12 Noémi Bontridder and Yves Poullet
#
#is marginally controlled by public authorities through the review of the report by the Commission. In
September 2018, the Sounding Board of the Multistakeholder Forum on Disinformation, which gathers
representatives of the media, civil society, fact checkers and academia, had already expressed its skepticism
regarding the “so-calledCode of Practice” (SoundingBoard, 2018). Its concerns are nowmostly confirmed,
as the recent assessment of theCode (EuropeanCommission, 2020a). exposes various shortcomings of such
self-regulation: disparity between the reports and the measures undertaken by the signatories; lack of
coverage of the sector; lack of participation of some key platforms (such as WhatsApp); lack of an
independent oversight mechanism and of cooperation mechanisms; lack of access to data necessary to
verify the signatories’ practices; lack of consequences in case of breach; and lack of protection of
fundamental rights, including through mechanisms for redress. This assessment therefore paves the way
for a “descending” co-regulatory approach.
4.2. … Toward descending co-regulation
TheDigital ServicesAct,37 a newproposal for a Regulation presented inDecember 2020 by theCommission,
proposes such a co-regulatory approach to tackle the disinformation problem, providing for a co-regulatory
backstop for the measures that should be included in the revised and strengthened Code of Practice on
Disinformation.38 The revised and strengthened Code of Practice will build on the guidance of the
Commission,39 as announced in the European Democracy Action Plan (European Commission, 2020b).
The declared purpose of the proposal is to enhance platforms’ accountability by requiring them to assume
their responsibility for the actions they take and the systemic risks they pose, in order to build a safe digital
spacewhere fundamental rights of all users of digital services are protected. The proposal provides for specific
mechanisms and measures to tackle illegal content but does not define harmful content that is not illegal and
does not subject it to removal obligations, while further prohibiting general monitoring obligations.40
Nevertheless, and this is of particular interest for our study, some provisions relate to content moderation
as such and to algorithmic systems that shape information flows online, which we specify here below.
Some transparency obligations are imposed on all providers of intermediary services41: they “shall
include information on any restrictions that they impose in relation to the use of their service in respect of
37 European Commission, Proposal for a Regulation of the European Parliament and of the Council on a Single Market For
Digital Services (Digital Services Act) and amending Directive 2000/31/EC (15 December 2020) 2020/0361 (COD).
38 Digital Services Act, Article 35 and recitals 68 and 69.
39 The guidance was published onMay 26, 2021. European Commission (2021). “The Guidance calls for reinforcing the Code of
Practice on Disinformation in the following areas to ensure a complete and consistent application across stakeholders and EU
countries:
– Larger participation with tailored commitments.
– Better demonetising of disinformation.
– Ensuring the integrity of services.
– Improving the empowerment of users.
– Increasing the coverage of fact-checking and providing increased access to data to researchers.
– Creating a more robust monitoring framework.
The signatories of the Code of Practice should convene to strengthen the Code in line with the Commission’s guidance and
present a first draft in autumn.”
We will analyse this new document in another contribution.
40 Digital Services Act, Article 7.
41 “‘Intermediary service’ means one of the following services:
– a ‘mere conduit’ service that consists of the transmission in a communication network of information provided by a recipient
of the service, or the provision of access to a communication network;
– a ‘caching’ service that consists of the transmission in a communication network of information provided by a recipient of the
service, involving the automatic, intermediate and temporary storage of that information, for the sole purpose ofmakingmore
efficient the information’s onward transmission to other recipients upon their request;
– a ‘hosting’ service that consists of the storage of information provided by, and at the request of, a recipient of the service”
Digital Services Act, Article 2(f).
Data & Policy e32-13
#
#information provided by the recipients of the service, in their terms and conditions. That information shall
include information on any policies, procedures, measures and tools used for the purpose of content
moderation, including algorithmic decision-making and human review.”42 They also have to annually
report on any content moderation they engage in.43 The transparency reporting obligations are expanded
for online platforms,44 which must include in their report information on “any use made of automatic
means for the purpose of content moderation, including a specification of the precise purposes, indicators
of the accuracy of the automated means in fulfilling those purposes and any safeguards applied;”45 and
very large online platforms46 see their reporting obligations further expanded.47
Where a hosting service decides to remove or disable access to specific information provided by a
recipient of the service, it must inform the recipient and provide a clear and specific statement of reasons.
That statement contains, “where applicable, information on the use made of automated means in taking the
decision, includingwhere the decisionwas taken in respect of content detected or identified using automated
means,” but also information on the redress possibilities,48 which should always include judicial redress.49
These decisions and statement of reasons must also be published in a publicly available dataset managed by
the Commission.50 Following a decision taken by an online platform (to remove or disable access to
information, to suspend or terminate the provision of the service to a recipient, or to suspend or terminate the
recipient’s account, on the ground that the information provided by the recipient is illegal or incompatible
with their terms and conditions), internal complaint handling systems meeting certain conditions must be
provided for,51 but also possibility of out-of-court dispute settlement by independent certified bodies.52
Regarding advertisingmethods, online platforms have some basic transparency obligations. First, they
“shall ensure that the recipients of the service can identify, for each specific advertisement displayed to
each individual recipient, in a clear and unambiguous manner and in real time: (a) that the information
displayed is an advertisement; (b) the natural or legal person on whose behalf the advertisement is
displayed; and (c) meaningful information about the main parameters used to determine the recipient to
whom the advertisement is displayed.”53 Second, in addition to those, very large online platforms are also
required to ensure public access to repositories containing information on the advertisements they display
on their online interface.54 Such repositories are requested in order to “facilitate supervision and research
into emerging risks brought about by the distribution of advertising online, for example in relation to […]
manipulative techniques and disinformation with a real and foreseeable negative impact on public health,
public security, civil discourse, political participation and equality.”55
Very large online platforms (platforms providing their services tominimum10%of the EU population)
are also required to set out, in their terms and conditions, the main parameters used in their recommender
systems (when they use such systems), in an easily comprehensible manner to ensure that the recipients
42 Digital Services Act, Article 12.
43 Digital Services Act, Article 13.
44 “‘Online platform’ means a provider of a hosting service which, at the request of a recipient of the service, stores and
disseminates to the public information, unless that activity is aminor and purely ancillary feature of another service and, for objective
and technical reasons cannot be used without that other service, and the integration of the feature into the other service is not ameans
to circumvent the applicability of this Regulation.” Digital Services Act, Article 2(h).
45 Digital Services Act, Article 23 §1(c).
46 Very large platforms are platforms providing their services to a number of averagemonthly active recipients of the service in the
EU equal or higher than 10% of the EU population (currently amounting to 45 million). Digital Services Act, Article 25.
47 Digital Services Act, Article 33.
48 Digital Services Act, Article 15 §1–2.
49 Digital Services Act, recital 42.
50 Digital Services Act, Article 15 §4.
51 Digital Services Act, Article 17.
52 Digital Services Act, Article 18.
53 Digital Services Act, Article 24.
54 Digital Services Act, Article 30.
55 Digital Services Act, Recital 63.
e32-14 Noémi Bontridder and Yves Poullet
#
#understand how information is prioritized for them, and to provide the recipients with the option tomodify
or influence such parameters, including at least one option that is not based on profiling.56
Furthermore, risk assessments and mitigation measures are required from very large online platforms.
They are requested to identify, analyse and assess the significant systemic risks stemming from the
functioning and use made of their services in the EU.57 In this regard, some categories of risks must be
assessed in-depth. Among these risks, the regulation proposal underlines the negative effects for the exercise
of the fundamental rights, which “may arise […] in relation to the design of the algorithmic systems used by
the very large online platforms, or the misuse of their service through […] methods for silencing speech or
hampering competition;”58 and, secondly, the intentional manipulation of their service, which “may arise
[…] through the creation of fake accounts, the use of bots, and other automated or partially automated
behaviors, whichmay lead to the rapid andwidespread dissemination of information that is illegal content or
incompatible with an online platform’s terms and conditions.”59 When conducting risk assessments, these
very large online platforms are required to “take into account, in particular, how their content moderation
systems, recommender systems and systems for selecting and displaying advertisement influence any of the
systemic risks […], including the potentially rapid and wide dissemination of illegal content and of
information that is incompatible with their terms and conditions.”60 They must then take measures to
mitigate the identified risks, such as adapting content moderation or recommender systems.61
In order to ensure compliance of very large online platforms with their obligations laid down in the
regulation, such platforms have to appoint compliance officers,62 and provide the Digital Services
Coordinator63 of the establishment or the Commission with access to or reporting of specific data upon
request.64 An independent audit is also foreseen, only for very large online platforms,65 “given the need to
ensure verification by independent experts.”66
We finally outline that the proposal allows the Commission to initiate the drawing up of crisis protocols
“for addressing crisis situations strictly limited to extraordinary circumstances affecting public security or
public health,”67 in order to “coordinate a rapid, collective and cross-border response in the online
environment.”68 It specifies that “[e]xtraordinary circumstances may entail any unforeseeable event, such
as earthquakes, hurricanes, pandemics and other serious cross-border threats to public health, war and acts
of terrorism,where, for example, online platformsmay bemisused for the rapid spread of illegal content or
disinformation or where the need arises for rapid dissemination of reliable information.”69
4.3. The Commission’s omissions regarding important issues
From our analysis of the regulation proposal, we observe three points to which the Commission should
have given more attention.
First, the proposal does not differentiate sufficiently the disinformation problem from the moderation
of illegal content. The adopted approach regarding harmful content that is not per se illegal is questionable:
while there was an agreement amongst stakeholders to not define such content and to not subject it to
removal obligations “as this is a delicate area with severe implications for the protection of freedom of
56Digital Services Act, Article 29 and Recital 62.
57 Digital Services Act, Article 26 §1.
58 Digital Services Act, Recital 57.
59 Digital Services Act, Recital 57.
60 Digital Services Act, Article 26 §2.
61 Digital Services Act, Article 27.
62 Digital Services Act, Article 32.
63 See Digital Services Act, Articles 39 and 41.
64 Digital Services Act, Article 31 and Recital 64.
65 Digital Services Act, Article 28.
66 Digital Services Act, Recital 60.
67 Digital Services Act, Article 37.
68 Digital Services Act, Recital 71.
69 Digital Services Act, Recital 71.
Data & Policy e32-15
#
#expression,”70 providers of intermediary services are allowed to decide what content to moderate as they
can impose restrictions in relation to the use of their service, as long as they inscribe those in their terms
and conditions.71 Then, they have to respect all their other obligations provided for in the proposal’s
provisions we outlined just above, which apply in a nondiscriminatory manner to all content moderation
irrespective of the content’s type. As we have already mentioned, it is for instance because Trump would
have violated Twitter’s and Facebook’s terms and conditions that his accounts were permanently
suspended by the platforms.72 Admittedly, the regulation proposal provides for various transparency
measures and requests the setting up of redress possibilities, but is it admissible to let private companies
decide which content should be moderated, if we consider the overarching importance of freedom of
expression? Facebook itself would prefer to make decisions about whether content is harmful “according
to frameworks agreed by democratically accountable lawmakers.”73 In this context, wewish to clarify that
false, inaccurate or misleading content is not the main element of the disinformation problem. What is
truly and legally problematic is the manipulation of individuals’ opinions, which is achieved cogently by
leveraging the current digital ecosystem, as we demonstrated in Section 2.
Second, the proposal provides for enhanced responsibilities for very large online platforms given their
importance “due to their reach, in particular as expressed in number of recipients of the service, in facilitating
public debate, economic transactions and the dissemination of information, opinions and ideas and in
influencing how recipients obtain and communicate information online,”74 and because such platforms
“may cause societal risks, different in scope and impact from those caused by smaller platforms.”75 As
indicated above, the proposal specifies that such risks may stem from the use of algorithmic systems, the
creation of fake accounts, or the use of automated behaviors such as bots.76 In this context, it appears that
such platforms are providing an essential service: they permit communication and reception of information
online. Such essential service could be considered as a public or, at least, universal service like the service
given by telecommunications’ operators. Indeed, the proposal further acknowledges that such platforms
“are used in away that strongly influences safety online, the shaping of public opinion and discourse, aswell
as on online trade. The way they design their services is generally optimized to benefit their often
advertising-driven business models and can cause societal concerns. In the absence of effective regulation
and enforcement, they can set the rules of the game, without effectively identifying and mitigating the risks
and the societal and economic harm they can cause.”77
Third, wewish to underline that the Commission’s self-conferred possibility to initiate the drawing up of
crisis protocols in case of emergenciesmay be particularly problematic regarding freedom of expression and
freedom of information, if applied. This provision has certainly been included in reaction to the many
problems related to the infodemic stemming from the Covid-19. The infodemic is sure enough problematic,
but what generates this infodemic? The digital ecosystem, again, as we explained in Section 2.2. Certainly,
ensuring that reliable information is always available and evenmore during a time of crisis is legitimate, but
content provided by the government should not silence people’s voices, and we fear that the enforcement of
crisis protocols would have such a consequence. The right approachwould be to tackle the misuse of online
platforms to spread disinformation, suchmisuse being furthermore one of the reasons for theCommission to
include this tendentious provision.78 It is however relevant to note that tackling such misuse and providing
people with reliable information are always necessary, not only in times of crisis.
70 Digital Services Act, p. 9.
71 Digital Services Act, Article 12.
72 Twitter, “Permanent suspension of @realDonaldTrump” (note 27); Facebook, “Referring Former President Trump’s Suspension
From Facebook to the Oversight Board” (note 27).
73 Facebook, “Referring Former President Trump’s Suspension From Facebook to the Oversight Board” (note 27).
74 Digital Services Act, Recital 53.
75 Digital Services Act, Recital 54.
76 Digital Services Act, Recital 57.
77 Digital Services Act, Recital 56.
78 Digital Services Act, Recital 71.
e32-16 Noémi Bontridder and Yves Poullet
#
#4.4. A more positive approach by the EU Parliament and the Council of the EU
On its part, the European Parliament adopted in November 2020 a resolution on strengthening media
freedom: protection of journalists in Europe, hate speech, disinformation and the role of platforms
(European Parliament, 2020). This resolution focuses precisely on the right to freedom of expression
and information and on democracy (European Parliament, 2020, para A), therefore adopting a more
adequate approach to tackle the disinformation problem. It first draws attention to the essential role of
the media and investigative journalists. Accordingly, the Resolution underlines the need to preserve
their freedom and pluralism by, inter alia, maintaining their independence from political or governmental
interference (European Parliament, 2020, paras 6–7); promoting measures aimed at financing
and supporting media and independent journalism (European Parliament, 2020, paras 3, 21); protecting
journalists and media workers (European Parliament, 2020, paras 10–12); avoiding media ownership
concentration (European Parliament, 2020, para 16); and supporting the development of a vibrant and
pluralistic media landscape through a EU media action plan (European Parliament, 2020, para 20).
Regarding the role of platforms in countering disinformation, it emphasizes the requirement to avoid
any drift to monopoly or concentration of information sources and over-censorship or removal
(European Parliament, 2020, paras 35, 37); it promotes collaboration between fact checkers, academic
researchers and stakeholders to identify, analyse and expose potential disinformation threats (European
Parliament, 2020, para 40); it pleads for joint action to counter disinformation and underlines the key
role that platforms should play to that end (European Parliament, 2020, para 41) in a transparent and
accountable manner (European Parliament, 2020, para 42). Finally, the document stresses that “using
automated tools in content moderation may endanger freedom of expression and information” (European
Parliament, 2020, para 43), and, therefore, encourages the creation of tools to enable users to report
and flag potential disinformation, and the review by independent and impartial third-party factchecking
organizations (European Parliament, 2020, para 45). Meanwhile, it also highlights, in line
with our statements above, that “online platforms are part of the online public space in which public
debate take place” (European Parliament, 2020, para 35); and that the business models based on microtargeting
advertising may generate negative impacts (European Parliament, 2020, para 39). The
resolution also calls for measures to increase effectively media literacy (European Parliament, 2020,
paras 46–49), which is essential as regards the growing flow of information online.
This resolution thus points out many important issues and promotes measures that are essential to
enhance freedom of expression and information. Of particular value in light of our study are also the
Parliamentary call on the Commission “to engage further with digital platforms [regarding the right of
individuals not to be subject to pervasive online tracking across sites and applications] and to step up
efforts to enforce the prohibition of such practices, combat the strategic, automated amplification of
disinformation through the use of bots and fake profiles online, and increase transparency with respect to
the financing and distribution of online advertising,” and its call on all online platforms “to ensure that the
algorithms that underpin their search functions are not primarily based on advertising” (European
Parliament, 2020, para 39).
The EU Council conclusions on safeguarding a free and pluralistic media system (Council of the
European Union, 2020), also adopted in November 2020, add some primordial points on sustainability,
pluralism and trustworthiness, three of which we underline here as they were not already present is such
clear terms elsewhere, and may address some of the main issues we pointed out regarding the spread of
disinformation through the digital ecosystem. First, in order to enhance the user’s autonomy, the Council
concludes that “offers of personalized content should be based on criteria which have been provided
voluntarily and/or selected by the user” (Council of the EuropeanUnion, 2020, para 21). Second, it invites
the Commission to “prevent public harm by addressing the manipulative dissemination techniques of
disinformation” (Council of the European Union, 2020, para 42). Third and as already cited, it agrees that
“with regard to the importance of freedom of speech, states and administrative regulatory authorities as
well as private platform providers should abstain from defining quality content or the reliability of content
itself” (Council of the European Union, 2020, para 39).
Data & Policy e32-17
#
#5. Conclusion
Though disinformation is a long-standing problem, AI systems present in the current digital ecosystem of the
web participate in the problem’s aggravation predominantly in two ways. First, they can be leveraged by
malicious stakeholders in order to manipulate individuals in a particularly effective manner and at a huge
scale. Secondly, they directly amplify the spread of such content. These AI systems are programmed to
enhance engagement, and therefore themain contributing factor to the spreadofdisinformation is thebusiness
model of the web. Besides that, social media bots are widespread, and fake content is increasingly realistic.
In reaction to all negative effects of disinformation on society, and particularly in the context of the
Covid-19 pandemic, social media platforms and search engines are increasingly requested to act against
the spread of disinformation online. As a consequence, many AI systems are developed to tackle the
problem. Yet, the use of AI systems to tackle disinformation content, the dissemination of which is
amplified by other AI systems, is not a miraculous solution and would generate additional concerns.79
Manipulative means should be impaired since they endanger many ethical values commonly shared at the
international level, but in doing so, the risks incurred by ethical values and fundament rights, especially by
freedom of expression and freedom to receive information, need particular attention.
As there is a clear tension between moderation of content online and free speech, the use of AI systems
for such moderation would be particularly problematic. The European Commission’s initiatives lack
some perspectives in this regard, as they provide for transparency and redress measures but do not provide
for any definition of harmful content that should be moderated by online platforms. Abstaining from
defining such content is partially in line with the EU Council’s conclusions, which properly outline that
public regulatory authorities should not define the quality of content or its reliability, but platforms neither
(Council of the European Union, 2020, para 39). Yet, while public authorities renounce any censorship,
private platforms do not.
In our opinion, a more constructive approach would be to change the current digital ecosystem in light
of the problems it generates. AI systems present in this ecosystem should indeed be adapted toward the
respect of fundamental rights and ethical values. Adapting the business model of the Web would be an
integral part of this process because as long as platforms and search engines count on the remuneration
received from advertisers, engagement of the users may be aimed. Moving to a subscriber-based or payto-pay
model for media and communication may not be the solution to advocate for, but the platforms’
tendency to constantly increase revenues from advertisers by seeking users’ engagement is disproportionate
as regards the consequences of such practices. The obligation, included in the regulation proposal,
for very large online platforms to provide their users with the possibility to modify or influence the
parameters used in the platform’s recommender systems, including at least one option which is not based
on profiling,80 is particularly relevant in this regard. The Draft Recommendation of the Council of Europe
on the protection of individuals with regard to the processing of personal data in the context of profiling81
goes along the same line, requesting from online intermediary services to “give data subjects both the
possibility to opt in as regards the profiling and the choice between the different profiling purposes or
degrees.”
In order to enhance access to accurate information while respecting freedom of expression and
information, support for media and journalists is needed, as the EU Parliament claims it. Media
pluralism is indeed of particular importance. The establishment of a public information service
providing citizens with controlled information can also be envisaged, but should by no means hamper
the individuals’ capacity to share their opinions or the media pluralism to be effective. We add that
79 Furthermore, as noted by Mireille Hildebrandt, assuming that such systems can do without the acuity of human judgement
would amount to “mistaking the imitation for what is imitated.” Hildebrandt (2018).
80 Digital Services Act, Article 29 and Recital 62.
81 Council of Europe (Directorate General of Human Rights and Rule of Law), Consultative Committee of the Convention for the
protection on individuals with regard to automatic processing of personal data—Convention 108, “Draft Recommendation on the
protection of individuals with regard to the processing of personal data in the context of profiling (revising Recommendation (2010)
13)” (19 February 2021).
e32-18 Noémi Bontridder and Yves Poullet
#
#while content moderation through AI systems is not the right approach, such systems may still be used
to counter the manipulation of the digital ecosystem, including through the accurate detection of AIgenerated
content, of social media bots, or through the detection of the malicious use of micro-
targeting systems to target disinformation content at each user. It is indeed not the content itself that
must be tackled, but rather the malicious use of technologies to amplify the conveyed message.
Besides all these measures, media literacy is primordial. Indeed, while education to the critical
evaluation of information has always been essential, it has substantially gained in importance with
the growing flow of information permitted by our data-driven world. Freedom of expression, freedom
of information, media pluralism, media literacy… all these are needed for democracy’s preservation—
or, may we say, for its revival.
Acknowledgments. A preprint version of this article was made available on ResearchGate. DOI: 10.13140/RG.2.2.28805.27365.
We thank the reviewers from Data and Policy for the valuable comments they made on that first version.
Data Availability Statement. All resources used are included in references. Where they are available online, a link is provided.
Author Contributions. Both authors have contributed to the conceptualization, data curation, formal analysis, methodology,
project administration, visualization, writing—original draft, writing—review and editing, and approved the final submitted draft.
Funding Statement. This work received no specific grant from any funding agency, commercial, or not-for-profit sectors.
Competing Interests. The authors declare no competing interests exist.
References
Akers J,Bansal G,CadamuroG,ChenC,ChenQ,Lin L,Mulcaire P,NandakumarR,RockettM, SimkoL,Toman J,WuT,
Zeng E, Zorn B and Roesner F (2018) Technology-Enabled Disinformation: Summary, Lessons, and Recommendations,
Technical Report UW-CSE, 21 December 2018. Available at https://arxiv.org/abs/1812.09383 (accessed 01 March 2021).
Assenmacher D, Clever L, Frischlich L, Quandt T, Trautmann H and Grimme C (2020) Demystifying social bots: On the
intelligence of automated social media actors. Social Media & Society 6(3), p. 1. https://doi.org/10.1177/2056305120939264
Balestrucci A (2020) How Many Bots Are You Following? 15 January 2020. Available at https://arxiv.org/abs/2001.05222v1
(accessed 01 March 2021).
Bergamini D (2020) Need for Democratic Governance of Artificial Intelligence. Committee on Political Affairs and Democracy –
Council of Europe, 24 September 2020, no. 15150. Available at https://pace.coe.int/en/files/27616 (accessed 01 March 2021).
Bindner L and Gluck R (2020) Social Media and the Murder of Samuel Paty, Global Network on Extremism and Terrorism,
6November 2020. Available at https://gnet-research.org/2020/11/06/social-media-and-the-murder-of-samuel-paty/ (accessed 01
March 2021).
Council of the European Union (2020) Council Conclusions on Safeguarding a Free and Pluralistic Media System, 13260/20,
Brussels, 27 November 2020. Available at https://data.consilium.europa.eu/doc/document/ST-13260-2020-INIT/en/pdf
(accessed 01 March 2021).
de Coorebyter V (2020) L’Internet: démocratie ou démagogie. In Poullet Y (ed.), Vie privée, liberté d’expression et démocratie
dans la société du numérique. Brussels: Larcier, p. 249.
Dixit P and Mac R (2018) How WhatsApp Destroyed a Village. Buzzfeed News, September 2018. Available at https://www.
buzzfeednews.com/article/pranavdixit/whatsapp-destroyed-village-lynchings-rainpada-india (accessed 01 March 2021).
EDPS Ethics Advisory Group (2018) Towards a Digital Ethics – Report. Available at https://edps.europa.eu/data-protection/ourwork/publications/ethical-framework/ethics-advisory-group-report-2018_en
(accessed 01 March 2021).
ERGA (2020) ERGA Report on disinformation: Assessment of the implementation of the Code of Practice (2020). Available at
https://erga-online.eu/wp-content/uploads/2020/05/ERGA-2019-report-published-2020-LQ.pdf (accessed 01 March 2021).
EuropeanCommission (2018a)AMulti-dimensional Approach toDisinformation:Report of the IndependentHighLevelGrouponFake
News and Online Disinformation. Directorate-General for Communication Networks, Content and Technology. Available at https://
ec.europa.eu/digital-single-market/en/news/final-report-high-level-expert-group-fake-news-and-online-disinformation (accessed 01
March 2021).
European Commission (2018b) Code of Practice on Disinformation. Available at https://ec.europa.eu/digital-single-market/en/
news/code-practice-disinformation.
EuropeanCommission (2018c) Synopsis Report of the Public Consultation on FakeNews andOnlineDisinformation. Available at
https://ec.europa.eu/digital-single-market/en/news/synopsis-report-public-consultation-fake-news-and-online-disinformation
(accessed 01 March 2021).
Data & Policy e32-19
#
#EuropeanCommission (2018d) Tackling Online Disinformation: A European Approach (Communication) COM(2018) 236 final.
Available at https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52018DC0236 (accessed 01 March 2021).
European Commission (2020a) Assessment of the Code of Practice on Disinformation — Achievements and areas for further
improvement. Commission Staff working document (SWD(2020) 180 final).
European Commission (2020b) European Democracy Action Plan (Communication) COM(2020) 790 final. Available at https://
eur-lex.europa.eu/legal-content/EN/TXT/?uri=COM%3A2020%3A790%3AFIN&qid=1607079662423 (accessed 01 March
2021).
European Commission (2021) Guidance on Strengthening the Code of Practice on Disinformation (COM(2021) 262 final).
Available at https://digital-strategy.ec.europa.eu/en/library/guidance-strengthening-code-practice-disinformation (accessed 03
July 2021).
European Parliament (2020) European Parliament Resolution on Strengthening Media Freedom: The Protection of Journalists in
Europe, Hate Speech, Disinformation and the Role of Platforms (25 November 2020) 2020/2009(INI). Available at https://
www.europarl.europa.eu/doceo/document/TA-9-2020-0320_EN.html (accessed 01 March 2021).
Festré A and Garrouste P (2015) The ‘Economics of attention’: A history of economic thought perspective.Œconomia. History,
Methodology, Philosophy 5(1), 3–36. https://doi.org/10.4000/oeconomia.1139
Fink C (2018) Dangerous speech, anti-Muslim violence and Facebook in Myanmar, Special issue: Contentious narratives: Digital
technology and the attack on liberal democratic norms. Journal of International Affairs 71(1.5), pp. 43–52. Available at https://
jia.sipa.columbia.edu/dangerous-speech-anti-muslim-violence-and-facebook-myanmar (accessed 01 March 2021).
Hannah M (2021) QAnon and the information dark age. First Monday 26(2). https://doi.org/10.5210/fm.v26i2.10868
Hanot M and Michel A (2020) Entre menaces pour la vie en société et risques réglementaires, les fake news: un danger pour la
démocratie? In Poullet Y (ed.), Vie privée, liberté d’expression et démocratie dans la société du numérique. Brussels: Larcier .
Harbinja E and Karagiannopoulos V (2019) Web 3.0: The Decentralised Web Promises to Make the Internet Free Again. The
Conversation, 11 March 2019. Available at https://theconversation.com/web-3-0-the-decentralised-web-promises-to-make-theinternet-free-again-113139
(accessed 01 March 2021).
Hildebrandt M (2018) Primitives of Legal Protection in the Era of Data-Driven Platforms, March 2018, p. 11. http://dx.doi.org/
10.2139/ssrn.3140594
Howard PN and Kollanyi B (2016) Bots, #StrongerIn, and #Brexit: Computational Propaganda during the UK-EU Referendum.
http://dx.doi.org/10.2139/ssrn.2798311
Human Rights Council (2018) Report of the Independent International Fact-Finding Mission on Myanmar, A/HRC/39/64,
12 September 2018. Available at https://www.ohchr.org/en/hrbodies/hrc/myanmarFFM/Pages/ReportoftheMyanmarFFM.aspx
(accessed 01 March 2021).
Kertysova K (2018) Artificial intelligence and disinformation: HowAI changes the way disinformation is produced, disseminated,
and can be countered. Security and Human Rights 29, 55–81. https://doi.org/10.1163/18750230-02901005
Kirby EJ (2016) The City Getting Rich from Fake News. BBC News, 5 December 2016. Available at https://www.bbc.com/news/
magazine-38168281 (accessed 01 March 2021).
Lamo M and Calo R (2018) Regulating Bot Speech. UCLA Law Review, 16 July 2018, p. 1. http://dx.doi.org/10.2139/
ssrn.3214572
Lance BennettWandLivingston S (eds) (2020) The Disinformation Age: Politics, Technology, andDisruptive Communication in
the United States. Cambridge: Cambridge University Press. https://doi.org/10.1017/9781108914628
Maréchal N and Biddle ER (2020) It’s Not Just the Content, It’s the Business Model: Democracy’s Online Speech Challenge - A
Report from Ranking Digital Rights, New America, 17 March 2020. Available at https://www.newamerica.org/oti/reports/itsnot-just-content-its-business-model/
(accessed 01 March 2021).
Marsden C and Meyer T (2019) Regulating Disinformation with Artificial Intelligence: Effect of Disinformation Initiatives on
Freedom of Expression and Media Pluralism. European Parliamentary Research Service (EPRS) – Scientific Foresight Unit
(STOA), March 2019, PE 624.279. Available at https://op.europa.eu/en/publication-detail/-/publication/b8722bec-81be-11e99f05-01aa75ed71a1
(accessed 01 March 2021).
Mork A (ed.) (2020) Fake for Real – A History of Forgery and Falsification. House of European History, Temporary Exhibition
Catalogue. Luxembourg: Publications Office of the European Union.
NewmanN,FletcherR, SchulzA,AndıS andNielsenRK (2020) Reuters Institute Digital News Report 2020. Available at https://
www.digitalnewsreport.org/ (accessed 01 March 2021).
Partnership on AI (2020) The Deepfake Detection Challenge: Insights and Recommendations for AI and Media Integrity.
Available at https://www.partnershiponai.org/ai-and-media-integrity-steering-committee/.
Perrigo B (2019) The World Wide Web Turns 30 Today. Here’s How Its Inventor Thinks We Can Fix It. Time, 12 March 2019.
Available at https://time.com/5549635/tim-berners-lee-interview-web/ (accessed 01 March 2021).
Poullet Y (2020a) La “Révolution” numérique: quelle place encore pour le droit? Brussels: Académie royale de Belgique, p. 78.
Poullet Y (2020b) Ethique et droits de l’Homme dans notre société du numérique. Brussels: Académie Royale de Belgique.
Poullet Y (2020c) Vue de Bruxelles. Modes alternatifs de régulation et libertés dans la société du numérique. In Castets-Renard C,
Ndior V and Rass-Masson L (eds), Enjeux internationaux des activités numériques: entre logique territoriale des Etats et
puissance des acteurs privés. Brussels: Larcier, p. 108.
Rundle M and Conley C (2007) Ethical Implications of Emerging Technologies: A Survey. UNESCO – Information for All
Programme. Available at https://unesdoc.unesco.org/ark:/48223/pf0000149992 (accessed 01 March 2021).
e32-20 Noémi Bontridder and Yves Poullet
#
#SankinA (2020)Want to Find aMisinformed Public? Facebook’s AlreadyDone It. TheMarkup, 23April 2020. Available at https://
themarkup.org/coronavirus/2020/04/23/want-to-find-a-misinformed-public-facebooks-already-done-it (accessed 01 March
2021).
Sounding Board (2018) The Sounding Board’s unanimous final opinion on the co-called Code of Practice. Available at https://
www.ebu.ch/files/live/sites/ebu/files/News/2018/09/Opinion%20of%20the%20Sounding%20Board.pdf (accessed 01 March
2021).
The Economist (2020a) Disinformation in Myanmar – Anti-social Network, Who Controls the Conversation? – Social Media and
Free Speech, 24 October 2020, pp. 45–46.
The Economist (2020b) The Great Clean-Up, Who Controls the Conversation? – Social Media and Free Speech, 24 October 2020,
p. 19.
Varol O, Ferrara E, Davis CA,Menczer F and Flammini A (2017) Online Human-Bot Interactions: Detection, Estimation, and
Characterization. Available at https://arxiv.org/abs/1703.03107 (accessed 01 March 2021).
WalorskaAM (2020)Deepfakes andDisinformation. Friedrich Naumann Foundation for Freedom,May 2020. Available at https://
fnf-europe.org/wp-content/uploads/2020/06/fnf_deepfakes_broschuere_en_web.pdf (accessed 01 March 2021).
World Health Organization (WHO) (2020a) Coronavirus Disease 2019 (COVID-19) Situation Report-45. Available at https://
www.who.int/docs/default-source/coronaviruse/situation-reports/20200305-sitrep-45-covid-19.pdf?sfvrsn=ed2ba78b_4.
World Health Organization (WHO) (2020b) The Joint statement byWHO, UN, UNICEF, UNDP, UNESCO, UNAIDS, ITU, UN
Global Pulse, and IFRC, Managing the COVID-19 infodemic: Promoting healthy behaviours and mitigating the harm from
misinformation and disinformation. Available at https://www.who.int/news/item/23-09-2020-managing-the-covid-19-info
demic-promoting-healthy-behaviours-and-mitigating-the-harm-from-misinformation-and-disinformation.
WrightR (2001) TheManWho Invented theWeb. Time, 24 June 2001. Available at http://content.time.com/time/magazine/article/
0,9171,137689,00.html (accessed 01 March 2021).
ZagoM,Nespoli P, Papamartzivanos D, PerezMG,Marmol FG,Kambourakis G and Perez GM (2019) Screening out social
bots interference: Are there any silver bullets? IEEE Communications Magazine 57(8), 98. https://doi.org/10.1109/
MCOM.2019.1800520
Cite this article: Bontridder N and Poullet Y (2021). The role of artificial intelligence in disinformation. Data & Policy, 3: e32.
doi:10.1017/dap.2021.20
Data & Policy e32-21
175
175
Islamophobia, Chinese Style: 
Total Internment of Uyghur 
Muslims by the People’s Republic 
of China
ISLAMOPHOBIA STUDIES JOURNAL 
VOLUME 5, NO. 2 Fall 2020, PP. 175–198.
Disclaimer:
Statements of fact and opinion in the articles, notes, perspectives, and so 
on in the Islamophobia Studies Journal are those of the respective authors 
and contributors. They are not the expression of the editorial or advisory 
board and staff. No representation, either expressed or implied, is made 
of the accuracy of the material in this journal, and ISJ cannot accept any 
legal responsibility or liability for any errors or omissions that may be 
made. The reader must make his or her own evaluation of the accuracy 
and appropriateness of those materials.
Published by:
Islamophobia Research and Documentation Project,
Center for Race and Gender, University of California, Berkeley
Ali Çaksu
Yıldız Technical University, Istanbul
#
#176
176 ISJ 5(2)
Islamophobia, Chinese Style: Total Internment of Uyghur Muslims by the 
People’s Republic of China
Ali Çaksu
Yıldız Technical University, Istanbul
ABSTRACT: Since 2016 the mistreatment of Uyghurs, a Turkic Muslim minority living 
in their homeland in northwest China, by the People’s Republic of China has been taken 
to extreme levels. Uyghurs now are clearly victims of genocidal policies through internment 
of all types. The total internment of Uyghurs involves physical, psychological and virtual 
internments. In this article, I discuss the issue in three parts, namely internment camps for 
Uyghurs, urban internment and virtual internment. While all of them have both physical and 
psychological aspects, the last two are based on an Orwelian use of surveillance technologies 
for the purpose of large-scale persecution and suppression of Uyghurs with the intention of 
achieving a China with one homogenous culture. The total internment which is currently 
being carried out has two main motives: Islamophobia and the crucial geo-strategical position 
of the Uyghur homeland.
Keywords: Uyghurs, China, Islamophobia, internment, surveillance capitalism, terror 
capitalism, genocide
One man’s imagined community is another man’s political prison.
Arjun Appadurai, Modernity at Large
People of various ethnic groups (mostly Turkic), such as Uyghurs, Kazakhs, Kyrgyzes 
and Tatars have been living for centuries in the Uyghur Region. While the region was subjected
Chinese invasions from time to time in history, since 1949 it has been under Chinese 
rule. The name Xinjiang, which was first used in 1884 during the Qing Dynasty, means “New 
Frontier” or “New Territory” in Chinese, showing the Chinese are not native to that land. 
People of the region traditionally call their land East Turkestan or Uyghuristan. The region 
is officially called Xinjiang Uyghur Autonomous Region (XUAR). For convenience, in this 
article I will call the XAUR the Uyghur Region and all the Turkic-Muslim people from this 
region as Uyghurs (regardless of their ethnic origins).
The ethnic composition of the Uyghur Region has shifted in the past half century. 
According to the 1953 government census, Han Chinese constituted 6% of the region’s 
population of 4.87 million, while Uyghurs made up 75%. The 2000 census found out the 
Han population at 40.57% and Uyghurs at 45.21% of a total population of 18.46 million 
(Toops, 2004). According to latest Chinese census, Muslims in the Uygur Region number 
over 11 million, while Uyghur sources claim the Muslim population to be around 15 million.
There is a growing Sinicization due to intense Han immigration. Recently the percent-
age of Han reached more than 50% in the region for the first time. So the Uyghurs have 
become a minority in their homeland.
Since the end of the Cultural Revolution until the Tiananmen Massacre in 1989 the 
region had a relatively calm and conflict-free period. But after the Tiananmen Massacre the 
#
#177
Chinese regime became bolder in its treatment of its citizens, including the Uyghurs. For 
instance, the 9/11 attacks in the USA were an excuse for the Chinese regime that claimed that 
it too was a victim of Islamic terrorism.
The region has many problems: it is one of China’s poorest areas. Uyghurs suffer from 
discrimination in employment (Fischer, 2014) and education (Hann, 2014), widespread corruption
and restrictions on their freedoms, religious practice and culture. Yet any criticism is 
portrayed by the state as a sign of separatism and extremism. From time to time, the region saw 
violence such as the violent protests against Han Chinese in July 2009 stemming from the 
Uyghurs’ desperation after decades of discrimination and persecution.
Since 2014 things have changed drastically in the Uyghur Region. The central leadership
declared in 2014 the “People’s War on Terror,” which had tremendously negative 
impacts on the lives of Uyghurs. In August 2016, Chen Quanguo became Xinjiang’s new 
Communist Party secretary and mass detention and surveillance of Uyghurs followed. Before 
that he was the party secretary of Tibet, where he pacified the region through intense securitization
and widespread social control mechanisms. The internment of Uyghurs increased 
after “Regulations on De-extremification” were adopted in March 2017. Byler (2018b) suggests
that “Chen, with the support of the Xi administration, made the decision to move from 
simply a police state security approach to a mass human re-engineering approach in managing
the Uyghur population.”
Chen Quanguo, a key ally of President Xi Jinping, is known for ruthless crackdowns 
and zero tolerance. What he did in Tibet foretold what he would do in Xinjiang. And what he 
did in Tibet might be summed up by the expression “Copper Ramparts, Iron Walls” (Human 
Rights Watch, 2017):
The term refers to an impenetrable “public security defense network” (zhi’an lianfang wangluo) 
consisting of citizen patrols, border security posts, police checkposts, surveillance systems, internet 
controls, identity card monitoring, travel restrictions, management of “focus personnel,” grid unit 
offices, informant networks, and other mechanisms that aim to control or monitor movement of 
people and ideas into, out of, or within a region or society. It describes the ideal of “stability 
maintenance” work, where authorities have successfully sealed off a region or society from people or 
ideas they regard as threatening or problematic.
INTERNMENT CAMPS FOR UYGHURS
In the last decade the Chinese government has built a network of internment camps for 
ethnic Turkic Muslim minorities in the Uyghur Region and since 2017 the number of camps 
has increased tremendously. The government has been sending to those camps a lot of people, 
including women of various ages, teenagers and old people. Initially, the Chinese state officially 
denied the existence of such camps (until October 2018), but due to overwhelming evidence, 
including satellite photographs and leaked photographs and videos, the acceptance came. 
Nevertheless, initially the authorities called them “vocational schools” which people willingly 
attend. Later they were called “vocational schools for criminals.” Yet when many camps held 
academics, intellectuals, artists and even elderly people, this label obviously sounded ridiculous.
Then the authorities labeled the camps as “re-education” camps for terrorists or extrem-
ists. One should note that they had to add the term “extremists,” as common people of the 
Uyghur Region, including teenagers and elderly people who ended up in the camps had 
#
#178 ISJ 5(2)
nothing to do with terrorism and hardly any of them were charged with any terrorist attack or 
event. People are simply locked up without trial and they do not even know what their crimes 
are. Some authorities also called these internment camps “hospitals for ideological illness” 
because, as I will discuss below, they consider Islam as a mental disease. In brief, the camps are 
definitely internment or concentration camps with daily forced indoctrination, forced labor, 
torture and rape.
In fact, the concept of “re-education” has a long history in Communist China: in the 
1950s, the state established the practices of “reform through labor” and “re-education through 
labor” (Fu, 2005; Yu, 2010). Later, in the early 2000s, the government initiated “transformation
through education” classes for Falun Gong followers. The current application of such 
concepts to Uyghurs is presented as “de-extremification campaigns” and it seems that the 
Chinese Communist Party’s (CCP) methods for that purpose have not changed much. This 
time the CCP is allegedly fighting “the three evils,” namely separatism, extremism and terrorism.
For instance, businessman Yu Ming, who was imprisoned for 12 years and tortured nearly 
to death in labor camps in China for his beliefs in Falun Gong, said that from the reports of 
what has been happening in the “re-education camps” in Uyghur Region, he could tell that the 
Chinese state is employing the same methods it has been using against Falun Gong followers 
to force them to give up their faith. According to him, even “the lies, slogans, and propaganda 
are the same” (Zeng, 2019).
As for the number of the detainees in the interment camps, Gay McDougall, a member
of the United Nations Committee on the Elimination of Racial Discrimination, stated in 
August 2018 that more than one million Uyghurs and Muslim minorities were forced into 
the camps (UN Human Rights, 2018). Since then the number has definitely increased, since 
we know that internment of Uyghurs is continued by the Chinese state. Indeed, a testimony 
given at the US Senate on 4 December 2018 by Scott Busby from Senate Foreign Relations 
Committee claimed that “possibly more than 2 million” Uyghurs were detained (Busby, 
2019). Finally, various Uyghur human rights organizations claim that number to be around 
three million detainees.
Most of the internment camps were until recently scattered throughout the Uyghur 
Region. By obtaining satellite images from applications like Google Earth Pro, some Chinese 
and Western activists discover, list and display information on China’s growing network of 
internment camps in the region. For example, Shawn Zhang (2018), an online activist has 
listed (as of 29 May 2019) 94 internment camps with their satellite images, locations and GPS 
coordinates. Likewise, Adrian Zenz (2019) has provided very valuable information on internment
camps by making use of publicly available evidence from official sources, including gov-
ernment websites, media reports and other Chinese internet sources. Similarly, a report by 
Australian Strategic Policy Institute’s (ASPI) International Cyber Policy Centre provides opensource
research into the province. The results are added into a single database which gives valu-
able and more detailed information on various camps, including official documents and media 
articles. The research shows that the listed facilities are “punitive in nature and more akin to 
prison camps” than transformation through education centers (Ryan, Cave and Ruser, 2018). 
The database also identifies and gives information on some of the camps discovered by Shawn 
Zhang and Adrian Zenz, as shown in Figure 1.
One should note that in China there are also internment camps for Uyghurs outside the 
Uyghur Region. This is primarily because the authorities try to hide the numbers of internees 
from the world due to avoid a negative international reaction. As we have seen, even satellite 
imagery can give us an approximate idea of the number of detainees in a certain camp, especially
a newly built one. Besides, due to huge numbers of Uyghurs kept in internment camps 
#
#179
in the Uyghur Region, the network of camps there does not have enough space for newcomers, 
hence the need to send some of the inmates elsewhere. For example, according to sources in the 
CCP, the provinces of Shaanxi, Inner Mongolia, Gansu, Heilongjiang and others have been 
assigned quotas of internees to take. Currently, sources report that Shaanxi Province in central 
China has a quota of about 25,000 people and nearly 500,000 Uyghurs will be dispersed to the 
internment camps throughout China (Zaili, 2018).
In spite of the official denials and hindrances, we have detailed information about 
physical situation of the internment camps. Some of it comes from the reports of former 
inmates or former Uyghur teachers who were forced to teach in the camps, while some comes 
from secret video recordings. Satellite photographs too provide significant information, 
especially on the locations, size and number of the camps. Even some propaganda photographs
published by the Chinese state too give us reliable information about some aspects of 
the camps or situation of the inmates. All the interment camps have definitely been built for 
the purpose of internment. They have walls and watchtowers just like prisons. The inmates 
are monitored 24 hours a day and deprived of any freedom and privacy. The rooms have iron 
gates and the windows are sealed with iron bars and wire netting. It is nearly impossible to 
escape from the camps. For instance, one can see a video clip of the newly built Yingye-er 
Re-education Camp, in Yining city in Xinxiang (Bitter Winter, 2018a). As understood from 
satellite images, the camp site was an empty space in October 2017 and by August 2018 
most of the construction was completed. The camp already had several thousand inmates in 
September 2018. It has four building complexes with a floor space area of about 110,000 m² 
and can accommodate several thousand people. As the Bitter Winter video shows in detail 
and tells us, “inside the buildings, the interior constructions is just like the cells of a prison 
or detention center.” There are hundreds of surveillance cameras inside the buildings, including 
the dormitories, washrooms and toilets.
Figure 1. Number of New Security Facilities Detected in Xinjiang Province, 2011–2018
#
#180 ISJ 5(2)
As for the activities in the camps, according to official information released by video 
(Made in China, 2018), the inmates learn the Chinese language (Mandarin), receive legal education
(i.e. learn laws and regulations, national security, anti-terrorism) and vocational educa-
tion. I believe that the “legal education” forms the indoctrination and brainwashing part of 
the re-education, as we know that the inmates are often asked to memorize and recite policy 
documents and speeches delivered by Xi Jinping, the current president of China. Also, as far 
as I gather from the video, the vocational education might include forced labor, as the camp 
produces and sells equipment to other parts of China. It seems that others who have watched 
similar official videos have the same impression (Buckley and Ramzy, 2018). In fact, contrary 
to the official information on the activities in the camps, the testimonies of many former 
inmates present a completely different picture: the camp work centered on military style 
training and denunciations of Islam. In the camps, the inmates have to repudiate their Islamic 
beliefs, criticize themselves and their loved ones and thank the ruling Communist Party. They 
are frequently lectured about the dangers of Islam (Shih, 2018). A video (Bitter Winter, 
2018b) shot secretly in the newly built Yingye-er Re-education Camp (mentioned above) 
shows various signboards with rules and regulations written in Chinese and comments on 
some of them. For instance, article one of the “Heart-to-Heart Regulations” states that “antiextremism
should be incorporated within the content of heart-to-heart chats” and explains 
how this should be done. This is apparently the official name for the brainwashing process and 
the ideological indoctrination.
In some camps, washing of hands and feet is not allowed, as it is equated with Islamic 
ablution. Women are forced to apologize for wearing long clothes in Muslim style, praying or 
teaching the Quran to their children. As part of “de-extremification” campaigns, the inmates 
are often forced to eat pork and drink alcohol (both of which are forbidden in Islam), as we learn 
from the reports by former inmates (Shih and Kang, 2018). The detainees are not allowed to 
use the toilet for long periods of time, are not allowed to see a doctor when they are ill and are 
forced to perform drills like soldiers. If an inmate does not comply, he/she is sent to solitary 
confinement, deprived of food and/or sleep for a long time, beaten or tortured physically and/
or psychologically. Such treatment is done regardless of the gender or age of the inmates. A 
Uyghur woman, Mihrigul Tursun (aged 29) told reporters in Washington that she was interrogated
for four days in a row without sleep, had her hair shaved, was electrocuted and was 
subjected to an intrusive medical examination. “I thought that I would rather die than go 
through this torture and begged them to kill me,” she told the journalists (Cockburn, 2018).
Who are taken to the internment camps? According to Eldost, who was a Uyghur 
reporter for Xinjiang TV and was in mid-2017 forced to teach Chinese history and culture in 
an internment camp because he spoke excellent Mandarin, the internment system classified the 
inmates into three levels of security and duration of sentences. The first group in general consisted
of illiterate Uyghur farmers who did not commit any crimes other than not speaking 
Chinese. The second class had people who were caught at home or on their smartphones with 
religious content or so-called separatist materials. And the final group was made up of those 
who had studied religion abroad and came back or were thought to be affiliated with foreign 
elements. In the latter cases, the inmates were often sentenced to prison terms of 10–15 years 
(Shih, 2018). Also, one can see thousands of individual profiles of the detainees on a website: 
Xinjiang Victims Database, which is run by an activist named Gene A. Bunin, who records 
testimonies of or about the Uyghur victims who were/are kept in concentration camps. The 
database provides, as much as possible, detailed information about each detainee including 
name, age, gender, ethnicity, profession, location, health status and reason for detention. The 
database also records people who are missing.
#
#181
For interment camps, in addition to the ordinary people just listed, China targets 
prominent Uyghur intellectuals, academics and artists to erase the Uyghur ethnic identity. As 
of 25 March 2019, the Uyghur Human Rights Project had identified at least 386 Uyghur 
intellectuals detained and disappeared since early 2017. For instance, Rahile Dawut, an anthropologist
at Xinjiang University who studied Muslim shrines, traditional songs and folklore was 
detained in December 2017 and has not been heard from since. Similarly, former Xinjiang 
University President Tashpolat Teyip, writer and critic Yalqun Rozi and former director of the 
Xinjiang Education Supervision Bureau Satar Sawut disappeared in 2017, as they perhaps criticized
China’s increasingly hardline policies in the Uyghur Region. Again, China sentenced 
Ilham Tohti, a Beijing-based Uyghur economics professor, to life in prison and he was charged 
with various crimes like criticizing the government, advocating separatism, inciting ethnic 
hatred and voicing support for terrorism. Tohti is, in fact, widely respected internationally as a 
moderate voice within the Uyghur community. Throughout his two-day trial in Urumqi, he 
asserted that he always has opposed separatism and that he had spent his life trying to promote 
better relations between Uyghurs and the Han majority. The Chinese state has targeted many 
other “moderate” Uyghur scholars and intellectuals. According to Rune Steenberg, a postdoctoral
researcher at the University of Copenhagen, such scholars offered a moderate path, where 
Uyghurs could continue with their religious and cultural practices without resorting to extreme 
and isolationist ideas: “This is the really big tragedy about the clampdown . . . They were actually
bridge builders of integration of broader Uighur society into modern Chinese society and 
economy” (Ramzy, 2019). Professor Bruce Jacobs from Monash University sees Tohti’s life 
imprisonment for doing nothing more than criticize Beijing’s strategies as a deliberate attempt 
by the Communist Party to push Uyghurs towards radicalism and extremism:
The Uighurs have no options. By throwing Tohti into jail for the rest of his life—or even just 
sentencing him—they’re showing that moderate action has no future. The only actions which 
have the option, or the potential for success, are the radical actions, because the Chinese colonial 
state is so violent and very vigorous in pushing down any sort of Uighur self consciousness. 
(McDonell, 2014)
Fergus Ryan, an analyst and China expert at ASPI’s International Cyber Policy Centre 
is of the same opinion: “By detaining such a huge amount of people for no legal reason China 
is really running the risk of radicalising these people and creating the perfect conditions 
for violent extremism to happen in the future.” Likewise, Soliev (2019, p. 75) believes that 
“[w]ithout treating the grievances of the Uyghur and other minority grievances sensitively, 
it will be impossible to effectively curb the potential for ethnic riots and the public support 
for terrorism.” In 2006 Panda (2006) conjectured that China would not follow such a policy: 
“China will not be interested in directly confronting the Uygurs in Xinjiang through a heavy 
hand, which in turn will inflame their passion and political desire” (p. 42), but this view has 
proved to be completely wrong. While such observations come from common sense, then why 
does the Chinese state provoke radicalization and extremism? In my opinion, it is a deliberate 
policy, since the top authorities of the regime, who suffer from an intoxication of power, will 
use radicalism, extremism and separatism as excuses for furthering their genocidal policies, 
which will be used to annihilate Uyghur ethno-religious identity.
I will end the discussion about the interment camps with a final note on the kind of 
“crimes” that may lead a Uyghur in China to end up in an interment camp. Tanner Greer, a 
researcher on East Asian strategic thought, has summarized 48 ways to get detained in an 
internment camp, from reports, interviews with ex-detainees and governmental documents. 
#
#182 ISJ 5(2)
Note that most of these “48 ways” are what people in most of the world would consider 
human, normal or even trivial (Greer, 2018). Table 1 lists all of them (although there are 
many other additional “crimes” which I myself have come across while reading testimonies of 
the victims such as stopping watching television, not using a smartphone, using an “abnormal”
amount of electricity, not socializing with neighbors, complaining about working 
Owning a tent Telling others not to swear Speaking with someone who has 
traveled abroad
Owning welding equipment Telling others not to sin Having traveled abroad yourself
Owning extra food Eating breakfast before the sun comes 
up
Merely knowing someone who has 
traveled abroad
Owning a compass Arguing with an official Publicly stating that China is 
inferior to some other country
Owning multiple knives Sending a petition that complains 
about local officials
Having too many children
Abstaining from alcohol Not allowing officials to sleep in your 
bed, eat your food and live in your 
house
Having a VPN (virtual private 
network)
Abstaining from cigarettes Not having your government ID on 
your person
Having WhatsApp
Wailing, publicly grieving, 
or otherwise acting sad 
when your parents die
Not letting officials take your DNA Watching a video filmed abroad
Wearing a scarf in the 
presence of the Chinese flag
Wearing a hijab (if you are under 45) Going to a mosque
Praying Fasting Listening to a religious lecture
Not letting officials scan 
your irises
Not letting officials download 
everything you have an your phone
Not making voice recordings to 
give to officials
Speaking your native 
language in school
Speaking your native language in 
government work groups
Speaking with someone abroad (via 
Skype, WeChat, etc.)
Wearing a shirt with Arabic 
lettered writing on it
Having a full beard Wearing any clothes with religious 
iconography
Not attending mandatory 
propaganda classes
Not attending mandatory flag-raising 
ceremonies
Not attending public struggle 
sessions
Refusing to denounce your 
family members or yourself 
in public struggle sessions
Trying to kill yourself when detained 
by the police
Trying to kill yourself when in the 
education camps
Performing a traditional 
funeral
Inviting multiple families to your 
house without registering with the 
police department
Being related to anyone who has 
done any of the above
Table 1. Red Flags for Detainment in Xinjiang
Source: Greer (2018)
#
#183
conditions, speaking Arabic or Turkish and following the Muslim custom of washing the 
body of a deceased person).
While Uyghur adults are being sent to the internment camps, many Uyghur children 
face a similar fate. Chinese authorities have been putting the children of Muslims in Xinxiang 
into dozens of state-run orphanages across the region. When the parents of the children are 
taken into internment camps, the authorities often do not allow their close relatives like grandparents
and aunts to take care of them; instead the children are moved to orphanages where 
they are allegedly “re-educated” in accordance with the Chinese culture and the values of the 
Communist Party (Zaili, 2019).
As far as can be ascertained, there are various reasons for establishing Uygur orphanages.
Of course, as expected, the main reason is brainwashing children and training them to be 
“proper Chinese,” without any tie to Uyghur culture and Islam, thus erasing their ethnic and 
religious identity. A second reason is tearing Uyghur families apart by distancing children 
systematically from their families and culture and thus causing trauma for the Uyghurs in 
China as well as abroad. This is part of the Chinese genocidal policies in the region. A third 
reason is blackmailing the Uyghurs abroad to return to China, if they have children in the 
country. If the blackmail does not work and the parents do not return to China, then they have 
to bear the punishment in the form of not knowing anything about the fate of their children, 
let alone being together with them. Finally, it seems that some Chinese authorities even use 
Uyghur children in orphanages as blackmail. For instance, someone claiming to be a police 
officer in Xinjiang added a Uyghur family living in Turkey on a Chinese messaging application.
“For nearly three years now, the man has sporadically sent them photos or updates about 
their son, promising to continue sharing information if the couple helps him spy on Uighurs 
in Turkey” (Wang and Kang, 2018).
Unfortunately, orphanages for Uyghur children as a result of the new Chinese policies 
are not limited to those in China. It seems that Chinese government policies have indirectly 
created orphanages abroad too. In fact, in Istanbul currently there is an orphanage for Uyghur 
children whose parents have been arrested, put into internment camps or killed when they 
visited China and I am afraid that there might be other such orphanages in cities where Uyghurs 
have taken refuge in great numbers. Some of those parents might have already left the camps 
and be alive, but are not permitted to leave China. However, it is almost impossible to find and 
contact them. I visited the orphanage in Istanbul on 17 May 2019, together with a colleague 
who is a professor of law, and some of the information I provide here is based on what the 
administrators of the orphanage told us. The reasons those parents visited China while leaving 
their children in Turkey differ: when the situation in Xinjiang was more tolerable and these 
parents did not know about the interment camps, some went there to close down their businesses
or sell their properties; some visited their homeland to see their parents who were dying; 
some went there to pick up their other children. Later, even when the interment camps started, 
some parents were afraid of the official blackmail threatening the lives of their parents, if they 
did not return to China immediately. In addition, there are also some children in the orphanage 
whose parents sent them to Turkey when the situation in Uyghur Region deteriorated badly. 
Their parents just wanted to get their children out of China, which had become like a hell for 
them, lest they end up in orphanages in case parents are taken to internment camps. Those 
children were mostly sent to Turkey together with some close relatives when the international 
movement of Uyghurs was not yet completely restricted.
One should note that many Uyghurs have joined the police force as low-level officers 
in order to protect themselves and their family members from being interned in re-education 
#
#184 ISJ 5(2)
camps and their children in orphanages. So some members of the police, internment camp 
guards and the local officials are actually Uyghurs who found themselves in such a situation. 
As far as I gather from the testimonies of the Uyghurs who are victims of the state aggression,
some of these Uyghur officials actually help their fellow Uyghurs in some ways, such as 
giving positive grades for the “students” in the internment camps, which prevents worse 
treatment at least. Of course, such help by the Uyghur officials is very risky for them, because 
if they are suspected of disloyalty towards Chinese polices, they would be declared “twofaced”
and probably face conditions worse than those in the interment camps. Chinese 
authorities often accuse and punish “two-faced” Uyghur cadres (including some at higher 
levels) who are suspected of paying lip service to Communist Party rule while secretly passing
over state policies.
URBAN INTERNMENT: UYGHUR CITIES AS OPEN-AIR PRISONS
Chinese campaigns for re-education are not limited to the internment camps, but practiced
almost everywhere. One can say without exaggeration that Uyghur cities and villages too 
function like interment camps. As mentioned above, in the Uyghur Region individuals caught 
praying, fasting, growing a beard or wearing a hijab are arrested and sent to interment camps, 
as they are considered extremists or even terrorists. Nevertheless, avoiding such “extremist” or 
“terrorist” actions is not enough; those Uyghurs outside internment camps too must show their 
sincerity in distancing themselves from Islamic injunctions like avoiding eating pork and 
drinking alcohol. Thus, Uyghurs are forced to eat pork and drink alcohol in internment camps, 
in some festivals that they are forced to celebrate (Sharman, 2019) as well as their homes, which 
are often visited by authorities who stay with Uyghurs for some days or weeks, as I explain 
below. In addition, the Chinese authorities carry out anti-halal campaigns in Uyghur as well as 
Hui (Chinese Muslim) regions “to safeguard ethnic unity,” as a pro-government Chinese daily 
informs us (Li, 2018). We also learn from notices on government websites that any Uyghur 
restaurant that closes during Ramadan loses its license (Shelton and Zhao, 2019).
Official harassment for re-education is also carried out in some unusual ways on the 
streets of Uyghur cities. For example, sometimes officials stop Uyghur women on the street and 
cut off parts of their dresses with scissors, if they are found to be a bit long. One should note 
that in the authentic photographs depicting such events, Uyghur women wear jeans and shorts 
or dresses and no hijab, headscarf or veil (Smith, 2018). Like most of its inhuman practices, 
officials have again a slogan for this one: “Wear civilized, good-looking clothes, be beautiful 
Urumqi people, resolutely oppose abnormal clothing and behavior!”
Urban internment in the Uyghur Region can declare anyone a terrorist for anything 
that is not accepted or liked by the state and one can come across even a “terrorist” of time. 
According to Human Rights Watch (2018), China detained a man on terror charges after he 
set his watch two hours behind Beijing time (perhaps to follow Muslim prayer times accurately).
Mao Zedong merged China’s time zones into one to enhance “national unity” but some 
still use unofficial Xinjiang/Urumchi time. Setting clocks to “Urumchi time” is therefore seen 
as a form of resistance against the CCP.
Chinese officials have taken urban internment to unbelievable levels, annihilated any 
sort of privacy and violated human dignity. Now re-education is being practiced inside 
Uyghur homes. Officials often visit Uyghur homes and stay there for weeks as uninvited 
guests. This practice is not denied by the authorities at all, but even openly propagated. 
According to the ruling Communist Party’s official newspaper, 1.1 million local government 
officials have been deployed to spend about a week every two months living in the home of 
#
#185
a Uyghur host family. The official name for this practice is the “cultural exchange” program. 
And the childish Chinese slogan for this is the “Pair Up and Become Family” campaign. 
Teaching of Chinese culture, singing patriotic songs together, attending at night various 
classes on Xi’s vision for a New China are among the activities. Of course, the Chinese “relatives”
watch the Uyghurs all the time, take notes and assess their loyalty to the country as 
well as their level of Chinese language. They ask questions of the hosts as well as their children
in order to collect as much information as possible. In the homestay program, Chinese 
officials also offer the Uyghurs alcohol and non-halal food and see the reaction of the hosts 
and report accordingly. Those who are reported for acting negatively end up in internment 
camps. Such a policy is a human engineering project that is about power, domination and 
exploitation and shows the arrogance and facelessness of the CCP in demonizing Uyghurs 
and having no limits to its project. For Uyghurs, their families and faith are the last space of 
refuge and security, but they are now under Chinese attack and invasion. This is a wellplanned
policy in order to break the spirits of Uyghurs and traumatize them. In fact, one 
Uyghur man in Istanbul told me in May 2019 as follows: “Not only our homeland, but also 
our homes are invaded.” Byler (2018a) provides significant and detailed information on the 
homestay program, which started in 2014, as he managed to interview both his Uyghur and 
Chinese friends who participated in them.
The campaign “Pairing Up and Becoming Family” is also taken to a further level by the 
Chinese authorities and practiced in the form of forced marriages. While Uyghur men are put 
in concentration camps, a lot of Uyghur girls are forced to marry Chinese men in order to rescue 
their relatives from the camps, to denounce their Islamic tradition and to show their loyalty to 
China (Everington, 2018). One can say that such marriages are state-sponsored rape and contribute
to ethnic cleansing and genocide.
The urban internment of Uyghurs also takes the form of a well-planned urbanicide or 
urbicide by Chinese authorities. One might describe urbicide as a deliberate wrecking or “killing”
of a city. Urbicide can be done by razing historical quarters of a city or demolishing build-
ings of great symbolic value. By demolishing the urban spaces that are an essential part of a 
culture, one also destroys that culture itself. In fact, this is what Chinese authorities have 
exactly been doing recently.
China has been continuing with cultural cleansing and has recently increased its demolition
of historic mosques and buildings. A reporter visited the Yizhou District of Xinjiang’s 
city of Hami where he witnessed a lot of demolished mosques. He learned from local officials 
that over 200 of the region’s 800 mosques had already been destroyed, with over 500 scheduled 
for demolition in 2018. Residents said that often their local mosques had disappeared overnight
without any warning (Bitter Winter, 2018c). A recent Guardian and Bellingcat investi-
gation confirmed that more than two dozen Islamic religious sites have been partly or completely 
demolished since 2016 (Kuo 2019). Among the partly destroyed buildings, one witnesses that 
especially minarets or domes are demolished.
Mosques are not the only targets in the urban internment. Whole cities are being redesigned,
allegedly to facilitate maximum security and surveillance of the Uyghurs. The court-
yard houses are often being replaced with five-story concrete block apartment buildings. Even 
sites of architectural interest have been demolished and rebuilt as the government wanted. In 
addition the authorities “have recreated their own concrete Disney version of Old Kashgar in a 
small section, shorn of character and peopled with show Uighurs” (Cecil, 2018). As Harris 
(2019) expresses, this is part of an attempt to destroy an entire society. Again, according to 
James Millward, a China historian at Georgetown University, “[c]ultural cleansing is Beijing’s 
attempt to find a final solution to the Xinjiang problem” (cited in Shih, 2018).
#
#186 ISJ 5(2)
VIRTUAL INTERNMENT
The total internment of Uyghurs would not be complete without virtual internment; 
in fact, it is the essential part of it. Bazian who coined the term virtual internment defines it in 
the context of the US “War on Terror” as “a quasi-visible but repressive, intimidating, and 
confining structure employed by the U.S. administration and its allies on a global scale against 
individuals, communities, and organizations deemed unsupportive, and possibly hostile, in 
their worldview toward American and ‘global’ interests” (Bazian, 2004, pp. 5–6). One should 
note here the adjective “quasi-visible” does not really suit the Chinese case as the internment 
in China (i.e. all the three types I discuss) is done on a massive scale and defended openly by the 
state and it is “proven or directly observable” (unlike the second meaning of the term “virtual” 
mentioned by Bazian). Most other aspects of virtual internment by the USA are common with 
China: virtual internment includes all members of the targeted communities. It focuses on 
forced behavioral change through a mentally induced state of control that aims to shape future 
generations. I should admit that Bazian’s virtual internment includes also detention, renditions 
and prisons, but these are individual cases, some of which are not even easy to prove, while in 
the case of China, more than a million Uyghurs are kept in internment camps openly and the 
practice is officially advocated. That is why (and also for the sake of easier analysis) in this article
I use “virtual internment” more in the sense of internment by surveillance technologies. The 
word internment fits well, since
virtual refers to a total structure directly employed against and experienced by the targeted group 
even when they are not physically held in prisons or subject to legal sanctions. Such a virtual state 
induces in the individual and the community a mode of behavior that is more reflective of that of 
prisoners than of free men and women. (Bazian, 2004, p. 9)
China is building a high-tech authoritarian country and wants to create a vast and 
unprecedented national surveillance system to crush dissent. It invests billions of dollars 
into surveillance technologies. Thousands of companies in China offer smartphone surveillance
equipment, facial-recognition technology, deep-packet inspection gear and application 
filtering. The government also supports research and technologies that track faces, clothing 
and even a person’s gait. Almost all these companies provide data to the government. For 
instance, Eyecool Shenzhen Technology is said to be adding two million facial images each 
day to a flourishing big-data police system called Skynet, which is China’s massive video surveillance
network (Mozur, 2018). Companies like Beijing Hisign Technology compile data 
gathered from smartphones and cameras into an online database of China’s near 1.4 billion 
people. Hisign Technology states that its desktop and portable phone scanners can retrieve even 
deleted data from nearly 100 mobile applications on smart phones, including platforms like 
Facebook and Twitter. The company also has software that analyzes audio files, converts voice 
messages into text and translates minority languages and dialects like Uyghur into Mandarin 
Chinese (Li and Cadell, 2018).
While all these surveillance technologies are used for all the citizens of China, the 
Uyghur Region has extras. China has recently turned the region into a laboratory for new surveillance
technology. Since early 2017, Uyghurs have been living under an increasingly strict 
security regime. Uyghur cities have now armed checkpoints in city centers and streets that are 
full of security cameras and facial recognition-equipped CCTV (closed circuit TV). It is reported 
that hi-tech drones that look and move like real birds fly over the Uyghur region (Chen, 2018). 
The Chinese government has also forced Uyghurs to download and install an application called 
#
#187
Jingwang Weishi (Clean Net Guard) that scans photos, videos, audio files, ebooks and other 
documents. Also, sometimes police officers stop Uyghurs in the street and plug unidentified 
electronic devices into their cellphones to scan them without explanation. Therefore, a simple 
trip to the market or to see friends can take hours because of the unpredictable and intrusive 
nature of the police checks. In addition, Human Rights Watch says authorities in the Uyghur 
Region have forced locals to submit to the collection of biometric data, including voices, blood 
groups, DNA samples, face scans and iris scans. For example, Tahir Imin, a 38-year-old Uighur, 
had his blood drawn, his face scanned and his voice recorded by the authorities who called it a 
“free health check,” about which he had doubts (Wee, 2019).
China uses DNA to track its people, benefitting from the help of American expertise. 
Various technology firms in the United States are providing expertise, reputational credence 
and even technology to Chinese surveillance companies, deliberately or otherwise. For instance, 
the Chinese authorities received help from biotech giant Thermo Fisher (a Massachusetts company)
and Kenneth Kidd (a prominent Yale University geneticist) as they built an enormous 
system of surveillance and control. After this situation became clear, Thermo Fisher stopped 
selling DNA sequencers in the Uyghur Region, but apparently not elsewhere in China (Gorman 
and Schrader, 2019). So this means that Chinese companies in charge can easily buy DNA 
sequencers outside the Uyghur Region and use them in the region as usual. Apparently, some 
other US universities like Princeton University and private foundations have helped fund 
Chinese startups whose technologies are used by the Chinese government to survey and profile 
its own citizens (Mac, Adams and Rajagopalan, 2019).
Using QR (quick response) codes on Uyghur houses or even kitchen utensils is another 
way of surveillance of Uyghurs. We find out that Chinese authorities in the Uyghur Region 
installed QR codes on many Uyghur homes in order to get instant access to the personal details 
of people living there (Embury-Dennis, 2018). In comparison, this seems much more than 
drawing David’s Star on Jews’ homes in Nazi Germany. Further, in the Uyghur Region even 
kitchen utensils are not spared from surveillance and, for example, kitchen knives too are 
tracked with QR codes. So any knife bought in the Uyghur Region, even kitchen knives, must 
now be laser engraved with a QR code unique to the purchaser. Josh Chin, a reporter in China, 
gives an example together with a photo: “In Aksu, we interviewed a salesman at a knife shop 
that had to spend thousands of dollars on a machine that turns a customer’s ID card data into 
a QR code and laser-etches it into every knife they sell” (Chin, 2017). This obviously shows the 
regime’s suspicions and fear of peaceful Uyghurs, who, according to them, might use kitchen 
knives and similar utensils when they feel desperate about government atrocities. If someone 
uses a knife for a crime against government, the QR code on the knife can be spotted by a police 
investigation and reveal the buyer, i.e. owner of that knife and therefore he/she and probably 
his/her whole family will be punished.
Uyghurs living abroad too are under surveillance. Chinese embassies keep an eye on 
Uyghurs living in foreign countries and keep pressuring governments to deport Uyghurs back 
to China. They blackmail students or workers by holding their families hostage back in the 
Uyghur Region. Therefore, it is not possible to say that a particular country is safe for Uyghurs. 
Uyghurs have to keep an eye on their present country’s relationship with China, because if 
Chinese influence increases, so does the danger.
China tracks abroad other Chinese Muslims as well. Chinese Muslims going on the 
annual Hajj pilgrimage to Mecca are equipped with “smart cards” which include GPS trackers. 
The state-run China Islamic Association stated that the purpose of the system is to make the 
trip safer for the pilgrims required to wear the device (Dou, 2018). However, this is probably 
part of China’s widespread surveillance program. It can be observed that even the number of 
#
#188 ISJ 5(2)
Chinese pilgrims in recent years shows government control and restriction of Chinese Muslims. 
In a country where there are tens of millions of Muslims, the number of pilgrims attending the 
pilgrimage in Mecca has been decreasing since 2016. It was 14,500 in 2016, 12,800 in 2017 
and 11,500 in 2018.
In addition to Uyghurs and Chinese critics of the Communist Party living abroad, 
some scholars who write critically about China are under also Chinese surveillance and harassment.
For instance, Anne-Marie Brady, a China expert from New Zealand, has been subject to 
an ongoing harassment and intimidation in her own country, since the publication of her 
(Brady, 2017) academic paper titled “Magic Weapons: China’s Political Influence Activities 
under Xi Jinping,” which details the extent of Chinese influence in New Zealand (Roy, 2019). 
Her home was burgled and her office was broken into twice. She has received anonymous phone 
calls and threatening letters. In short, the Chinese state knows no boundaries in intimidating 
and harassing any human being who is critical of it.
Surveillance Capitalism
What is happening in China with respect to virtual internment is not peculiar to that 
country, but is part of a global phenomenon called “surveillance capitalism.” Surveillance capitalism
has meanings around the commodification of personal information. Basically, it is the 
monetization of data obtained through monitoring people’s behaviors online and in the physical
world. It was first discussed by Foster and McChesney (2014) with a different meaning, and 
later was used and popularized by Zuboff (2019).
Zuboff sees surveillance capitalism as a new variance of capitalism. According to her, it 
was invented around 2001 by Google Inc. and further developed by Facebook (Zuboff, 2015). 
As we know, the new capitalists such as Google, Facebook and WhatsApp provide free services 
which billions of people happily use, and the service providers monitor the behavior of the users 
in detail, often without their explicit consent. By using tools like machine intelligence, artificial
intelligence and algorithms, they analyze our behavioral data and successfully predict our 
future behaviors. Initially our behavioral data were used for commercial purposes and, for 
instance, were sold to companies who wanted to sell their products and services to us. But later, 
things changed drastically. What should be alarming for us is that surveillance capitalism went 
beyond surveillance and began to change our behaviors too. For instance, in one interview 
Zuboff conducted with the chief data scientist of a well-known Silicon Valley company that 
develops applications to improve students’ learning, the scientist told her the following:
The goal of everything we do is to change people’s actual behavior at scale. When people use our 
app, we can capture their behaviors, identify good and bad behaviors, and develop ways to reward 
the good and punish the bad. We can test how actionable our cues are for them and how profitable 
for us. (Zuboff, 2016)
In fact, Facebook made, together with some academics, two massive-scale experiments 
to see if they could change people’s behavior and the result was positive. In the first experiment, 
by presenting subliminal cues in people’s News Feed and on Facebook pages, they actually 
made more people cast real votes in the real world during the 2012 mid-term elections in the 
USA, so they could affect real-world behavior, while bypassing the users’ awareness. A second 
experiment was to see if they could manipulate our emotional state with the same kind of 
methodologies. They found out that they could make us feel either more happy or more sad by 
again using subliminal cues in the online environment (Mack, 2019).
#
#189
The surveillance capitalism and surveillance state go together—in fact the two merge 
(Mosco, 2014, p. 10; Wills, 2017). Surveillance states have complete access to all the data that 
companies have. The partnership formed between state surveillance and capitalist surveillance 
is now dividing citizens everywhere into two groups: the watchers (invisible, unknown and 
unaccountable) and the watched. As expected, this has great impact on democracy, since asymmetries
of knowledge turn into asymmetries of power. While in the past there was at least some 
sort of checks on state surveillance, currently there is hardly any on actors of surveillance, who 
mostly are unaccountable.
According to Zuboff, surveillance capitalism produces a deeply anti-democratic and 
anti-egalitarian power that functions as a coup from above. It is a sort of tyranny that feeds on 
people, although it is paradoxically portrayed and celebrated as “personalization.” It demeans 
and degrades human dignity by undermining principles and practices of self-determination. 
Currently, knowledge, authority and power rest with surveillance capital, for which we are 
simply human natural resources. For Zuboff, the goal now is to change actual human behavior 
to a great extent and in fact to automate us. According to Naughton (2019):
This power to shape behaviour for others’ profit or power is entirely self-authorising. It has no 
foundation in democratic or moral legitimacy, as it usurps decision rights and erodes the processes 
of individual autonomy that are essential to the function of a democratic society.
As for a solution, for Zuboff, this human-made phenomenon must be confronted in 
the realm of politics by mobilizing the resources of democratic institutions, including elected 
officials. Likewise, Richards (2013, p. 1964) believes that surveillance must be constrained by 
legal as well as social rules.
As we have seen, one key feature of surveillance capitalism is its ability to modify 
human behavior. Actually, such a power can be traced back to Bentham’s Panopticon, where 
even the mere existence of surveillance can keep the people in line. For surveillance technology 
to be effective, it does not even have to work all the time. When people do not know if they are 
being monitored at a given time, the uncertainty makes them more obedient. This is just like 
a Panopticon: people will follow the rules exactly because they do not know whether they are 
being watched. As discussed by Foucault (1995) in his Panopticism, the Panopticon provides 
self-control of the people in the target group, their change of behavior as well as experimentation
with these people and all these are done in a much more sophisticated way by surveillance 
technologies. These technologies are also able to record and store information about the target 
people and this enables post-facto punishments of past transgressions, as, for instance, Chinese 
authorities punishes Uyghurs for past “crimes” like texting a friend a congratulatory message 
for a Muslim religious day.
In brief, surveillance capitalism that aims at changing human behavior suits well the 
current context of China, where the ruling Communist Party ideology asserts that social harmony
can only be achieved with the “standardization of human behavior.” However, China’s 
ongoing surveillance on Uyghurs goes beyond surveillance capitalism, which is based mostly 
on the collection and manipulation of the data of people who voluntarily participate in and use 
the social media. In this context, another term, namely “terror capitalism,” might be helpful.
Terror Capitalism
Byler suggests that we are witnessing in the Uyghur Region a “terror capitalism” and 
digital dictatorship which are trying to eliminate the Uyghur culture. According to him, this 
#
#190 ISJ 5(2)
is a project far more extreme than the Stanford Prison Experiment. For him, terror capitalism 
refers to the security industrial complex that is now supporting and running much of the 
economy in the region. In 2009 there was widespread protests, riots and state violence in the 
region and since then the number of private security companies working there has risen to 
more than 1400. Many of these companies are sophisticated in artificial intelligence development.
From the perspective of Chinese state economic planning, the project to control, sup-
press and transform the Uyghur population is a venture capitalist experiment with vast 
potential (Byler, 2018c, p. 328). As to why it should be called terror capitalism, Byler (2018b) 
thinks as follows:
The reason why it is important to refer to the industrial complex as one marked by “terror,” is 
because the label “terror” posits that Uyghur and Muslims more generally pose an existential 
threat to the Chinese nation. As such, Uyghur society can be treated as a space of exception where 
the normal rules regarding basic human rights no longer apply. In China the term “terrorist” is 
generally associated only with bodies marked as Muslim, so it allows Chinese leaders, and the 
Chinese public as a whole, to see it only as a threat associated with a different people in distant 
borderland. Labeling Uyghur society in this way also provides cover for the Chinese state when 
confronted with the fact of their crimes against humanity by international institutions such as the 
United Nations.
China’s policies in the Uyghur Region are “arguably the country’s most intense campaign
of coercive social re-engineering since the Cultural Revolution” (Zenz, 2019, p. 124). The 
state’s “War on Terror” is definitely a euphemism for forced ethnic and cultural assimilation. 
According to Rian Thum, a professor at Loyola University in New Orleans, China’s re-education 
system echoes some of the worst human rights violations in history: “The closest analogue is 
maybe the Cultural Revolution in that this will leave long-term, psychological effects . . . This 
will create a multigenerational trauma from which many people will never recover” (cited in 
Shih, 2018). In my opinion, such a trauma is intended by the Chinese state, which is not able to 
kill Uyghurs in the millions (as the Nazis did in Europe). The trauma aims to break the spirit 
of the Uyghurs and might be worse than killing, as one Uyghur man in Istanbul told me: “It 
does not kill; it makes us worse than killed.” The impact of the trauma might also be felt in 
what another Uyghur man in Istanbul told me: “Sometimes I wonder if what I experience is 
real or just a nightmare.”
MOTIVES AND REASONS FOR THE TOTAL INTERNMENT
What might be behind the total internment of Uyghur Muslims? Obviously, one factor 
is Islamophobia: China sees and treats Islam like a mental illness. In fact, in China in general, 
religious belief is seen as pathology. As we mentioned, the authorities also call the internment 
camps for Uyghurs, hospitals. The authorities believe that Uyghurs are infected with the disease
of Islam. Here is an excerpt from an official Communist Party audio recording, which was 
transcribed and translated by Radio Free Asia (2018):
Members of the public who have been chosen for reeducation have been infected by an ideological 
illness. They have been infected with religious extremism and violent terrorist ideology, and 
therefore they must seek treatment from a hospital as an inpatient . . . The religious 
extremist ideology is a type of poisonous medicine, which confuses the mind of the people . . . If 
#
#191
we do not eradicate religious extremism at its roots, the violent terrorist incidents will grow and 
spread all over like an incurable malignant tumor. (Emphasis mine)
The Communist Party audio message also tries to justify even the practice of interning 
innocent people who have not committed any crime with the risk of illness, because those Uyghur 
Muslims, just by being Muslims, are already infected with the disease (Radio Free Asia, 2018):
Although a certain number of people who have been indoctrinated with extremist ideology have not 
committed any crimes, they are already infected by the disease. There is always a risk that the 
illness will manifest itself at any moment, which would cause serious harm to the public. That is 
why they must be admitted to a re-education hospital in time to treat and cleanse the virus from 
their brain and restore their normal mind. We must be clear that going into a re-education 
hospital for treatment is not a way of forcibly arresting people and locking them up for punishment, 
it is an act that is part of a comprehensive rescue mission to save them. (Emphasis mine)
The message ends with a warning, which explains the reason why the internment camps 
are not enough and why the total interment of the Uyghurs must be practiced permanently:
However, we must be cautious about one fact: having gone through re-education and recovered from 
the ideological disease doesn’t mean that one is permanently cured . . . After recovering from an 
illness, if one doesn’t exercise to strengthen the body and the immune system against disease, it could 
return worse than before.
Žižek likens this “rescue mission” (i.e. civilizing mission) in the form of “medicalization”
to the attitude of the “notorious Sherbsky Institute in Moscow which treated dissidence 
as a form of mental illness” (Žižek, 2019).
Then what is the solution for the mental and ideological disease of Islam in China? Can 
quarantine or elimination be a solution, as Roberts (2018, p. 21) suggests?
As in many countries engaged in “wars on terror,” in China the terrorist label has come to 
represent a virtual biological threat to the social order. This is a threat to the health of the 
organism it imagines as China’s “harmonious society,” a threat that can only be mitigated by 
quarantine or elimination.
For the Chinese authorities, “Sinicization” of Islam is the final solution and indeed it 
involves first the quarantine of Uyghurs and then elimination of them as a separate ethnic-religious
group. Chinese authorities often mention that Sinicization of religion must be upheld. 
In fact, the push to “Sinicize religion,” which was introduced by President Xi Jinping in 2015, 
is an attempt by the officially atheist Communist Party to bring religions under its absolute 
control and into line with Chinese culture. Sinicization of religion is meant for Muslims as well 
as Christians, but in the case of Uyghurs it goes much beyond Sinicization, as the Uyghurs are 
not even allowed to practice the very basic requirements of their religion such as daily prayers, 
fasting and going to mosques. Moreover, Sinicization is based on the superiority of the Han 
Chinese and their culture as well as their control of political power and therefore it is “hierarchizing”
rather than egalitarian and democratic (Berlie, 2004, p. 140). Thus Sinicization 
means to Uyghurs total assimilation and religious persecution. The Sinicization of Islam in 
China is planned to be complete within four years (Baynes, 2019).
#
#192 ISJ 5(2)
In addition to the attempts of Sinicization of Islam, as far as I see, there are other 
pressing elements for Beijing to pursue a definitive solution to the “Uyghur question.” One 
can discuss at least three economic and geopolitical motives and reasons for the genocidal policies
carried out by the Chinese government in the Uyghur Region. I will cite them very 
briefly: first, the Uyghur Region contains rich coal, natural gas and oil resources and has 
extensive deposits of silver, copper, lead, nitrates, gold and zinc. All these are very important 
for the flourishing Chinese economy. Second, the Uyghur Region is China’s key land route to 
the world and is seen as the core hub of the Belt and Road Initiative, which tries to create 
trade corridors between Beijing and the rest of the world. For this project, the region is a critical
location that will serve as the passage for all economic expansion into Central Asia, South 
West Asia and Europe. It will connect China to Pakistan, Turkey and Russia. Finally, the US 
domination of the sea routes makes the Uyghur Region essential for China, which tries to 
avoid excessive dependence on vulnerable sea routes. Currently, the Chinese economy is fairly 
dependent on the sea routes that China cannot secure all the time. This is because the US navy 
controls the world’s oceans and any blockade of the small islands that surround China in the 
region or of crucial points at the sea routes can cripple the Chinese economy (Clarke, 2018). 
Therefore, the Belt and Road Initiative can be seen as part of the reason for genocidal policies 
towards the Uyghurs.
CONCLUSION
To conclude, Chinese Islamophobia is a deliberate state policy serving the plans of a 
would-be global hegemon. It is based on a tradition of Chinese political culture with its ruthless
crushing of any dissent as well as the delusions of one man who wishes to rule the most 
populous country of the earth until he dies. Today Uyghurs are victims of a Chinese blend of an 
old tradition (Maoism) and new capitalism (surveillance capitalism and terror capitalism). In 
fact, the current Chinese regime is an anomaly with a capitalist economy run by a so-called 
Communist Party headed by one man. Obviously, such a rule is neither really capitalist nor 
communist, but an authoritarian regime moving towards totalitarianism.
As it has been clear in the last 30 years since the Tiananmen Massacre, in the Chinese 
case, economic growth and technological progress do not bring improvements in human rights, 
democracy and freedom, but rather consolidate control and authoritarianism (and even have 
brought totalitarianism to the Uyghurs). Today, the Uyghur Region is the living embodiment 
of the Orwellian dystopia of 1984. The regime with its inhuman policies is currently carrying 
out genocide through concentration camps, surveillance capitalism and terror capitalism.
However, China is now subject to sousveillance, i.e. bottom-up surveillance. That is to 
say, the master of surveillance itself is under surveillance or the watcher is also watched globally.
Satellite imagery, by which activists have been discovering and displaying newly con-
structed internment camps and bulldozed historical quarters and religious buildings, secret 
photos and videos from inside concentration camps, testimonies and experiences of the Uyghur 
victims found in the media all over the world expose globally the crimes of the Chinese state. 
Official denials, propaganda, blackmailing and misinformation do not succeed in covering the 
atrocities committed by the Chinese state.
As for my conjectures about the fate of the Uyghurs in the near future, I would like to 
cite them as follows: given the ruthlessness and facelessness of the Chinese state, in the short 
run, the already awful situation might deteriorate for the Uyghurs. Provocative and genocidal 
Chinese policies will probably continue, as the authorities aim at an alleged final solution but 
such policies will further alienate and radicalize many Uyghurs who already feel that they have 
#
#193
nothing to lose. The Chinese state is in fact preparing the groundwork for the very extremism 
it says it wants to prevent. In addition, Uyghurs might also be further victims of the hegemonic
struggle and a prospective clash between the USA and China.
The oppressive Chinese Islamophobic policies are now slowly spreading into Chinese 
Muslim communities too. The Hui (Muslim Chinese) people are regularly targeted by the CCP 
in “Sinicization” campaigns that include the destruction of mosques and removal of domes, 
Islamic decor and Arabic signs in Chinese Muslim areas. As the regime considers Islam a mental
disease, sooner or later it will approach Chinese Muslims too for treatment and 
re-education.
Further, Chinese oppression in the Uyghur region will eventually lead to deteriorate its 
relations with Muslim countries. With its investments and business partnerships, China has 
been buying the silence of many Muslim countries, but as various experts assert, business as 
usual is no longer possible. Reactions are already developing at the popular level in countries 
like Turkey, Pakistan, Malaysia and Indonesia and popular resentment and protests will probably
exert impact soon on the governments in those countries. Also, one should remember that 
from the Uyghur Region to Turkey, almost all the countries and peoples are Turkic Muslims, 
just like Uyghurs are. So no one in their right mind would attempt to annihilate Uyghurs by 
declaring a war against Islam at home, while at the same time trying to revive the Silk Road 
together with those Turkic Muslim countries and also do business with nearly 50 Muslim 
countries. In my opinion, the current insane Chinese strategy is nothing but a result of the 
intoxication of power.
China’s attempts to demonize all Uyghur opposition as “terrorism” have already backfired
by raising the Uyghur Region and Uyghur issues to an unprecedented level of international 
prominence (Clarke, 2011, p. 175). Chinese oppression, especially after the proliferation of the 
internment camps, is now widely noted in the West and the media all over the world have been 
writing about atrocities, internment camps and surveillance. Although some of this interest in 
spotting and displaying crimes against humanity committed in China is for political purposes, 
rather than sincere concern with the fate of the victims, the plight and sufferings of Uyghurs are 
read almost every day. In brief, China cannot go on any more as if nothing has happened.
Indeed, we have begun to see some official reactions as well. More than a dozen states, 
including Turkey, Australia, Germany and the United States, have called on China to close 
down the concentration camps and release the detainees. In early February 2019 Turkey’s 
Foreign Ministry made the following comment on the camps: “The reintroduction of internment
camps in the 21st century and the policy of systematic assimilation against the Uighur 
Turks carried out by the authorities of China is a great shame for humanity.” In addition, there 
are some positive developments at the legal level too: Sweden now grants refugee status to all 
Uyghurs from Xinjiang. The Swedish migration agency said that the minority would be automatically
considered at risk of persecution in Xinjiang province.
At the individual level too, we see very positive developments. For instance, an initiative
of Concerned Scholars from nearly 45 countries issued a statement on 26 November 2018 
condemning China for internment camps and proposed policy suggestions. Some other scholars 
have started discussing starting a boycott, divestment and sanctions (BDS) movement against 
China. In fact some such actions have already started, although for different reasons, for example,
the recent US policies and actions against China’s high-tech companies like Huawei and 
high tariffs for goods imported from China etc. After a few weeks, China had already begun to 
feel its negative economic impacts.
Last but not least, Uyghurs in general are resisting, in spite of the awful situation 
they are in and the trauma they have been experiencing. For instance, they are well organized 
#
#194 ISJ 5(2)
in Kazakhstan and trying to do the same in Turkey. From Japan to the USA, they are successful
in informing the world about their plight and promoting their causes. They do not 
give up but fight.
In conclusion, in my opinion, a state which puts millions of its citizens into concentration
camps in order to brainwash them and blackmails its citizens studying or working abroad 
by treating their children or parents in Xinjiang as hostages does not have a chance to become 
a world power. A state that is run like a criminal gang will sooner or later destroy its own image 
and in fact that process has already started. Financial and political power are not free from one’s 
image and neither Chinese investments abroad nor the Confucius Institutes that China opens 
abroad are enough to win the hearts and minds of people. As Hann (2014, p. 205) has expressed 
so well for China,
[t]he evidence of recent decades suggests that the pursuit of the harmonious society, the overt goal 
of power holders, may be incompatible with the pursuit of the homogeneous society, the covert goal. 
One or the other objective will eventually have to be abandoned.
REFERENCES
ASPI Database. “ASPI ICPC Xinjiang Re-education Camp Database.” Accessed May 1, 2019. 
https://docs.google.com/spreadsheets/d/e/2PACX-1vR48u6lKYD21gv6mqM-2dV2
lL8axuJ3yG5QJr2KNfG6bZNhy2dXDib_ZyFl9QKwvTRP0EBKZPYczwp9/
pubhtml
Baynes, Chris. 2019. “China Passes Law to ‘Make Islam More Compatible with Socialism’ 
Amid Outcry over Muslim Abuse.” The Independent, January 7. Accessed May 30, 
2019. www.independent.co.uk/news/world/asia/china-uighur-muslim-crackdownxinjiang-islam-united-nations-sinicize-a8715506.html
Bazian, Hatem. 2004. “Virtual Internment: Arabs, Muslims, Asians and the War on 
Terrorism.” The Journal of Islamic Law and Culture 9, no. 1: 1–26.
Berlie, Jean A. 2004. Islam in China: Hui and Uyghurs Between Modernization and Sinicization. 
Bangkok: White Lotus.
Bitter Winter, 2018a “A Jail by Any Other Name: Xinjiang Re-education Camp 
for Uyghurs.” November 26. Accessed May 25, 2019. www.youtube.com/
watch?v=uQHuOfKgNsI
Bitter Winter. 2018b. “Xinjiang Camp’s Prison Like Management Regulations Exposed.” 
December 11. Accessed May 26, 2019. www.youtube.com/watch?v=RPl5PErfL0M
Bitter Winter. 2018c. “Chinese Government Demolishes Mosques in Xinjiang.” June 19. 
Accessed May 28, 2019. https://bitterwinter.org/chinese-government-demolishesmosques-in-xinjiang/
Brady, Anne-Marie. 2017. “Magic Weapons: China’s Political Influence Activities Under Xi 
Jinping.” Clingendael Institute. Accessed May 15, 2019. www.wilsoncenter.org/sites/
default/files/media/documents/article/magic_weapons.pdf
Buckley, Chris, and Austin Ramzy. 2018. “China’s Detention Camps for Muslims Turn to 
Forced Labor.” The New York Times, December, 16. Accessed May 30, 2019. www.
nytimes.com/2018/12/16/world/asia/xinjiang-china-forced-labor-camps-uighurs.html
Busby, Scott. 2018. “Testimony of Deputy Assistant Secretary Scott Busby Senate Foreign 
Relations Committee Subcommittee on East Asia, the Pacific, and International 
#
#195
Cybersecurity Policy.” United States Senate Committee on Foreign Relations, 
December 4. Accessed May 13, 2019. www.foreign.senate.gov/imo/media/
doc/120418_Busby_Testimony.pdf
Byler, Darren. 2018a. “China’s Nightmare Homestay.” Foreign Policy, October 26. Accessed 
May 30, 2019. https://foreignpolicy.com/2018/10/26/china-nightmare-homestayxinjiang-uighur-monitor/
Byler, Darren. 2018b. “Xinjiang Specialist Darren Byler for Sinopsis: A Project Far More 
Extreme than the Stanford Prison Experiment”, Sinopsis, November 21. Accessed 
May 30, 2019. https://sinopsis.cz/en/xinjiang-specialist-darren-byler-for-sinopsis-aproject-far-more-extreme-than-the-stanford-prison-experiment/
Byler, Darren T. 2018c. “Spirit Breaking: Uyghur Dispossession, Culture Work and Terror 
Capitalism in a Chinese Global City.” PhD diss., University of Washington.
Cecil, Rose. 2018. “How the Chinese Government Destroyed Kashgar, Fulcrum of the Silk 
Road.” Reaction, October 5. Accessed May 30, 2019. https://reaction.life/orwellianchinese-government-destroyed-kashgar-fulcrum-silk-road/
Chen, Stephen. 2018. “China Takes Surveillance to New Heights with Flock of Robotic 
Doves, But Do They Come in Peace?” South China Morning Post, June 24. Accessed 
May 30, 2019. www.scmp.com/news/china/society/article/2152027/china-takessurveillance-new-heights-flock-robotic-doves-do-they
Chin, Josh. 2017. Twitter, December 19. Accessed May 30, 2019. https://twitter.com/
joshchin/status/943159015994880000/photo/1
Clarke, Michael. 2018. “The Belt and Road Initiative: Exploring Beijing’s Motivations 
and Challenges for its New Silk Road.”, Strategic Analysis 42, no. 2: 84–102, DOI: 
10.1080/09700161.2018.1439326.
Clarke, Michale E. 2011. Xinjiang and China’s Rise in Central Asia: A History. London and 
New York: Routledge.
Cockburn, Harry. 2018. “Muslim Woman Describes Torture and Beatings in China 
Detention Camp: ‘I Begged Them to Kill Me’.” The Independent, November 28. 
Accessed May 30, 2019. www.independent.co.uk/news/world/asia/uighur-muslimchina-mihrigul-tursun-torture-reeducation-camps-a8656396.html
Dou, Eva. 2018. “Chinese Surveillance Expands to Muslims Making Mecca Pilgrimage.” The 
Wall Street Journal, July 31. Accessed May 30, 2019. www.wsj.com/articles/chinesesurveillance-expands-to-muslims-making-mecca-pilgrimage-1533045703
Embury-Dennis, Tom. 2018. “China Installing QR Codes on Uyghur Muslim Homes as 
Part of Mass Security Crackdown.” The Independent, September 11. Accessed April 
15, 2019. www.independent.co.uk/news/world/asia/china-uyghur-muslims-xinjiangprovince-qr-codes-security-crackdown-hrw-a8532156.html
Everington, Keoni. 2018. “Netizens Outraged by ‘Gene Washing’ Wedding between Chinese 
Man and Uyghur Woman.” Taiwan News, May 28. Accessed April 5, 2019. www.
taiwannews.com.tw/en/news/3442256
Fischer, Andrew Martin. 2014. “Labour Transitions and Social Inequalities in Tibet and 
Xinjiang: A Comparative Analysis of the Structural Foundations of Discrimination 
and Protest.” In On the Fringes of the Harmonious Society: Tibetans and Uyghurs in 
Socialist China, edited by Trine Brox and Ildikó Bellér-Hann, 29–68. Copenhagen: 
Nias Press.
Foster, John Bellamy, and Robert W. McChesney. 2014. “Surveillance Capitalism: 
Monopoly-Finance Capital, the Military-Industrial Complex, and the Digital 
#
#196 ISJ 5(2)
Age.” Monthly Review, July 1. Accessed April 3, 2019. https://monthlyreview.
org/2014/07/01/surveillance-capitalism/
Foucault, Michel. 1995. Discipline And Punish: The Birth of the Prison, translated by Alan 
Sheridan. New York: Vintage Books.
Fu Hualing. 2005. “Re-Education through Labour in Historical Perspective.” The China 
Quarterly no. 184 (December): 811–830.
Gorman, Lindsay and Matt Schrader. 2019. “U.S. Firms Are Helping Build China’s 
Orwellian State.” Foreign Policy, March 19. Accessed May 15, 2019. https://
foreignpolicy.com/2019/03/19/962492-orwell-china-socialcredit-surveillance/
Greer, Tanner. 2018. “48 Ways to Get Sent to a Chinese Concentration Camp.” 
Foreign Policy, September 13. Accessed May 21, 2019. https://foreignpolicy.
com/2018/09/13/48-ways-to-get-sent-to-a-chinese-concentration-camp/
Hann, Chris. 2014. “Harmonious or Homogeneous? Language, Education and Social 
Mobility in Rural Uyghur Society.” In On the Fringes of the Harmonious Society: Tibetans 
and Uyghurs in Socialist China, edited by Trine Brox and Ildikó Bellér-Hann, 183–
208. Copenhagen: Nias Press.
Harris, Rachel. 2019. “Bulldozing Mosques: the Latest Tactic in China’s War against Uighur 
Culture.” The Guardian, April 7. Accessed April 25, 2019. www.theguardian.com/
commentisfree/2019/apr/07/bulldozing-mosques-china-war-uighur-culture-xinjiang
Human Rights Watch. 2017. “Tibet: A Glossary of Repression.” Accessed April 12, 2019. 
https://www.hrw.org/video-photos/interactive/2017/06/20/tibet-glossary-repression
Human Rights Watch. 2018. ““Eradicating Ideological Viruses”: China’s Campaign of 
Repression Against Xinjiang’s Muslims.” Accessed April 28, 2019. www.hrw.org/
report/2018/09/09/eradicating-ideological-viruses/chinas-campaign-repressionagainst-xinjiangs
Kuo, Lily. 2019. “Revealed: New Evidence of China’s Mission to Raze the Mosques of 
Xinjiang.” The Guardian, May 7. Accessed May 28, 2019. www.theguardian.com/
world/2019/may/07/revealed-new-evidence-of-chinas-mission-to-raze-the-mosquesof-xinjiang
Li Ruohan. 2018. “Linxia Vows to Fight against Pan-Halal Tendency to Safeguard Ethnic 
Unity.” Global Times, July 3. Accessed May 21, 2019. www.globaltimes.cn/
content/1092308.shtml
Li, Pei and Cate Cadell. 2018. “At Beijing Security Fair, an Arms Race for Surveillance 
Tech.” Reuters, May 30. Accessed May 21, 2019. www.reuters.com/article/us-chinamonitoring-tech-insight/at-beijing-security-fair-an-arms-race-for-surveillance-tech-
idUSKCN1IV0OY
Mac, Ryan, Rosalind Adams and Megha Rajagopalan. 2019. “US Universities and Retirees 
are Funding the Technology behind China’s Surveillance State.” BuzzFeed News, 
May 30. Accessed June 3, 2019. www.buzzfeednews.com/article/ryanmac/us-moneyfunding-facial-recognition-sensetime-megvii
Mack, Zachary. 2019. “Shoshana Zuboff on Surveillance Capitalism.” The Verge, March 
26. Accessed April 17, 2019. www.theverge.com/2019/3/26/18282360/age-ofsurveillance-capitalism-shoshana-zuboff-data-collection-economy-privacy-interview-
vergecast
Made in China. 2018. “Xinjiang “Re-education Camps”, China.” October 30. Accessed May 
23, 2019. www.youtube.com/watch?v=GMBqamNfb0o
McDonell, Stephen. 2014. “China’s Hotbed: Who Created the Violence?” ABC News, 
September 29. Accessed April 21, 2019. www.abc.net.au/news/2014-09-29/
mcdonell-chinas-hotbed:-who-created-the-violence/5775492
#
#197
Mosco, Vincent. 2014. To the Cloud: Big Data in a Turbulent World. Colorado: Paradigm 
Publishers.
Mozur, Paul. 2018. “Inside China’s Dystopian Dreams: A.I., Shame and Lots of Cameras.” 
The New York Times, July 8. Accessed May 21, 2019. www.nytimes.com/2018/07/08/
business/china-surveillance-technology.html
Naughton, John. 2019. “‘The Goal is to Automate Us’: Welcome to the Age of Surveillance 
Capitalism.” The Guardian, January 20. Accessed May 20, 2019. www.theguardian.
com/technology/2019/jan/20/shoshana-zuboff-age-of-surveillance-capitalism-googlefacebook
Panda, Damodar. 2006. “Xinjiang and Central Asia: China’s Problems and Policy 
Implications.” Indian Journal of Asian Affairs 19, no. 2 (December): 29–44.
Radio Free Asia. 2018. “Xinjiang Political ‘Re-Education Camps’ Treat Uyghurs ‘Infected by 
Religious Extremism’: CCP Youth League.” Accessed March 20, 2019. www.rfa.org/
english/news/uyghur/infected-08082018173807.html
Ramzy, Austin. 2019. “China Targets Prominent Uighur Intellectuals to Erase an Ethnic 
Identity.” New York Times, January 5. Accessed May 26, 2019. www.nytimes.
com/2019/01/05/world/asia/china-xinjiang-uighur-intellectuals.html
Richards, Neil M. 2013. “The Dangers of Surveillance.” Harvard Law Review 126, no. 7 
(May): 1934–1965.
Roberts, Sean R. 2018. “The Biopolitics of China’s “War on Terror” and the 
Exclusion of the Uyghurs.” Critical Asian Studies 50, no. 2: 1–27. DOI: 
10.1080/14672715.2018.1454111.
Roy, Eleanor Ainge. 2019. “‘I’m Being Watched’: Anne-Marie Brady, the China Critic 
Living in Fear of Beijing.” The Guardian, January 23. Accessed May 10, 2019. www.
theguardian.com/world/2019/jan/23/im-being-watched-anne-marie-brady-the-chinacritic-living-in-fear-of-beijing
Ryan, Fergus, Danielle Cave and Nathan Ruser. 2018. “Mapping Xinjiang’s ‘Re-education’ 
Camps.” ASPI, November 1. Accessed May 25, 2019. www.aspi.org.au/report/
mapping-xinjiangs-re-education-camps
Sharman, Jon. 2019. “China ‘Forcing Muslims to Eat Pork and Drink Alcohol’ for Lunar 
New Year Festival.” The Independent, February 7. Accessed May 29, 2019. www.
independent.co.uk/news/world/asia/china-muslims-xinjiang-pork-alcohol-lunar-newyear-spring-festival-uighur-islam-a8767561.html
Shelton, Tracey and Iris Zhao. 2019. “China Cracks down on Ramadan Fasting, Prompting 
Activist Boycott of Chinese products.” ABC News, May 8. Accessed May 9, 2019. 
https://mobile.abc.net.au/news/2019-05-07/china-cracks-down-on-fasting-duringramadan/11082244
Shih, Gerry. 2018. “China’s Mass Indoctrination Camps Evoke Cultural Revolution.” AP 
News, May 18. Accessed May 27, 2019. https://apnews.com/6e151296fb194f85ba69
a8babd972e4b
Shih, Gerry and Dake Kang. 2018. “Muslims Forced to Drink Alcohol and Eat Pork 
in China’s ‘Re-education’ Camps, Former Inmate Claims.” The Independent, 
May 18. Accessed April 29, 2019. www.independent.co.uk/news/world/asia/
china-re-education-muslims-ramadan-xinjiang-eat-pork-alcohol-communist-xijinping-a8357966.html
Smith, Nicola. 2018. “Chinese Authorities Accused of Cutting Uighur Dresses in Latest 
Crackdown on Muslim Minority.” The Telegraph, July 17. Accessed April 20, 2019. 
www.telegraph.co.uk/news/2018/07/17/chinese-authorities-accused-cutting-uighur-
dresses-latest-crackdown/
#
#198 ISJ 5(2)
Soliev, Nodirbek. 2019. “Uyghur Violence and Jihadism in China and Beyond.” Counter 
Terrorist Trends and Analyses 11, no. 1 (January): 71–75.
Toops, Stanley. 2004. “Demographics and Development in Xinjiang after 1949.” 
East West Center, May. Accessed April 13, 2019. https://web.archive.org/
web/20070716193518/http://www.eastwestcenter.org/fileadmin/stored/pdfs/
EWCWwp001.pdf
UN Human Rights (The Office of the High Commissioner for Human Rights). 2018. 
“Committee on the Elimination of Racial Discrimination Reviews the Report 
of China.” Accessed May 20, 2019. www.ohchr.org/EN/NewsEvents/Pages/
DisplayNews.aspx?NewsID=23452&LangID=E
Wang, Yanan and Dake Kang. 2018. “Empty Places at the Table: Uighur Children Missing 
in China.” Fox News, September 20. Accessed May 1, 2019. www.foxnews.com/
world/empty-places-at-the-table-uighur-children-missing-in-china
Wee, Sui-Lee. 2019. “China Uses DNA to Track Its People, With the Help of American 
Expertise.” The New York Times, February 21. Accessed April 13, 2019. www.
nytimes.com/2019/02/21/business/china-xinjiang-uighur-dna-thermo-fisher.html
Wills, Jocelyn. 2017. Tug Of War: Surveillance Capitalism, Military Contracting, and the Rise of 
the Security State. Montreal: McGill-Queen’s University Press.
Xinjiang Victims Database, www.shahit.biz/eng/. Accessed June 15, 2019.
Yu Jianrong. 2010. “The Two Stages of the Re-education Through Labour System: From 
Tool of Political Struggle to Means of Social Governance.” Translated by Stacy 
Mosher. China Perspectives 82, no. 2: 66–72.
Zaili, Li. 2018. “Uyghurs Secretly Moved to Hide Mass Detentions.” Bitter Winter, December 
17. Accessed May 25, 2019. https://bitterwinter.org/uyghurs-moved-to-hide-massdetentions
Zaili, Li. 2019. ““Hanification”: Uyghur Children Cut off from Their Roots.” Bitter Winter, 
March 23. Accessed April 25, 2019. https://bitterwinter.org/uyghur-children-cut-offfrom-their-roots/
Zeng, Jennifer. 2019. “Survivor of Persecution in China Warns About Regime’s Treatment of 
Uyghurs.” The Epoch Times, April 26. Accessed May 11, 2019. www.theepochtimes.
com/survivor-of-persecution-in-china-warns-about-regimes-treatment-ofuyghurs_2894675.html
Zenz, Adrian. 2019. “‘Thoroughly Reforming Them towards a Healthy Heart Attitude’: 
China’s Political Re-education Campaign in Xinjiang.” Central Asian Survey 38, no. 1: 
102–128. DOI: 10.1080/02634937.2018.1507997.
Zhang, Shawn. 2018. “List of Re-education Camps in Xinjiang.” Medium. Accessed May 28, 
2019. https://medium.com/@shawnwzhang
Žižek, Slavoj. 2019. “From China to the APA, ‘Medicalisation’ is Used to Dismiss Political 
Opponents.” The Spectator, March 4. Accessed May 28, 2019. https://life.spectator.co.uk/
articles/from-china-to-the-apa-medicalisation-is-used-to-dismiss-political-opponents/
Zuboff, Shoshana. 2015. “Big Other: Surveillance Capitalism and the Prospects of an 
Information Civilization.” Journal of Information Technology 30, no. 1: 75–89. 
DOI:10.1057/jit.2015.5.
Zuboff, Shoshana. 2016. “The Secrets of Surveillance Capitalism.” Frankfurter Allgemeine 
Zeitung, March 5. Accessed April 5, 2019. www.faz.net/aktuell/feuilleton/debatten/
the-digital-debate/shoshana-zuboff-secrets-of-surveillance-capitalism-14103616.html
Zuboff, Shoshana. 2019. The Age of Surveillance Capitalism: The Fight for a Human Future at the 
New Frontier of Power. New York: Public Affairs.
Research Article
A New Early Rumor Detection Model Based on BiGRU
Neural Network
Xiangning Chen,1,2 Caiyun Wang,1,2 Dong Li,1,2 and Xuemei Sun 1,2
1School of Computer Science and Technology, Tiangong University, Tianjin 300387, China
2Tianjin Key Laboratory of Autonomous Intelligence Technology and Systems, Tianjin, China
Correspondence should be addressed to Xuemei Sun; sunxuemei@tiangong.edu.cn
Received 20 May 2021; Accepted 4 August 2021; Published 18 August 2021
Academic Editor: Juan L. G. Guirao
Copyright © 2021 Xiangning Chen et al. (is is an open access article distributed under the Creative Commons Attribution
License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is
properly cited.
With the progress of society and the rapid development of computer technology, rumors arise on social media, which seriously
affects the social economy. How to detect rumors accurately and rapidly has become one hot research topic. In this paper, a new
early rumor detection model is proposed. (e aim of this model is to increase the efficiency and the accuracy of rumor detection
simultaneously. Specifically, in this model, the input data is firstly refined through account filtering and data standardization, then
the BiGRU is used to consider the context relationship, and a reinforcement learning algorithm is applied to detection. Experimental
results show that compared with other early rumor detection models (e.g., checkpoints), the accuracy of the proposed
model is improved by 0.5% with the same speed, which testifies the effectiveness of this model.
1. Introduction
Rumors refer to statements that have no corresponding
factual basis but are fabricated and promoted through
certain means. In today’s highly developed situation of
information dissemination media, it can spread quickly
through social media, and malicious rumors may affect
economy and society significantly. (e negative impact of
rumors may increase significantly when certain major events
occur, such as the traceability of COVID-19 in 2019. (is
makes people realize that if malicious rumors are not discovered
in time, they may continue to cause significant
damage, so the timing of their detection is crucial. Figure 1
shows an example of a rumor propagating on Twitter which
is named “German Wings Crash.” (e source message
started a claim about the crash could be an Airbus A320
German wings. A German wing Airbus A320 with 150
people on board crashed in Barcelonite, southern France,
with no one surviving. (e message was retweeted by
multiple users on Twitter, either by reposting, commenting,
or questioning the original source message. We extracted
several related tweets within 24 hours.
Currently, the most research on rumors uses the Twitter
social platform and Weibo platform as the main research
objects. Considering that the research of rumor detection
technology on the Weibo dataset has been relatively mature,
and the latest accuracy rate has reached about 95%, this
paper uses public standard Twitter dataset as the main research
object. Unlike funny videos and celebrity gossips that
are popular onWeibo, hot social events are the most popular
topics on Twitter.
Based on the platform of Twitter, many researchers have
conducted research on rumor detection. When most researchers
focus on improving the accuracy of rumor de-
tection, Ma et al. [1] and Kwon et al. [2] have proposed the
use of presettings in recent years. (e method of fixed
checkpoints can evaluate the timeliness of the discovery of
rumors, but this method has the disadvantage of not being
able to capture the changes in different rumors spreading
modes. On this basis, Farajtabar et al. [3] proposed a
combination of reinforcement learning and a point process
network activity model to detect false news and achieved
good results. (is assessment of the timeliness of rumor
detection is also one of the focuses of this paper.
Hindawi
Discrete Dynamics in Nature and Society
Volume 2021, Article ID 2296605, 11 pages
https://doi.org/10.1155/2021/2296605
#
#At the same time, although data preprocessing has been
widely used in NLP (Natural Language Processing) to improve
performance, few researchers conduct preprocessing
on target data for rumor detection. Moreover, this paper
finds that most studies focus on the extraction of various
features the Twitter standard dataset to capture rumor indicators
but ignore the data complexity on Twitter as a large
social platform. Firstly, Twitter has a large number of spam
accounts. (e tweets sent by these accounts usually have a
certain commercial purpose, and the research on rumor
detection for these tweets is usually weakly relevant or invalid.
(ey are intended to be included in the tweets for this
paper. (e extracted value information has caused serious
interference, so it is necessary to filter it out. Secondly, the
short text and randomness of tweets in Twitter makes it
difficult for people and computers to accurately understand
the information of tweets, so it is necessary to preprocess the
tweets to standardize the tweets on the Twitter standard
dataset.
(e key contributions of our work are summarized as
follows:
(1) In rumor detection, for the early rumor detection
with checkpoints, this paper proposes to apply
BiGRU to the early rumor detection model. Using
BiGRU to consider the characteristics of context
relations, combined with the sequence before and
after, the two posts input before and after the event
are included in the detection, so as to improve the
effect of rumor detection.
(2) In rumor detection, aiming at the early rumor detection
with checkpoints, this paper proposes a data
preprocessing method based on account filtering and
text standardization for the first time. (e account
filtering method is used to remove the junk account.
Text standardization is used to standardize tweets in
Twitter standard dataset so that the data can better
express the meaning of the text so as to improve the
accuracy of rumor detection.
(3) In this paper, Q-learning, a reinforcement learning
algorithm, is applied for rumor detection to dynamically
determine checkpoints, thereby improving
the timeliness of rumor detection.
2. Related Work
In the contemporary era of the emergence of various social
media, rumor detection has attracted the attention of all
parties, and the current research on rumor detection has
achieved initial results. (e current research on rumor
detection is mainly divided into two categories: one is based
on traditional machine learning for rumor detection, and the
other is based on feature learning to extract main features for
rumor detection.
ID: 580317556516483072
Favorite Count: 37
Followers Count: 57279
“We got reports the crash could
be an Airbus A320
Germanwings between
Barcelona and Dusseldorf.”
2015-03-24
10:37:41
START
13:44:51
10:54 :44
14:43:47
2015-03-25
11:46:54
12:24:34
Germanwings Crash
ID: 580706841769820160
Favorite Count: 17
Followers Count: 328255
“At least 72 Germans and 35
Spanish killed in Germanwings
crash: Airline.” 
ID: 580379486732066817
Favorite Count: 45
Followers Count: 3360512
“Distress call linked with
Germanwings jet came from
air traffic control, not plane-aviation
official.”
ID: 580321844089212928
Favorite Count: 94
followers Count: 891879
“Airbus A320 with 142
passengers and 6 crew
members crashes in Digne,
France 4U9525.”
ID: 580364656608428032
Favorite Count: 20
Followers Count: 1452091
“AFP: Aviation official says
Germanwings crew did not
send u distress signal.”
ID: 580697361799876608
favorite Count: 69
followers Count: 4972128
“Reports of “moving body”
amidst Germanwings
wreckage A320.”
Figure 1: Rumors spread about the schematic “German Wings Crash” events (excerpt).
2 Discrete Dynamics in Nature and Society
#
#Rumor detection methods based on traditional machine
learning mainly use decision trees, SVM, and other classifiers
to classify events. Liang et al. [4] no longer use a single
classifier, such as KNN and SVM but propose a BP neural
model and an improved excitation function and add an
impulse term to make it possible to detect rumors in the
propagation process. Lu et al. [5] noticed that there is an
imbalance in the data, which has affected the implementation
of rumor detection and proposed a Co-Forest
algorithm to improve the imbalanced data and balance the
data distribution. Mao et al. [6] used an integrated classifier
to detect rumors based on characteristics such as emotional
orientation and communication process. In recent years, the
rise of artificial intelligence has made the application of deep
learning increasingly widespread. Similarly, in the field of
rumor detection, deep learning plays an important role.
Takahashi and Igata [7] developed a system to detect rumors
by studying the spread of rumors and conducted experiments
on Twitter and found that they can effectively detect
rumors, which opened up a new chapter in rumor detection.
Karamchandani and Franceschetti [8] proposed a method of
detecting the source of rumors to control the rumors, which
extended the best estimator of rules and irregularities to
achieve the purpose of detecting the source of rumors.
Similarly, Wu et al. [9] also considered propagation in the
rumor detection technology and proposed a hybrid SVM
classifier based on the graph kernel, which not only captures
semantic features such as topics and emotions but also
captures high. (e first-order propagation mode improves
the classification accuracy.
With the rise of artificial intelligence, rumor detection
has entered a new stage. (e method of rumor detection
based on feature learning mainly uses advanced artificial
intelligence ideas. Li et al. [10] combine the convolutional
layer of the convolutional neural network to extract text
features, use the GRU network to process the features, and
then judge whether it is a rumor. Ren et al. [11] considered
thatWeibo text is a graph structure, and information such as
the attitude of users’ comments will affect the spread of
Weibo text and proposed a rumor detection model based on
time series. Liao et al. [12] considered the potential information
of some Weibo texts and partial user information
and proposed a social media rumor detection method based
on a hierarchical attention network. Srinivasan and Dhinesh
Babu [13] proposed a double convolutional neural network
method with a new activation function for the sparse data
with little available information that can be used to distinguish
rumors at the beginning, because this method has
faster generalization speed and more high precision and has
a very good effect in rumor detection. After summarizing
many studies, Zhou et al. [14] found that their research
seldom considered the timeliness of rumor detection and
proposed an early rumor detection model to process rumors
through two modules, a rumor detection module and a
checkpoint module. (e rumor detection module is used to
extract features, and the checkpoint module is used to solve
the problem of timeliness, which is used to trigger the rumor
detection module to ensure the timeliness of rumor detection
while ensuring accurate identification of rumors. Lin
et al. [15] raised the issue of word independence and found
that some common words appear in rumors. Once these
words appear, they can be judged as rumors. (ey proposed
a deep sequence model to consider the two aspects of rumors:
falsehood sex and influence, using long- and short-
term memory units to learn falsehood, and combining deep
sequences and social characteristics to learn influence.
Asghar et al. [16] proposed a bidirectional long-term shortterm
memory model based on convolutional neural net-
work, which uses convolutional neural network to extract
post features and uses bidirectional long-term short-term
memory method to store points and consider contextual
information, effectively detecting rumors on Weibo, focusing
on the research of rumor detection in Arabic,
extracting information from users and content, and proposing
the use of semisupervised expectation maximization
(EM) to train newsworthy tweets topics to achieve the
purpose of rumor detection [17].
In recent years, the research on rumor detection has
mainly focused on extracting features and analyzing features.
According to the text content of the given data, the
main features expressed by the data are extracted and
analyzed. Since the data comes from Twitter accounts,
there are some spam accounts that guide public opinion,
which interferes with the process of rumor detection and
affects the results of the detection. At the same time, the
longer the rumors spread, the more harmful it will be to the
society, and timely detection of rumors is an important
aspect of rumors detection research. Aiming at two aspects,
this paper starts from the two directions of preprocessing
and timeliness. On the one hand, the accuracy of rumor
detection is improved through account screening and tweet
standardization. On the other hand, reinforcement
learning algorithms are used to save the time of rumor
detection as much as possible.
3. DDR Model Architecture
(e model mainly consists of three submodels: a data
preprocessing model based on account filtering and standardization
(DP model), a rumor detection model based on
deep learning (DL model), and a checkpoint model based on
reinforcement learning (RL model). (erefore, the proposed
model is called DDR (data preprocessing and rumor detection
based on deep learning and reinforcement learning)
model for short as shown in Figure 2. In the DP model, this
paper proposes basic information of users to analyze users,
filter spam accounts in Twitter data, refine the data, and use
standardized Twitter text to enhance the data to achieve
accurate detection of rumors purpose. In the DLmodel, data
characteristics are mainly analyzed, and BiGRU is used to
consider the context, analyzes Twitter text data, and detects
rumors. (e RL model mainly solves the problem of the
timeliness of rumors detection. (e Q-learning algorithm is
used to judge the detection results, and the reward and
punishment mechanism is set to make the model trade-off
between timeliness and accuracy to improve the timeliness
of detection. At the same time, the accuracy of detection is
improved.
Discrete Dynamics in Nature and Society 3
#
#3.1. DP Model
3.1.1. Account Filtering. For a Twitter account, its main
features are generally divided into features based on user
portraits and features based on tweet. User portrait features
are a series of features that can be directly extracted from
user information, such as the number of people that users
follow and number of fans. User tweet features are statistical
features extracted from user tweets, such as the proportion
of tweets containing URLs and the proportion of liked tweets
to the total tweets. Since the tweets of a single Twitter
account in the dataset used for rumor detection usually is not
larger enough to extract the user-based tweet text features, it
is impossible to obtain complete user tweet features. At the
same time, if a tweet of a single Twitter account has a high
number of likes, the tweet can be considered valuable,
thereby reducing the probability of the account being
filtered.
Based on the above description, it can be concluded that
when a certain type of Twitter account meets certain conditions,
the tweets belonging to this account are usually
weakly relevant or invalid for rumor detection. In this paper,
RL
Model
ai pi
ei
hb0 hb1 hbN
hfN
mN
eN
softmax
Output
InputNInput1Input0
Specific data format
Original data dictionary
Original
JSON file
Sydneysiege
Ottawashooting
Germanwingscrash
Ferguson
Rumor
non-rumor
Charliehebdo
DP
Model
BiGRU State
Data loading
&
Account filtering
Data format
processing
Text processing
Words
Embedding
Layet
BiGRU Layer
Max-pooling
& Dropout
Layer
DL
Model
hf0 hf1
m0 m1
e0 e1
rewardi
New Length Seqence
Figure 2: Architecture of DDR (data preprocessing and rumor detection based on deep learning and reinforcement learning).
4 Discrete Dynamics in Nature and Society
#
#this type of account is defined as “filtered account,” or FA,
and using the user’s portrait features and the number of likes
on user tweets to determine whether an account is FA. (e
definition of FA is as follows:
(1) An FA will follow many people, while few users will
follow the FA. Based on this feature, the authenticity
definition of user accounts is proposed, as shown in
the following:
authenticity �
FollowersCount
FollowersCount + FriendsCount
, (1)
where FollowersCount denotes user followers and
FriendsCount refers to the number of users
following.
(2) Generally, the personal information of spam accounts
in Twitter is not complete enough, and it is
rare to fill in user description and user location
information. (erefore, HasDesc and HasLoc
are defined to indicate whether the user has description
information and location informa-
tion. (us, HasDesc or HasLoc is 1; otherwise, it is
0. Based on the above analysis, the definition
of user authority is proposed, as shown in the
following:
authority � authenticity + 0.5 · (HasDesc + HasLoc)
+
TweetsLike
AvgLike
,
(2)
where TweetsLike denotes the total number of likes
corresponding to the current user’s tweet and
AvgLike refers to the average of the total number of
likes of all users.
(3) (e authority of all users is sorted in a nonincreasing
manner. It is concluded that the bottom 5% of users
in the ranking have lower authority, and these users
are defined as FA.
We filter out the FAs in the Twitter standard dataset and
filter the tweets belonging to these FAs.
3.1.2. Tweet Standardization. (e standardization of text is a
part of text preprocessing, which mainly refers to the correction
of some irregularities or errors in the text, thus
transferring it to a text that people can understand correctly.
Based on the characteristics of tweets in the Twitter standard
dataset, a standardization method for tweet text is proposed
in this section.
Tweets in Twitter are usually random and short. On the
one hand, tweets are generally limited to 140 words. On the
other hand, compared with traditional standard texts,
tweets contain many irregularities or errors in terms of
wording, grammar, format, and so on, such as spoken
language, colloquialisms, acronyms, Internet terms, or
emoji expressions, which greatly increase the difficulty of
computer understanding of the text, disturbing factors in
the difficulty of understanding the text. At the same time,
the tweets also contain some symbols and network links
that have no actual meaning, and other factors that have no
relation with the semantics of the text.
In view of the above characteristics of tweets, in order to
strengthen the computer’s understanding of tweets, this
paper carries out the following standardized processing on
tweets:
(1) Unit replacement is as follows: replacing the unit in
the text with a unified format, such as replacing
“4 kgs” and “4 kg” with “4 kg”
(2) Acronym replacement is as follows: replacing the
acronyms in the text with complete words, such as
replacing “can’t” with “can not”
(3) Spelling proofreading is as follows: replacing some
network terms or punctuation of words with irregular
spelling, such as replacing “rep” with “reply”
(4) Punctuation is as follows: adding spaces on both
sides of all punctuation
(5) Symbol replacement is as follows: replacing all
logical symbols with words, such as “and” with the
word “and”
(6) Redundant information processing is as follows:
removing extra spaces, “@” and “#” symbols in
hashtags and removing all hyperlink information
(7) Delete stopwords is as follows: deleting a series of
stop words such as “if” and “to”
(8) Part-of-speech restoration is as follows: restoring an
English word of any form to its general form, such as
“does,” “did,” and “done” unified reduction to “do”
3.2. DL Model. Rumor detection model based on deep
learning processes the tweets after tweet standardization,
dividing into words embedding layer, max-pooling and
dropout layer, and BiGRU layer. It is used to transform a
piece of text into the final state and to judge whether the text
is a rumor through the softmax function.
3.2.1. Words Embedding Layer. In the words embedding
layer, this paper first performs word segmentation on the
text Inputi that has been standardized. Considering that
simple splitting will destroy the semantics of compound
words such as “eleven-years-old,” this paper uses a word
segmentation method based on phrase dictionary matching
for text segmentation.
After word segmentation, we map the words to word
vectors wn
i according to word frequency.(is paper sets Ei �
Input1, Input2 , . . . , Inputn􏼈 􏼉 to indicate that there are n
tweets at a time, where Inputi � w1
i , w2
i , . . . , wn
i􏼈 􏼉 means that
the tweet has word vectors, and these word vectors are
combined together and obtain the vector matrix ei of the
tweet formed after Inputi is processed by the words embedding
layer.
Discrete Dynamics in Nature and Society 5
#
#3.2.2. Max-Pooling and Dropout Layer. In order to get the
most prominent features of posts, the maximum pooling
method is used for pooling, so that keywords or sentence
features are reduced, and parameters are reduced. Finally, a
fixed-size vector mi can be generated. At the same time, in
order to slow down overfitting and enhance the model
generalization ability, the dropout layer is added.
3.2.3. BiGRU Layer. In order to strengthen the model’s
understanding of contextual semantics, this paper uses
BiGRU to simultaneously combine the before and after
sequences to make predictions. BiGRU is composed of two
GRU stacked on top of each other, and its main structure is a
combination of two unidirectional GRU. For each time t, the
input will be provided to the two GRU in opposite directions
at the same time, and the output will be jointly determined
by the two unidirectional GRU.
As shown in Figure 3, xt is the input data, ht is the output
of the GRU unit, zt is the update gate, zt and rt jointly
control the calculation from the hidden state of ht−1 to the
hidden state of ht, and the update gate also controls the
current input data and previously memorized information
ht−1, output a value zt between 0 and 1, and zt determines
how much ht−1 is transferred to the next state; the specific
unit is calculated as the following formulae show:
zt � σ Wz · ht−1, xt􏼂 􏼃( 􏼁, (3)
rt � σ Wr · ht−1, xt􏼂 􏼃( 􏼁, (4)
􏽥ht � tanh W · rt × ht−1, xt􏼂 􏼃( 􏼁 , (5)
ht � 1 − zt( 􏼁 × ht−1 + zt × 􏽥ht, (6)
where σ is the Sigmoid function and Wz, Wr, W are the
weight matrix of update gate, reset gate, and candidate
hidden state, respectively. (e reset gate controls the importance
of ht−1 to the result ht. When the previous memory
ht−1 is completely related to the new memory, the reset gate
can be used to increase the impact of the previous memory.
According to the calculation results of reset gate, update
gate, and hidden state, the output ht at the current moment
can be obtained by formula (6), thereby obtaining the relationship
between BiGRU and a large number of posts.
(en, we use the final state hN (N � the number of posts
received so far) to judge the rumors through the softmax
function:
p � softmax ωT
· tanh hN( 􏼁􏼐 􏼑. (7)
3.3. RLModel. In addition to the accuracy of detection, this
paper also considers the timeliness of detection. (is paper
uses the Q-learning algorithm to dynamically determine the
best checkpoint to improve the timeliness of rumor detection.(e
Q-learning algorithm has a calculation action value
and a reward mechanism. (e action value function calculates
the Q value according to the obtained state
expression. As shown in formula (8), combining the Q value
and the post state expression, the action value function is
used to determine whether to terminate or continue, and as
shown in formula (9), the characteristic representations hfi
and hbi are used as inputs to calculate the action value:
Qi+1(s, a) � E r + cmaxQi s′, a′( 􏼁 | s, a􏼂 􏼃, (8)
ai � Wa ReLu Wh hfi + hbi􏼐 􏼑 + bh􏼐 􏼑􏼐 􏼑 + ba, (9)
where c is the discount rate, r is the reward value, a0 is
rumor, and a1 is nonrumor.
According to the action value and state value, we get the
max reward value, used to optimize the action value.
(e main features of the posts are maintained in the
words embedding layer unchanged. (rough the BiGRU
layer, the new state value is obtained by combining the main
features of the current input post and the previously obtained
state value as the input of reinforcement learning.
(en, the action value obtained by reinforcement learning
decides whether to terminate or continue. If it terminates
and the prediction is correct, the model will give a reward. If
it terminates but the prediction is wrong, this model will give
a big penalty. If it continues, this model will also give a small
penalty.(e calculation formula is shown in formula (10). In
this way, the model will make a consideration between
whether to continue or terminate in the end and, at the same
time, make a trade-off between accuracy and timeliness:
log N terminate with correct prediction
−P terminate with incorrect prediction
−ε continue
,
⎧⎪
⎨
⎪
⎩
(10)
where N is the number of correct predictions accumulated
thus far, P is a large value to penalize an incorrect prediction,
and ε is a small penalty value for delaying the detection.
4. Experiments and Analyses
4.1. Dataset. (is paper uses the public standard Twitter
dataset proposed by Ma et al. [18]. (is dataset was proposed
in 2016 and has been recognized by the academic community.
It has since been widely used in the field of rumor detection
and is a classic dataset on the problem of rumor detection. It
contains 5802 events, each of which contains several Twitterrelated
JSON files. Each JSON file represents a tweet, including
the information of the tweet and the basic information of the
user who posted the tweet.(e specific data information of the
dataset is shown in Table 1. (is paper uses part of the information
to perform account filtering and then standardizes
tweets, uses the creation time of the tweets as the test basis for
early detection, encapsulates the data in a certain format, and
uses the ratio of 8 : 2 to divide the preprocessed dataset into a
training set and a test set for the application of the subsequent
sequence in the training and testing of the model.
4.2. Evaluation Indicators. For the evaluation criteria of the
model, this paper uses four indicators consistent with the
literature [19, 20], namely, Accuracy, Precision, Recall, and
F1. (e calculation formula is shown as follows:
6 Discrete Dynamics in Nature and Society
#
#accuracy �
TP + TN
TP + FN + FP + TN
, (11)
precision �
TP
TP + FP
, (12)
recall �
TP
TP + FN
, (13)
F1 �
2 × precision × recall
precision + recall
, (14)
where TP, FN, FP,TN are shown in Table 2 [21].
4.3. Environment and Hyperparameters. In this paper, the
model is implemented under the Linux system, and the
model is trained under the GPU environment of Python 3.6
and TensorFlow 1.13.1. In the training process, this paper
uses the per-trained GloVe as the initialization of the word
vector to input, which contains 840 billion words. Also, the
dimensionality of word embedding is set to 300, while the
dropout rate to embedding layer is 0.5. (e Adam optimiser
(Kingma and Ba) [22] with a learning rate of 0.01 for DL
model and 0.001 for RL model are used as the optimization
method. Set the size of each batch to 50 and the number of
DL model and RL model alternate training rounds to 20.
4.4. Results
4.4.1. Training Loss and Reward. (e training loss and reward
values over iterations are presented in Figures 4 and 5.
In this paper, the DL model and RL model are trained in an
alternating manner. It can be seen that the training loss of
the DL model tends to be dynamically balanced after about
5000 iterations, and the loss value drops below 0.2, achieving
the optimal value. In addition, the reward curve fluctuates
more as the reward was calculated based on the accuracy of
DL model. When switching between training DL and RL
model, the reward value tends to change abruptly. But with
the improvement of accuracy over time, a consistent improvement
of reward value can be seen.
4.4.2. Detection Model and Comparison. In order to effectively
testify the accuracy and effectiveness of the model, this
paper compares the proposed model with the following
models. (e RNN model mainly uses recurrent neural
networks to analyze data and detect whether a text is a
rumor.(e LSTMmodel is an improvement of the recurrent
neural network model, which considers the contextual relationship.
(e GRU-2 model [18] is an improvement of the
LSTM model, which reduces the parameters and improves
the efficiency. Specifically, this model first divides the event
into time periods, then uses the tf-idf method to calculate the
text representation of each time period, uses a two-layer
GRU network to learn the hidden layer representation of
each event, and finally realizes the classification of the event.
(e HMM model [23] uses the group’s point of view for
analysis and finally achieves the classification effect. (e
GAN-GRUmodel [24] uses a generative adversarial network
1tanh
ht-1
xt
ht
rt
σ σ
zt ht
~
Output
GRU
Input
Figure 3: BiGRU (left) and GRU update process (right).
Table 1: Twitter standard dataset information.
Statistics Twitter
User# 49,345
Posts# 103,212
Events# 5,802
Rumors# 1,972
Nonrumors 3,830
Avg. hours per event 33.4
Avg. # of posts per event 17
Max # of posts per event 346
Min # of posts per event 1
Table 2: Confusion matrix.
Actual
1 0
Predicted 1 True positive (TP) False positive (FP)
0 False negative (FN) True negative (TN)
Discrete Dynamics in Nature and Society 7
#
#to train the discriminator for rumor detection.(e generator
is mainly designed to produce uncertain or conflicting
sounds, which complicates the original dialogue thread to
make the different supercharged to learn stronger indicative
representations of rumors from the enhanced and more
challenging examples. (e RDM model is a part of the ERD
model. It mainly uses a single-layer GRU network to extract
main information and uses reinforcement learning to
conduct rumor detection in time.
In this section, this paper evaluates DDR based on four
evaluation indicators on the test set, as shown in Table 3.(is
paper is mainly based on the improvement of the ERD
model proposed in 2019 and proposes a preprocessing
module based on account filtering and standardization. By
processing the input of themodel, the data is refined, and the
accuracy of rumor detection is improved. It can be seen from
the chart that the indicators of DDR are basically better than
traditional models, such as RNN model, LSTM model,
0.7
0.6
0.5
0.4
0.3
0.2
0 2000 4000 6000
Iterations
8000 10000
Lo
ss
Figure 4: Changes in loss.
0.1
0.0
–0.1Re
w
ar
ds
–0.2
–0.3
0 25 50 75 100
Iterations (k)
150 175125
rewards
polyfit
Figure 5: Reward during training.
8 Discrete Dynamics in Nature and Society
#
#HMM model, and GAN-GRU model. At the same time, the
model in this paper is compared with the ERD model that
also considers the timeliness of rumor detection. (e ERD
model has an accuracy of 0.858 in the accuracy of detection.
After improving on the basis of the ERD, the accuracy of the
model has reached 0.863, an increase of 0.005.
Comparing the three indicators of Precision, Recall, and
F1, the model proposed in this paper is 0.027 lower than the
ERD model on the Precision indicator, and the other two
indicators have increased by 0.123 and 0.051, respectively. It
can be seen that the model proposed in this paper has a good
effect in the process of rumor detection. However, when
reinforcement learning is added, reinforcement learning
must consider the timeliness of detection, and you will want
to get results faster when making trade-offs. (erefore, reinforcement
learning has an impact on the accuracy of the
model, compared to not considering timeliness. For the
RDM model, the accuracy of the model proposed in this
paper is slightly lower than 0.01.
In summary, the model proposed in this paper improves
the accuracy of rumor detection while considering the
timeliness of model detection. It can be seen from the graph
that the proposed model is effective in detecting rumors.
4.4.3. Detection Timeliness. (en, in order to evaluate the
timeliness of detection, based on the Twitter standard
dataset, this paper focuses on comparing the DDR model
with the GRU-2 model. (e biggest difference between the
two models is that GRU-2 uses fixed checkpoints. (e DDR
dynamically determines checkpoints through the DL model.
(is paper presents the proportion of events that are
classified by DDR and the classification accuracy over time
(6-hour interval) in Figure 6. Firstly, it can be seen that 70%
of rumors are discovered within 6 hours, and the best
checkpoint of GRU-2 (vertical dashed line) is 12 hours, so
DDR can detect most rumors earlier than GRU-2. Secondly,
it can be clearly seen that the classification accuracy of DDR
is better than GRU-2 (horizontal dotted line) at all
checkpoints.
In summary, it can be seen that DDR improves the
timeliness of rumor detection compared with the GRU-2
model.
5. Conclusions
(is paper presents a new early rumor detection model. (e
model is divided into three submodels: DP model, DL
model, and RL model. In rumor detection, according to the
early rumor detectionmodel, this paper proposes to filter the
account according to the user’s portrait characteristics and
the user tweet’s praise number. (e text standardization for
tweets is defined. (e original data is processed more precisely
and more precisely to improve the accuracy of the
detection. At the same time, BiGRU is proposed to enhance
Table 3: Detection accuracy on Twitter.
Method Accuracy Precision Recall F1
Baseline 0.612 0.355 0.465 0.398
RNN 0.785 0.707 0.659 0.682
LSTM 0.796 0.719 0.683 0.701
GRU-2 0.808 0.741 0.694 0.717
HMM — — — 0.524
GAN-GRU 0.781 0.773 0.796 0.784
RDM 0.873 0.817 0.823 0.820
ERD 0.858 0.843 0.735 0.785
DDR (ours) 0.863 0.816 0.858 0.836
100
80
Pe
rc
en
t (
%
) 60
40
20
0-6 6-12 12-18 18-24 24-30 30-36 36-42
Checkpoints (Hours)
42-48 48-54
0
1.0
0.8
Ac
cu
ra
cy
0.6
0.4
0.2
0.0
Percent
Accuracy
Figure 6: Proportion of Twitter events classified and detection accuracy by DDR over time. Vertical dashed line indicates the optimal
checkpoint for GRU-2 and horizontal dotted line indicates GRU-2’s accuracies.
Discrete Dynamics in Nature and Society 9
#
#the understanding of context semantics, fully consider the
relationship between the post text, and improve the effect of
model training. (e model is trained by the method of
intensive learning to ensure the timeliness of rumor detection.
(e results are compared with the early rumor
detection model. (e results show that the accuracy of DDR
model is 86.3% in the test set, 0.5% higher than ERD model,
and 5.5% higher than that of GRU-2 model. It is proved that
the model proposed in this paper has achieved good results
in rumor detection. 70% of them are found in 6 hours, and
the detection of GRU-2model takes 12 hours.(erefore, this
paper improves the accuracy of rumor discovery on the
premise of ensuring the timeliness of rumor discovery. With
the development of science and technology, there is an
increasing trend about the investigation of the real world
problems such as the development of Faults Detection [25],
Vortex (eory [26], Periodic Orbit [27], Dynamics [28] and
other research fields. It is hoped that the model proposed in
this paper can be applied to these problems in the near
future.
Data Availability
No data were used to support this study.
Conflicts of Interest
(e authors declare that they have no conflicts of interest.
Acknowledgments
(is work was supported by the Natural Science Foundation
of Tianjin under Grant no. 19JCYBJC15400 and the Program
for the Science and Technology Plans of Tianjin, China,
under Grant 20YDTPJC01200.
References
[1] J. Ma, W. Gao, Z. Wei et al., “Detect rumors using time series
of social context information on microblogging websites,” in
Proceedings of the 24th ACM International on Conference on
Information and Knowledge Management, vol. 3, pp. 1751–
1754, Melbourne Australia, October 2015.
[2] S. Kwon, M. Cha, and K. Jung, “Rumor detection over varying
time windows,” PLoS One, vol. 12, no. 1, pp. e0168344–19,
2017.
[3] M. Farajtabar, J. Yang, X. Ye et al., “Fake news mitigation via
point process based intervention,” in Proceedings of the 34th
International Conference on Machine Learning, pp. 1097–
1106, Sydney, Australia, August 2017.
[4] C. Liang, Y. Qiu, and S. Lu, “Research on microblog rumor
detection method,” Computer applications and software,
vol. 30, no. 2, pp. 226–228, 2013.
[5] T. Lu, B. Shi, Z. Yan et al., “A semi supervised learning algorithm
for microblog rumor detection,” Computer appli-
cation research, vol. 33, no. 3, pp. 744–748, 2016.
[6] E. Mao, G. Chen, X. Liu et al., “Research on microblog rumor
detection based on deep feature and ensemble classifier,”
Computer application research, vol. 33, no. 11, pp. 3369–3373,
2016.
[7] T. Takahashi and N. Igata, “Rumor detection on twitter,” in
Proceedings of the 6th International Conference On Soft
Computing and Intelligent Systems, pp. 452–457, Kobe, Japan,
November 2012.
[8] N. Karamchandani and M. Franceschetti, “Rumor source
detection under probabilistic sampling,” in Proceedings of the
IEEE International Symposium On Information :eory Proceedings
(ISIT), Istanbul, Turkey, July 2013.
[9] K. Wu, S. Yang, and K. Q. Zhu, “False rumors detection on
sina Weibo by propagation structures,” in Proceedings of the
IEEE 31st International Conference On Data Engineering
(ICDE), pp. 651–662, Seoul, South Korea, April 2015.
[10] L. Li, G. Cai, and J. pan, “Microblog rumor detection method
based on C-GRU,” Journal of Shandong University, vol. 49,
no. 2, pp. 102–106, 2019.
[11] W. Ren, B. Qin, and T. Liu, “Research on rumor detection
based on time series network,” Intelligent computer and application,
vol. 9, no. 3, pp. 300–303, 2019.
[12] X. Liao, Z. Huang, D. Yang et al., “Social media rumor detection
based on hierarchical attention network,” Chinese
Science: Information Science, vol. 48, no. 11, pp. 1558–1574,
2018.
[13] S. Srinivasan and L. D. Dhinesh Babu, “A parallel neural
network approach for faster rumor identification in online
social networks,” International Journal on Semantic Web and
Information Systems, vol. 15, no. 4, pp. 69–89, 2019.
[14] K. Zhou, S. Chang, B. Li et al., “Early rumour detection,”
Association for Computational Linguistics, vol. 1, pp. 1614–
1623, 2019.
[15] D. Z. Lin, B. Ma, D. L. Cao, and J. H. Lau, “Chinese microblog
rumor detection based on deep sequence context,” Concurrency
and Computation: Practice and Experience, vol. 31,
no. 23, 2019.
[16] M. Z. Asghar, A. Habib, A. Habib et al., “Exploring deep
neural networks for rumor detection,” Journal of Ambient
Intelligence and Humanized Computing, vol. 12, 2019.
[17] S. M. Alzanin and A. M. Azmi, “Rumor detection in arabic
tweets using semi-supervised and unsupervised expectationmaximization,”
Knowledge-Based Systems, vol. 185, 2019.
[18] J. Ma, W. Gao, P. Mitra et al., “Detecting rumors from
microblogs with recurrent neural networks,” in Proceedings of
the 25th International Joint Conference on Artificial Intelligence,
pp. 3818–3824, New York, NY, USA, July 2016.
[19] F. Yu, Q. Liu, S. Wu et al., “A convolutional approach for
misinformation identification,” in Proceedings of the 26th
International Joint Conference on Artificial Intelligence,
pp. 3901–3907, Melbourne, Australia, August 2017.
[20] N. Ruchansky, S. Seo, and Y. Liu, “CSI: a hybrid deep model
for fake news detection,” in Proceedings of the ACM on
Conference on Information and Knowledge Management,
pp. 797–806, ACM, Singapore, Asia, November 2017.
[21] M. Yuan and H. Tang, “Research on rumor identification
based on graph convolution network,” Computer Engineering
and Application, vol. 57, no. 13, 2020.
[22] D. P. Kingma and J. Ba, “Adam: a method for stochastic
optimization,” in Proceedings of the International Conference
on Learning Representations (ICLR), San Diego, CA, USA,
May 2015.
[23] S. Dungs, A. Aker, N. Fuhr et al., “Can rumour stance alone
predict veracity,” in Proceedings of the 27th Inter-national
Conference on Computational Linguistics, pp. 3360–3370,
Santa Fe, NM, USA, August 2018.
[24] J. Ma, W. Gao, and K. F. Wong, “Detect rumors on twitter by
promoting information campaigns with generative adversarial
learning,” in Proceedings of the World Wide Web Con-
ference, pp. 3049–3055, San Francisco, CA, USA, May 2019.
10 Discrete Dynamics in Nature and Society
#
#[25] M. Miguel, J. A. Antonino-Daviu, F. D. C. Castellá Pedro, and
C. Alberto, “Higher-order spectral analysis of stray flux signals
for faults detection in induction motors,” Applied
Mathematics and Nonlinear Sciences, vol. 5, no. 2, pp. 1–14,
2020.
[26] M. Sharifi and B. Raesi, “Vortex theory for two dimensional
Boussinesq equations,” Applied Mathematics and Nonlinear
Sciences, vol. 5, no. 2, pp. 67–84, 2020.
[27] A. A. Abozaid, H. H. Selim, K. A. K. Gadallah, I. A. Hassan,
and E. I. Abouelmagd, “Periodic orbit in the frame work of
restricted three bodies under the asteroids belt effect,” Applied
Mathematics and Nonlinear Sciences, vol. 5, no. 2, pp. 157–
176, 2020.
[28] S. S. Hassan, M. P. Reddy, and R. K. Rout, “Dynamics of the
modified n-degree Lorenz system,” Applied Mathematics and
Nonlinear Sciences, vol. 4, no. 2, pp. 315–330, 2019.
Discrete Dynamics in Nature and Society 11
Received: 9 December 2021 - Revised: 1 June 2022 - Accepted: 6 June 2022 - IET Information Security
DOI: 10.1049/ise2.12073
REV I EW
The COVID‐19 scamdemic: A survey of phishing attacks and
their countermeasures during COVID‐19
Ali F. Al‐Qahtani1 | Stefano Cresci2
1College of Science and Engineering, Hamad Bin
Khalifa University (HBKU), Doha, Qatar
2Institute of Informatics and Telematics (IIT),
National Research Council (CNR), Pisa, Italy
Correspondence
Stefano Cresci, Institute of Informatics and
Telematics (IIT), National Research Council (CNR),
via G. Moruzzi 1, Pisa 56124, Italy.
Email: stefano.cresci@iit.cnr.it
Funding information
Qatar National Library, Grant/Award Number:
QRLP10‐G‐1803022; Qatar National Research Fund
Abstract
The COVID‐19 pandemic coincided with an equally‐threatening scamdemic: a global
epidemic of scams and frauds. The unprecedented cybersecurity concerns emerged
during the pandemic sparked a torrent of research to investigate cyber‐attacks and to
propose solutions and countermeasures. Within the scamdemic, phishing was by far the
most frequent type of attack. This survey paper reviews, summarises, compares and
critically discusses 54 scientific studies and many reports by governmental bodies, security
firms and the grey literature that investigated phishing attacks during COVID‐19, or that
proposed countermeasures against them. Our analysis identifies the main characteristics
of the attacks and the main scientific trends for defending against them, thus highlighting
current scientific challenges and promising avenues for future research and
experimentation.
1 | INTRODUCTION
The COVID‐19 pandemic had a dramatic worldwide impact
on all sides of our lives, including the way business and social
interactions are conducted, and the overall organisation of our
work. Regarding the latter, lockdowns and the enforcement of
social distancing measures resulted in an unprecedented
number of people experiencing changes in their working
habits. Many employees had to adapt – oftentimes even
abruptly – to using digital platforms, messaging apps and novel
communication channels for their everyday activities [1]. In a
line, we witnessed a huge worldwide shift from office work to
remote (home) work.
This high‐level change implied several lower‐level
fundamental shifts in how work was conducted, especially
from a security perspective. In fact, while office work was
characterised by a mixture of physical and digital interactions
—the latter occurring in relatively secure and monitored
environments, remote working necessarily involved the use
of digital systems operated in largely insecure and unmanaged
environments. Moreover, a large number of people
were not used to remote working and did not receive any
specific training on how to work remotely in a secure way.
The inevitable consequence was the increase of cyber‐risks,
which eventually resulted in a massive escalation of cyber‐
attacks [2, 3]. An early report from the International Association
of IT Asset Managers (IAITAM) warned that
working from home during the COVID‐19 pandemic could
allow for plentiful data breaches.1 The warnings from the
IAITAM report were later confirmed when a large‐scale
survey involving more than 3000 employees across 12
countries found that 94% of them experienced data breaches
via cyber‐attacks during the course of the pandemic,
resulting in an average number of more than 2 breaches
suffered per employee [1].
In addition to the regular and well‐known security risks
of remote working, other peculiar risks arose as a consequence
of the chaos induced by the pandemic. As a para-
mount example, the widespread fear and uncertainty that
followed the diffusion of COVID‐19 resulted in a huge demand
for information (e.g., how to protect from, or cure, the
infection), which set the stage for the emergence of a
COVID‐19 infodemic [4, 5]. As part of such uncontrolled
flow of information, a surge in the registration of covid‐
related domains was observed. Several investigations
demonstrated that a large share of such new domains were
This is an open access article under the terms of the Creative Commons Attribution‐NonCommercial‐NoDerivs License, which permits use and distribution in any medium, provided the
original work is properly cited, the use is non‐commercial and no modifications or adaptations are made.
© 2022 The Authors. IET Information Security published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.
1
https://www.techrepublic.com/article/covid‐19‐lockdowns‐are‐causing‐a‐huge‐spike‐in‐data‐breaches
324 - IET Inf. Secur. 2022;16:324–345. wileyonlinelibrary.com/journal/ise2
#
#outright malicious or, at the very least, suspicious to serve as
threat vectors for the exploitation of cyber‐attacks [6]. The
most common type of attack related to the newly‐created
COVID‐19 websites was phishing.2
Many cyber‐attacks involve social engineering techniques
to boost their chances of success. To this regard, the
increased anxiety caused by the pandemic resulted in a
higher success rate for cyber‐attacks occurred during
COVID‐19 [7]. Coupled with the overall increase in cyber‐
attacks, this figure depicts a worrying scenario. Moreover,
for workers employed in critical business sectors – such as
healthcare professionals – the pandemic also meant exceptional
workloads, with a consequent increase in stress which
also affects the success rate of cyber‐attacks. Indeed, a
statistically significant positive correlation was measured in
[8] between workload and the probability of a healthcare
staff opening a phishing email. Finally, the steep increase in
demand for certain goods such as personal protective
equipment (e.g., masks and gloves) exposed health services
and even governments to a plethora of digital scams,
especially in the form of phishing attacks [9, 10]. Given this
picture, it comes with little surprise that critical national
infrastructures such as healthcare services and hospitals were
among the most frequent targets of cyber‐attacks during
COVID‐19 [1, 7].
1.1 | Scope and contributions
The COVID‐19 pandemic was accompanied by an equally‐
dangerous epidemic of frauds and manipulations, as noted
by the United Nations and the World Health Organization
(WHO) [11]. When referring to the manipulation of online
information, this digital epidemic was dubbed infodemic. In
addition to this, the pandemic also created the conditions
for the rise of a multitude of cyber‐attacks and cybersecurity
issues: the COVID‐19 scamdemic. Within the scamdemic,
phishing was by far the most frequent type of attack [7].
Phishing attacks that occurred during the pandemic also
featured unique characteristics aimed at exploiting the peculiarities
of COVID‐19 in order to increase their chances
of success. The combination of the large number of
phishing attacks, together with their new characteristics,
attracted scholarly attention and many dedicated studies.
This survey reviews, summarises, compares and critically
discusses 54 scientific studies and many reports by governmental
bodies and security firms that investigated phishing
attacks during COVID‐19, or that proposed solutions and
countermeasures against them. Our analysis identifies the
main characteristics of the attacks and the main scientific
trends for defending against them, thus highlighting current
scientific challenges and promising avenues for future
research and experimentation.
1.2 | Significance
The rise of cyber‐attacks – and particularly of phishing attacks
– occurred during COVID‐19, combined with the increased
vulnerabilities of critical systems and persons that underwent
extreme levels of stress, holds the potential to cause serious
real‐world consequences and motivates research on this
important topic.
1.3 | Organization
The remainder of our survey is organised as follows. In Section
2 we briefly discuss the recent meta‐analyses, surveys and
review articles that are mostly related to our present work. In
doing that, we position our survey with respect to the existing
ones. Then, we introduce the problem of phishing attacks
during COVID‐19. The results of our literature review are
presented in Sections 3 and 4, which respectively focus on
phishing attacks and countermeasures. Both sections are
structured according to a top‐down approach, where we first
present the overall synthesis and the main themes that emerged
from the surveyed studies, followed by the detailed discussion
of each analysed study. Next, in Section 5 we critically discuss
the main findings of our survey, also highlighting challenges
and promising directions of future research and experimentation.
Finally, in Section 6 we conclude our work by summa-
rising the results of our literature review.
2 | BACKGROUND AND
PRELIMINARIES
This section begins with a brief critical review of the existing
surveys that are mostly related to our present work. Our
analysis allows positioning this survey with respect to existing
ones, highlighting the novelty and contributions of focussing
on phishing attacks occurred during the pandemic. Subsequently,
we introduce the problem of phishing and we
highlight its importance within the broader landscape of
COVID‐19 cyber‐attacks.
2.1 | Differences with existing surveys
The unprecedented consequences brought about by
COVID‐19 resulted in a remarkable wave of research produced
to contrast the many covid‐induced issues. Among this
new wave of research are a number of studies that focussed on
cyber‐crime, cyber‐attacks and cybersecurity issues occurred
during the pandemic. Original research in this direction was
complemented by a few survey papers. Here, we briefly review
the existing surveys that investigated the relationship between
cyber‐attacks and COVID‐19, highlighting their differences
with respect to our present survey. Table 1 provides an overview
of the existing surveys that are mostly related to our work.
In the following we briefly describe each of these works.
2
https://www.zdnet.com/article/thousands‐of‐covid‐19‐scam‐and‐malware‐sites‐are‐
being‐created‐on‐a‐daily‐basis/
AL‐QAHTANI AND CRESCI - 325
#
#The analysis presented in [7] describes COVID‐19 from a
cyber‐crime perspective and highlights the range of cyber‐
attacks experienced globally during the pandemic. Cyber‐
attacks were analysed and considered within the context of
key global events to reveal the modus‐operandi of cyber‐attack
campaigns. Results of the systematic and longitudinal analysis
revealed that cyber‐criminals leveraged salient events and
governmental announcements to carefully craft and execute
more effective cyber‐crime campaigns. This work represents a
nice introductory study on cyber‐attacks and COVID‐19,
without however going into the details of neither attacks nor
countermeasures. The survey in [1] presents the results of a
systematic multivocal (i.e., grey and scientific) literature review
of social engineering‐based cyber‐attacks during the COVID‐
19 pandemic. The survey covers 52 studies that investigated
attacks such as phishing, scamming, spamming, smishing, and
vishing, perpetrated via fake emails, websites, mobile apps,
trojans, bots, and ransomware. This survey only discusses the
high‐level characteristics of the cyber‐attacks, without
providing a technical analysis of the techniques proposed for
defending against them. The survey is also heavily focussed on
grey literature and only to a lower extent on scientific literature.
The study presented in [15] briefly reviews some of the malicious
cyber activities associated with COVID‐19 and the po-
tential mitigation solutions. Being published in 2020, the
analysis only covers attacks occurred within the first months of
the pandemic. Among the surveyed types of cyber‐attacks are
denial of service, ransomware, spyware, phising and vishing,
and other digital frauds. There is no specific focus on phishing
nor a detailed analysis of the detection techniques. The meta‐
analysis presented in [10] identifies the key cybersecurity
challenges, the solutions, and the areas of improvement in the
health sector, with respect to the cyber‐attacks occurred during
the COVID‐19 pandemic. The review highlighted a recent
increase in cyberattacks (e.g., phishing campaigns and ransomware
attacks) that exploit new vulnerabilities in technology
and people. In turn, such vulnerabilities are due to changes in
habits, behaviours, and working conditions caused by the
COVID‐19 pandemic. This meta‐analysis is non‐technical and
not specific to phishing, but provides interesting insights into
the challenges faced in the healthcare sector.
In addition to the above studies that heavily focussed on
COVID‐19, there also exists a few recent surveys, mostly
technical ones, that investigated certain attacks and countermeasures
without however considering the context of the
pandemic. The recent study discussed in [3] critically reviews
AI‐based approaches for defending against phishing attacks.
This survey exclusively focuses on Web phishing attacks and
categorises defensive techniques as either: (i) URL‐based, (ii)
HTML‐based, or (iii) visual similarity‐based. Similarly to [3],
the survey in [16] analyzes machine learning‐based phishing
detection systems that classify Web pages. In particular, it
specifically focuses on the analysis of the main machine
learning features used in such systems and on their impact on
classifier's accuracy. The survey presented in [13] reviews
works based on natural language processing techniques for
detecting phishing emails, while the survey in [12] focuses on
applications of artificial intelligence to detect phishing attacks.
The previous overview of the existing surveys, literature
reviews and meta‐analyses reveals that the majority of studies
that considered the context of the pandemic, focussed on high‐
level, preliminary investigations rather than in‐depth, technical
analyses. In other words, such surveys mainly provided scoping
reviews instead of systematic, detailed reviews. On the contrary,
several technical surveys focussed on phishing attacks,
but without specific reference to the pandemic. In the present
survey we contribute to filling this gap by providing a detailed
analysis of phishing attacks occurred during COVID‐19, and
their countermeasures.
2.2 | Phishing attacks during COVID‐19
As sketched in Figure 1, phishing is a typology of cyber‐attacks,
heavily grounded in social engineering, where an attacker sends
a maliciously‐designed message with the goal of tricking the
TABLE 1 Overview of recent related
surveys and differences with this survey.
Related surveys are listed in reverse
chronological order
Relatedness
Survey Year Phishing COVID‐19 Analysis
Hijji & Alam [1] 2021 ◐ ⬤ High‐level/descriptive
Lallie et al. [7] 2021 ◐ ⬤ High‐level/descriptive
He et al. [10] 2021 ◐ ⬤ High‐level/descriptive
Valiyaveedu et al. [3] 2021 ⬤ ◯ In‐depth/technical
Basit et al. [12] 2021 ⬤ ◯ In‐depth/technical
Salloum et al. [13] 2021 ⬤ ◯ In‐depth/technical
Alkhalil et al. [14] 2021 ⬤ ◯ High‐level/descriptive
Hakak et al. [15] 2020 ◐ ⬤ High‐level/descriptive
Korkmaz et al. [16] 2020 ⬤ ◯ In‐depth/technical
This survey – ⬤ ⬤ In‐depth/technical
Note: ◯: unrelated; ◐: partially related; ⬤: related.
326 - AL‐QAHTANI AND CRESCI
#
#victim into performing a specific action. Oftentimes, the malicious
message points the victim to a system that is controlled
by the attacker, from where the victim unwittingly downloads
malicious software or simply discloses sensitive information
with the attacker. Both the initial message and the system used
to collect the victim's information are carefully crafted so as to
resemble those of legitimate, authoritative and trustworthy
entities (e.g., the WHO or National Health Service (NHS)).
Successful phishing attacks can be perpetrated by exploiting a
number of media, channels and technologies, including emails,
websites and mobile devices. Moreover, both the messages and
the systems used to mount phishing attacks can be personalised
so as to allow gathering potentially any kind of personal
and sensitive information. Because of these reasons, phishing
attacks are extremely common and widespread, and often
represent the first mandatory step in order to achieve complex
frauds and network infiltrations, including Advanced Persistent
Threats (APT) [17, 18]. Indeed, the first mandatory step in an
APT kill chain involves gaining access to the target network,
which can be achieved via phishing. After a successful phishing
attack, the next step typically involves deploying a payload,
such as a carefully crafted malware designed to be stealthy,
which persists in the network for long periods of time, exfiltrating
data or anyway helping fulfiling the APTobjective for as
long as it remains undetected. As practical examples occurred
during the COVID‐19 pandemic, phishing emails and text
messages (e.g., SMSs or WhatsApp messages) were used to lure
victims to fraudulent websites. The websites gathered personal
data which was used to commit financial fraud or, in other
cases, to instal malware (e.g., ransomware) which was then used
to commit extortion [7]. To this regard, phishing attacks
represent common entry points for a broad array of cyber‐
attack sequences [7, 12]. Phishing attacks can manifest in
several different ways, which led scholars to identify a few
noteworthy subcategories of attacks. Among these, smishing
refers to phishing attacks that exploit mobile phone text
messages (i.e., SMSs) to lure victims. Instead, vishing (i.e., voice
phishing) refers to the use of telephony, robocalls and voice
over IP to mount attacks. Finally, the term pharming is used
when attackers rely on compromising systems (e.g., user devices
or DNS servers) to redirect victims to malicious websites.
Based on the above, on the one hand promptly detecting
phishing attacks and reducing their efficacy represents a critical
step for defending against many cyber‐attacks. On the other
hand however, the multitude of subtypes, media and technologies
exploited in phishing attacks poses challenges to their
detection, since detection techniques must adapt and be
effective across a broad spectrum of possible scenarios. As
shown in Figure 1, current phishing attacks mainly leverage
two media: the Internet (and especially the Web), and telephony.
Within these media, a large number of different vectors
can be used to perpetrate the attack (e.g., to deliver the malicious
message). Among the attack vectors that are mostly used
are emails, instant messages and messaging apps, online social
networks (OSNs), websites, SMSs, robocalls and malware apps
for mobile devices.3 Also the goals of the attackers can be
diverse and multifold, with attacks aimed at stealing personal
credentials (e.g., usernames and passwords) for certain services,
data exfiltration, financial fraud, extortion, or at installing
malicious software such as ransomwares, trojans and key loggers.
The multitude of ways in which phishing attacks can be
mounted, demands the research and development of different
techniques. As such, existing phishing detection systems are
designed to leverage the combination of several different information
for uncovering attacks, including IP addresses; email
and SMS texts; websites text, URL, HTML code, images and
metadata; voice transcripts; mobile app permissions; and more.
The detailed literature analysis presented in the two following
sections highlights attacks, and recent progress for defending
against them, along these lines.
3 | ATTACKS
This section investigates phishing attacks occurred during the
COVID‐19 pandemic. We begin by discussing the main
peculiar characteristics of covid‐related phishing attacks in the
broader context of COVID‐19, and we conclude by presenting
a detailed literature review of the many studies that investigated
such attacks.
3.1 | Overview and synthesis
3.1.1 | The rise of phishing attacks
As anticipated, phishing represented by far the most frequent
type of cyber‐attack that occurred during the pandemic. Evidence
for this figure emerges from basically all studies and
F I GURE 1 Complexity and dimensions of phishing attacks. Attacks
can exploit several vectors, including websites, emails and Online Social
Networks (OSNs), as well as SMSs, robocalls and malwares. As such,
defensive techniques leverage a large set of different features to detect
possible attacks. Phishing attacks can be perpetrated for a wide array of
malicious goals, such as for stealing sensitive information and for financial
fraud. This diversity of goals and techniques poses challenges to the
detection of phishing attacks
3
https://us.norton.com/internetsecurity‐online‐scams‐coronavirus‐phishing‐scams.html
AL‐QAHTANI AND CRESCI - 327
#
#reports that investigated covid‐related cyber‐attacks. Examples
of this kind include measurements related to March 2020
indicating an increase in phishing attacks in the region of 600%
with respect to the previous month.4 To quantify the number
and scale of such attacks, Google reportedly blocked 18 million
phishing emails related to the virus, in April 2020 [19].
Results reported in scientific literature corroborate the
above findings. The analysis presented in [7] shows that
phishing – including its subcategories, such as smishing and
vishing – was involved in 86% of the attacks identified.
Moreover, in the context of UK specific cyber‐attacks, [7]
analysed 17 different attacks, all of which involved phishing at
some stage of the attack sequence. Similarly, [1] found that the
most frequent social engineering–based attacks were phishing,
scamming, spamming, smishing, and vishing. Pharming attacks
were much less common but did occur in 13% of cases [7].
Figure 2 shows the relative frequency of all types of cyber‐
attacks during the COVID‐19 pandemic, highlighting the
massive frequency of phishing attacks, while Figure 3 drills
down into the most frequent subcategories of phishing attacks.
Scholars motivate the widespread occurrence of phishing attacks
with their high cost‐efficiency: attacks are relatively low‐
cost and with reasonable success rate. To this end, analyses also
show that the relatively large likelihood of success for phishing
attacks during COVID‐19 depended on the strategy of
exploiting salient events, media and governmental announcements
to the advantage of the attackers [7]. Regarding the
platforms mostly used to perpetrate these attacks, emails
accounted for 25% of the attacks, followed by websites (20%)
and mobile apps (13%) [1].
3.1.2 | Vulnerability to phishing
The main vulnerabilities to covid‐related phishing attacks
derive from the changes induced by the COVID‐19 pandemic.
The scoping review presented in [10] identifies 5 main changes
that are responsible for increased vulnerability to phishing and
other cyber‐attacks, during COVID‐19. The first of such
changes is represented by the decreased mobility and by the
national border closures, which demanded increased reliance
on remote work [9, 15, 20]. The shift to remote work often
occurred abruptly, with little planning, and involved employees
with limited previous experience or training [8, 21–23]. These
conditions represented the second cause of increased vulnerability
to phishing attacks. A third change is related to the
necessary use of digital communication systems for personal
interactions. This exposed both workers and users of given
services to a variety of attacks [24].
The three previous causes of increased vulnerability affect
nearly every sector of our society. However, some sectors –
such as healthcare and governmental services – were affected
even more because of their peculiar conditions and critical role
in the pandemic. In these sectors, additional vulnerabilities
arose. In particular, the healthcare sector significantly lags
behind other industrial sectors in terms of cybersecurity and
digital literacy [25, 26]. This made attacks against these targets
more valuable and, consequently, more frequent. Finally, the
increased demand for certain goods – above all, personal
protective equipment – made healthcare and governmental
services increasingly exposed to scams [9]. Typical phishing
attacks of this kind involved luring emails purportedly selling
goods in high demand, with the goal of tricking victims into
disclosing sensitive information.
Other causes for the increased vulnerability to phishing
attacks during COVID‐19 are related to the extreme levels of
stress, anxiety and uncertainty experienced during the peaks of
the pandemic [7, 8]. While these conditions were experienced
by everyone in covid‐stricken countries, workers in the
healthcare and governmental sectors suffered them even more.
Finally, several studies highlighted that fraudsters systematically
created ad‐hoc phishing messages that echoed official announcements
by governmental organisations, in order to boost
F I GURE 2 Frequency of the different techniques used for cyber‐
attacks occurred during COVID‐19, over the total number of attacks. The
sum of the frequencies exceeds 100% since some attacks used multiple
techniques. Phishing includes all its subcategories: smishing, vishing and
spear‐phishing
F I GURE 3 Relative frequency of the prevalent subcategories of
phishing attacks occurred during COVID‐19
4
https://blog.barracuda.com/2020/03/26/threat‐spotlight‐coronavirus‐related‐
phishing/
328 - AL‐QAHTANI AND CRESCI
#
#their credibility and their chances of success [7, 27]. In many
cases, the delay between an official announcement and the
attack exploiting such announcement was remarkably short –
for example, in the region of a couple of days – which
contributed to lure more victims and to reduce their capacity to
detect the scam.
3.1.3 | Notable phishing attacks
The majority of notable phishing attacks occurred during
COVID‐19 revolved around impersonating government organisations,
the WHO, the US Centre for disease Control and
Prevention (CDC), the UK NHS, airlines, supermarkets and
communication platforms [7]. Table 2 reports a list of some of
the noteworthy attacks detected and documented in both scientific
and grey literature, also describing their main charac-
teristics, including the target, vector (e.g., website, email, SMS),
goal and date of each phishing attack.
Among the attacks that have been thoroughly studied, is one
where attackers impersonated the WHO. The attack vector was
a WHO‐branded email containing useful and legitimate guidance
on how to protect from, and curb the spread of, the
COVID‐19 infection. Notably, the text of the email contained
some grammatical errors and misspellings, and also made use of
propaganda techniques [28] appealing to the reader's emotions
TABLE 2 Noteworthy phishing attacks detected and described in literature in the first months of the pandemic. Attacks are listed in reverse chronological
order, whenever the date of the attack is available
Reference Country Target Goal Vector Date
Xia et al. [34] USA, Netherlands Citizens Credential theft Website 17/04/2020
Xia et al. [34] Malaysia ATB, bell, Canadian Government Malware, espionage Website 14/04/2020
O’Donell [35] World Citizens Credential theft Email 31/03/2020
Rodger [36] UK Citizens Credential theft SMS 24/03/2020
Lallie et al. [7] USA Citizens Malware SMS 24/03/2020
Lallie et al. [7] World Citizens Extortion Email 20/03/2020
Pilkey [37] Spain Citizens Malware Email 10/03/2020
Pilkey [37] USA Citizens Malware Email 08/03/2020
Pilkey [37] Italy Citizens Malware Email 02/03/2020
Lallie et al. [7] China Citizens Ransomware Email 09/02/2020
Patranobis [38] India Chinese medical institutes Credential theft Email 06/02/2020
Pilkey [37] Vietnam Citizens Malware Email 03/02/2020
Lallie et al. [7] China Citizens Credential theft Email 02/02/2020
Vergelis [39] USA Citizens Credential theft Email 31/01/2020
Lallie et al. [7] China Citizens Malware Email 29/01/2020
Walter [40] Japan Citizens Malware Email 28/01/2020
Pilkey [37] Phillipines Citizens Malware Email 23/01/2020
Doffman [41] China Mongolian Ministry of foreign Affairs Malware Email 20/01/2020
Henderson et al. [42] Vietnam Chinese Government Espionage Email 06/01/2020
Del Rosso [43] Libya Citizens Malware, data theft Email –
Greig [44] World Global shipping firms Malware, espionage Email –
Lallie et al. [7] World Canadian businesses, citizens Malware Email –
Lallie et al. [7] Spain Spanish medical institutes Ransomware Email –
Lallie et al. [7] UK Citizens Malware SMS –
Lallie et al. [7] Spain Citizens Credential theft SMS –
Smithers [45] UK Citizens Credential theft Email, website –
Vergelis [39] Singapore Citizens Credential theft Email –
Xia et al. [34] USA, Japan, Singapore BOA, paypal, Apple, Chase – Website –
Xia et al. [34] Russia Banco de Chile – Website –
AL‐QAHTANI AND CRESCI - 329
#
#by emphasising the value of human lives [29]. In addition to the
useful recommendations, the email also carried an attached ZIP
file, purportedly containing an e‐book about ‘the complete
research/origin of the coronavirus and the recommended guide
to follow to protect yourselves and others’. Upon execution of
the file contained in the archive, the GuLoader malware
downloaded FormBook, a popular trojan used to collect data
from the Windows clipboard, to keylog, and to steal Web
browser data. Stolen data was sent back to a C&C server
operated by the attackers [30]. Notably, the tactic of alternating
legitimate and malicious information during an attack, in order
to increase its credibility, is well‐known and also used in other
online scams, such as in the activity of social bots spreading
untrustworthy information (e.g., fake news) [31]. Similar techniques
were used in other attacks aimed at downloadingmalware
on the victim's system. These attacks were based on a fake NHS
website [32] and on a malicious website imitating the Johns
Hopkins University COVID‐19 dashboard [33], a Web resource
that was widely used during the course of the pandemic. The
WHO was also targeted by another attack. This time, it was
reported that a group of hackers created a malicious website
posing as an email login portal for WHO employees, in an
attempt to steal their passwords. The attempt was declared to be
largely unsuccessful by the WHO itself [10]. Nonetheless, the
increased phishing attacks targeting the WHO and its partners
led the WHO to issue a warning to the general public to raise
awareness on these threats.5 The warning page featured a
dedicated section for phishing attacks.
In the US, another attack was based on emails impersonating
the CDC and asking for donations to develop a COVID‐
19 vaccine. Donations were expected to be made in Bitcoins.
In addition to the typical techniques used to convince victims,
the attackers also asked recipients to share the message as
much as possible, thus aiming to exploit the increased
perceived trustworthiness of messages vetted by close ones [7,
46]. Finally, also communication platforms such as Zoom,
Microsoft Teams and Google Meet, were impersonated in
emails and through fake websites. The latter led to a surge of
Web domain registrations, a significant share of which was later
labelled as outright malicious or suspicious [47].
3.1.4 | The surge of covid‐related domain
registrations
The registration of covid‐related domains was a prominent
phenomenon that held the stage during the first months of the
pandemic. This phenomenon is not new nor peculiar to
COVID‐19. In fact, it is widely recognized that malicious
campaigns, including phishing, benefit from the prompt
exploitation of salient events [48]. In the case of COVID‐19 the
surge in domain registrations was so significant and abrupt to
motivate targeted scientific studies and even investigations by
law enforcement agencies [49, 50]. Among these, a statistical
report from Palo Alto researchers published at the end of
March 2020 showed that a total of 116,357 new domain titles
and registrations related to COVID‐19 were made since the
start of the year. Their results showed that 2% of such domains
were clearly malicious and 34% were considered to be high‐risk
[6]. A subsequent analysis by the security firm Check Point
reached similar results in May 2020, with 17% of the analysed
domains deemed malicious or suspicious [47]. An investigation
by the INTERPOL puts the rise of malicious domain registrations
into context. The INTERPOL measured, from
February to March 2020, a 569% growth in malicious registrations,
including malware and phishing, and a 788% growth in
high‐risk registrations.6 The rationale for exploiting covid‐
related domains in phishing attacks is straightforward. In fact,
as identified in [7, 10], domains using keywords such as ‘covid’,
‘coronavirus’ and ‘corona’ are likely to appear as believable, and
thus massively accessed. To boost accesses, fraudsters also
included other reputable words such as WHO and CDC, or
used appealing keywords such as ‘corona‐virusapps.com’,
‘anticovid19‐pharmacy.com’, and more.
The remaining share of domains that was not involved in
phishing attacks or in other scams was related to non‐malicious
yet nonetheless shady and lucrative practices. The study discussed
in [49] investigated the rationales for such covid‐related
domain registrations. Authors concluded that such domains
were registered mainly for two reasons: (i) for attracting and
then redirecting traffic to other, often totally unrelated, commercial
services; or (ii) for domain parking – that is, the
practice of registering a high‐demand domain in advance, thus
netting profits when reselling the domain later on, once the
demand curve is at its peak.
3.1.5 | Consequences and economic impact
The majority of assessments about the consequences of
phishing attacks derive from governmental bodies and security
firms, with only a small minority of scientific studies covering
this area. Independently of the source, all reports testify a sharp
increase in costs and losses due to cyber‐crime since the start
of the COVID‐19 pandemic. Overall, companies spent $110B
worldwide for protecting against cyber‐attacks in 2020, according
to Accenture's annual security report [1]. A survey by
BAE Systems highlighted the main factors that contributed to
cyber‐crime losses registered during the pandemic [51]. The
main losses derived from: (i) IT overtime for incident response,
remediation and clean‐up; (ii) payments for ransomware attacks;
(iii) operational outages; (iv) legal costs following a major
attack (e.g., in cases of class action lawsuits); and (v) customer
churn, with its associated financial costs.
Investigations from the US Federal Bureau of Investigation
(FBI) contribute to quantify the overall losses and to estimate
the trend with respect to pre‐pandemic conditions. For instance,
5
https://www.who.int/about/cyber‐security
6
https://www.interpol.int/en/News‐and‐Events/News/2020/INTERPOL‐report‐
shows‐alarming‐rate‐of‐cyberattacks‐during‐COVID‐19
330 - AL‐QAHTANI AND CRESCI
#
#the FBI estimated that spear phishing cost US businesses more
than 1.8$B in 2020, up from 1.7$B in 2019. In a notorious case, a
US business specialising in hand sanitizers wired nearly 1$M to
hackers pretending to sell ventilators. Conversely, losses associated
to generic phishing attacks decreased slightly, with 54$M
in losses in 2020, down from 57$M in 2019.7 This trend testifies
the increased personalisation and sophistication of recent attacks,
which in turn, mandates more advanced detection tech-
niques to keep up with the rapid pace of the attackers.
According to the FBI, ransomware attacks were also a
major source of losses, as already highlighted in the BAE
Systems survey [51]. The average ransomware payment reported
in Q4 2020 was in the region of 154,000$. Oftentimes
however, more severe losses derived from downtime and
customer churn rather than from the direct ransomware payment.
In a notable case involving a large US healthcare pro-
vider, losses due to losing customers to rival providers during a
ransomware attack summed up to 67$M. Moreover, while
extortion and high ransomware demands were previously
reserved for big‐budget enterprises, such attacks also hit the
small and medium business sector during the pandemic. The
average ransomware payment demand for SMBs in 2020 was
5600$, while the costs of the incurred downtime reached
247,000$, which represents a 94% increase with respect to
2019. Then according to IC3, the overall cost of ransomware in
the US tripled in 2020, with 29.1$M in losses compared to just
8.9$M in 2019. Notably, the FBI found that phishing emails
were the primary cause of ransomware attacks, underlining the
importance of defending against phishing for reducing the
efficacy of many of cyber‐attacks.
Among the few scientific studies that reported results on the
economic consequences of phishing attacks, is the work in [7].
The analysis focussed on UK firms and revealed that by early
May 2020, more than 160,000 suspect emails had been reported
to theUKNational Cyber Security Centre (NCSC). By the end of
May, 4.6£M had been lost to COVID‐19 related scams with
around 11,206 victims of phishing campaigns. In response, the
NCSC took down 471 fake online shops and Her Majesty's
Revenue andCustoms (HMRC) took down292 fakewebsites [7].
3.2 | Detailed literature review
While the previous section highlighted the rise and the main
characteristics of covid‐related phishing attacks, this section
summarises and presents the results of each study that investigated
such attacks.
3.2.1 | Early and introductory works
Out of all the research published on cyber‐attacks and
COVID‐19 – and specifically phishing attacks – the vast
majority of existing studies focussed on providing descriptions
and characterisations of the types of attacks. This large stream
of research is characterised by relatively general, descriptive
and high‐level analyses, rather than by technical and detailed
discussions. These contributions were among the first to be
made in the aftermath of the pandemic, and served as initial
assessments of such an unprecedented situation. Their utility
was in raising awareness on the increased cybersecurity issues
and in guiding subsequent, more technical and specific,
research. An example of this kind is the work presented in [52],
where the authors made a first step towards fully characterising
the landscape of COVID‐19 themed attacks. In detail, they
considered five classes of attacks – namely, malicious websites,
malicious emails, malicious mobile apps, malicious messaging,
and misinformation. Then, they proposed mapping them to
the Lockheed Martin's Cyber Kill Chain (LMCKC) [53], which
is a model consisting of 7 stages: (i) reconnaissance,
which corresponds to pre‐attack planning; (ii) weaponization,
which corresponds to setting up attack propagation mediums;
(iii) delivery, which corresponds to the attackers penetration
into a victim's system; (iv) exploitation, which corresponds to
the wage of actual attacks; (v) installation, which corresponds
to installation of malicious payloads; (vi) command‐and‐
control, which corresponds to attacker's use of remote access
to victims' systems; and (vii) objectives, which corresponds to
the accomplishment of the attacker's pre‐determined goal.
Finally, they discussed the defence space, with recommendations
on how to defend from malicious websites, malicious
emails, malicious mobile apps, malicious messaging, and malicious
misinformation. Similarly, the work presented in [54]
provided a detailed review about the COVID‐19 cybersecurity
attacks with a critical analysis. The paper also showed the latest
research contributions of cybersecurity during COVID‐19, in
the form of a literature review corroborated by examples of
how Google and Microsoft managed their privacy and cybersecurity,
as well as the deriving limitations. Then, the authors
discussed the reasons why people are vulnerable to cyber‐
attacks, especially with the increase in online activities
brought upon by the pandemic, and proposed unique solutions
to those problems. The goal of the study reported in [55] was
to examine the shift from physical‐ to cyber‐crime at the onset
of the COVID‐19 pandemic. Thus, this work aimed to shed
more light on how crime initially moved to cyberspace and
what were the implications for organisations and individuals.
The author's hypothesis is that there was a shift from physical
to cyber‐crime as a result of the mass quarantine around the
world at the beginning of the pandemic. The author used data
from news articles, government reports, private sector publications,
FBI data, and press releases. The results showed that
the United States Secret Service Cyber‐Fraud Task Force
actually registered an increase in frauds, and that according to
the FBI, cyber‐crime increased by 300% since the start of the
pandemic. The analysis reported in [56] identified the top‐ten
cybersecurity threats that took place during the pandemic.
Phishing emerged as one of the top threats, linked to many
frequent attack vectors such as malicious domain attacks,
malicious websites, malicious emails, malicious social media
7
https://www.vadesecure.com/en/blog/cybercrime‐statistics‐top‐threats‐and‐costliest‐
scams‐of‐2020
AL‐QAHTANI AND CRESCI - 331
#
#messaging, business email compromise and malicious mobile
apps.
Instead, in [57] the authors discussed the types of phishing
attacks and their impact during the COVID‐19 lockdown.
Specifically, they discussed different types and sub‐types of
attacks, such as deceptive phishing, whaling, spear‐phishing,
and pharming, also proposing some general recommendations
for thwarting them. Similarly, the analysis presented in [58]
discussed the security risks associated with working from home
due to the COVID‐19 pandemic and the imposed lockdown. It
discussed the increase in cyber‐attacks due to the pandemic
and provided a number of general recommendations, including
those directed to businesses for backing‐up their data in case of
a ransomware attack, recommendations for secure remote
networks for employees working from home, encouraging
employees to communicate with the IT department regarding
any concerns, periodic penetration testing, and educating employees.
The paper also discussed the challenges related to
dealing with an attack. The author of [9] discussed how the
pandemic‐driven disappearance of home‐work boundaries
expanded the cyber‐attack surface area. The study also gave
recommendations for employers to encourage employees in
using strong encryption on their home routers, strong passwords
on personal accounts, and in being extremely vigilant
with respect to their personal information. In addition to
[9, 58], some other works also focussed on the security challenges
introduced by the shift to remote working. Among
these, [59] discussed how the sudden change to remote work
impacted the security of many organisations. The author
described how the pandemic left many organisations with no
time nor resources to instal extra security measures on work‐
issued devices. The study recommended organisations to utilise
multi‐factor authentication instead of just passwords, and
to rely on end‐to‐end encryption and virtual private networks
(VPNs) for handling company data. Also the discussion in [60]
outlined the many challenges and security concerns caused by
the pandemic, and specifically, by the shift to remote working.
The author discussed the increase in phishing scams that are
preying on COVID‐19 fears and panics, and how cyber‐crime
cost the world 6$ trillion annually by 2021. Similarly to the
many other papers surveyed in this section, also this article
ends with some general recommendations, including the use of
multi‐factor authentication, the use of a VPN with an
encrypted network connection, updating the cybersecurity
policies, and communication between employees and their IT
department. The work in [61] discussed the cybersecurity issues
that have occurred during the COVID‐19 pandemic. The
authors emphasised that there was a correlation between the
pandemic and the increase in cyber‐attacks. Furthermore, they
also highlighted that healthcare organisations were one of the
main victims of cyber‐attacks during the pandemic. The
pandemic has also raised the issue of cybersecurity in relation
to: (i) the ‘new normal’ of expecting staff to work from home,
(ii) the possibility of state‐sponsored attacks, and (iii) increases
in phishing and ransomware. According to the authors, mitigation
techniques for these issues include raising user aware-
ness, utilising VPNs and multi‐factor authentication, ensuring
firmware and antiviruses are updated, and a strong cybersecurity
policy. Authors of [62] presented a discussion on the
vulnerabilities caused by the pandemic and on the many types
of cyber‐attacks experienced worldwide. The ultimate goal of
their analysis was to raise awareness on these issues, and on
cybersecurity in general, as a mandatory defensive step in order
to reduce the number and impact of the cyber‐attacks that
occurred as a consequence of the COVID‐19 pandemic. The
purpose of [63] was to raise awareness on the exploitation of
the pandemic as a cyber‐attack tool and to discuss possible
remediation strategies. The research was conducted through a
review of existing literature from websites and reputable databases,
including Google Scholar and IEEE Xplore. The
themes from the literature sources included the prevalence of
phishing, scamming, spamming, and malware as the common
attack vectors. Business enterprises, including operators in
healthcare, finance, and Internet service provision, were
advised to actively implement risk management plans to
monitor attack vectors and to secure their systems, clients, and
users from the COVID‐19 attack tools.
Still within the large body of initial research on phishing
attacks and COVID‐19, other papers investigated a number of
more specific issues. For example, the work in [20] focussed on
challenges of the heathcare sector, by outlining why cyber‐
attacks have been particularly problematic during COVID‐19
and by defining the ways in which healthcare industries
could better protect patients' data. The paper discussed how
the number of cyber‐attacks increased five‐fold after COVID‐
19, and that 90% of healthcare providers had already
encountered data breaches. Among the proposed mitigation
recommendations were penetration testing, well‐defined software
upgrade procedures, and the utilization of secure net-
works like virtual local area networks. Other scholars focussed
instead on analysing and describing national experiences.
Among them, [64] examined the extent to which organisations
in the UK and their staff were likely to have been prepared for
the unplanned outbreak of home working, along with the
increased cyber‐threats that they had to face. The preparedness
of businesses was evaluated along the following directions:
secure configuration, malware protection, network security,
managing user privileges, incident management, monitoring,
information risk management regime, user education and
awareness, home and mobile working, and removable media
controls. The results showed that the businesses that were
undertaking actions in each of these steps were as follows: 90%
for secure configuration, 88% for malware protection, 83% for
network security, 80% for managing user privileges, 68% for
incident management, 57% for monitoring, 35% for information
risk management regime, 30% for user education and
awareness, 25% for home and mobile working, and 23% for
removable media controls. Results of this analysis were useful
for promptly identifying those security directions requiring
additional efforts. Instead, the author of [65] discussed how
Croatia dealt with the pandemic‐related cybersecurity concerns.
The analysis revealed that Croatia has stayed completely silent
with regards to cybersecurity hazards, and it has left companies
to figure out their own ways of reacting to the increased cyber‐
332 - AL‐QAHTANI AND CRESCI
#
#threats, without even warning individuals. The analysis then
moved on to discuss the cybersecurity threats associated with
remote working, the Croatian cybersecurity legal regulation,
Croatia's (lack of) response to the increased cybersecurity
threats, and liability for personal data breaches arising from
cybersecurity attacks. The author concluded by making some
recommendations such as cybersecurity auditing, use of multi‐
factor authentication, and use of VPN solutions for connecting
to the workplace. In [66], the authors conducted a study by
identifying cyber‐incidents in Indonesia that exploited
COVID‐19. The analysis made use of a timeline that mapped
key events and cyber‐attacks to analyse targeted sectors and
their cybersecurity issues. The study illustrated how cyber‐
criminals artfully exploited pandemic issues and situations as
baits for social engineering techniques. In the analysed cyber‐
incidents, criminals using social engineering techniques took
advantage of the issue of COVID‐19 by not having a specific
target so that anyone could become a victim of their attacks.
Finally, differently from all works described above, the analysis
presented in [67] focussed on the skills needed by the cybersecurity
workforce in relation to the novel situation caused by
the pandemic. Specifically, the authors argued that the cybersecurity
workforce, which was already suffering a digital skills
crisis, also lacked the adequate soft skills required to effectively
tackle the insider threat that was exacerbated by the pandemic.
The work first examined the insider threat, and why it became
so much more insidious because of COVID‐19. Then, it
looked into the essential soft skills required to tackle this
threat, before examining how organisations could effectively
implement an apprenticeship strategy capable of generating
professionals with both hard and soft skills. The authors
concluded that many of the covid‐related issues could have
been avoided if the industry had not relied so heavily on
recruiting graduates rather than apprentices – that is, people
trained directly in cybersecurity by the company itself.
3.2.2 | Systematic analyses
Following the first wave of introductory research, some
scholars carried out systematic and large‐scale analyses of some
of the attacks that occurred during the first months of the
pandemic. For instance, in [68] the authors carried out a
comprehensive measurement study of online social engineering
attacks, with specific references to phishing. By collecting,
synthesising, and analysing DNS records, Transport Layer
Security (TLS) certificates, phishing URLs, phishing website
source code, phishing emails, web traffic to phishing websites,
news articles, and government announcements, they tracked
trends of phishing activity between January and May 2020 and
sought to understand the key implications of the underlying
trends. They found that phishing attack traffic in March and
April 2020 skyrocketed up to 220% of its pre‐COVID‐19 rate,
far exceeding typical seasonal spikes. The results also showed
that there was a record high of phishing victims during this
period, and that attackers remained several steps ahead of
typical modern anti‐phishing defenses. Findings from this
analyses could be used to develop more effective phishing
detection techniques. Then, the study in [27] developed a
multi‐level influence model to explore how cyber‐criminals
exploited the COVID‐19 pandemic by assessing situational
factors, identifying victims, impersonating trusted sources,
electing attack methods, and employing social engineering
techniques. Content and thematic analysis was conducted on
185 distinct COVID‐19 cyber‐crime scam incident documents,
including text, images and photos provided by FraudWatch, a
global online fraud and cybersecurity company tracking
worldwide COVID‐19 related cyber‐crime. The analysis
revealed interesting patterns about the sheer breadth and diversity
of COVID‐19 related cyber‐crime incidents and how
these crimes were continually evolving in response to changing
situational factors related to the pandemic. Similarly, the aim of
[69] was that of contributing to users' protection by exploring
online perpetrators' modus operandi applied to exploit Internet
users' coronavirus fears through phishing emails. To that end,
the content of 208 coronavirus‐themed phishing emails was
examined. The data was collected by searching for variations of
the terms ‘COVID‐19 phishing emails’ from search engines,
and then using the images from official websites such as the
Action Fraud, FBI, or web pages of universities or companies'
IT departments. 2372 images were collected in this way. The
results showed that phishers mostly employed social engineering
methods to coerce individuals into providing sensitive
information. The authors also identified 9 main variations of
phishing emails. While the previous work focussed on phishing
emails, the authors of [70] presented a systematic study of
coronavirus‐themed Android malware. First, they made a daily
growing COVID‐19 themed mobile app dataset, which contains
4322 COVID‐19 themed apk samples (2500 unique apps)
and 611 potential malware samples (370 unique malicious
apps) by the time of mid‐November, 2020. The authors then
presented an analysis of them from multiple perspectives
including trends and statistics, installation methods, malicious
behaviours and malicious actors behind them. The authors
observed that the COVID‐19 themed apps as well as malicious
ones began to flourish almost as soon as the pandemic broke
out worldwide. Most malicious apps were camouflaged as
benign apps using the same app identifiers (e.g., app name,
package name and app icon). Their main purposes were either
stealing users' private information or making profit by using
tricks like phishing and extortion. Notably, several of the
characteristics identified in this study are currently exploited as
part of many detection techniques for protecting against
phishing attacks mounted by means of malicious apps [52].
Moving on with relevant systematic analyses, in [71] the authors
presented the first measurement study of COVID‐19
themed cryptocurrency scams. They first created a comprehensive
taxonomy of COVID‐19 scams by manually analysing
the existing scams reported by users from online resources.
Then, they proposed a hybrid approach to perform the
investigation by (i) collecting reported scams in the wild, and
by (ii) detecting undisclosed ones based on information
collected from suspicious entities (e.g., domains, tweets, etc.).
195 confirmed COVID‐19 cryptocurrency scams in total were
AL‐QAHTANI AND CRESCI - 333
#
#collected, including many well‐known cryptocurrency scams
[72], such as: 91 token scams, 19 giveaway scams, 9 blackmail
scams, 14 crypto malware scams, 9 Ponzi scheme scams, and
53 donation scams. Over 200 blockchain addresses associated
with these scams were then identified, which led to at least 330
$K in losses from 6329 victims. For each type of scams, the
tricks and social engineering techniques they used were further
investigated. To facilitate future research, the authors released
all the well‐labelled scams to the research community.8 The
data for COVID‐19 scams were obtained from BitcoinAbuse,
CryptoScamDB, Threat Intelligence Platforms (e.g. AlienVault,
McAfee), and StopScamFraud. The authors also obtained data
about COVID‐19 themed cryptocurrency scams using a semi‐
automated analysis on Etherscan to search for scam tokens,
URLScan, RiskIQ, VirusTotal to search for scam domains,
Koodous, VirusTotal, AVClass to find Android apps and label
the app malware families, and Twitter and Telegram to identify
more scams. Given the surge of covid‐related malicious
domain registrations, the authors of [34] focussed on identifying
and characterising COVID‐themed malicious domain
campaigns, including the evolution of such campaigns, their
underlying infrastructures and the different strategies taken by
attackers behind these campaigns. Their exploration uncovered
some common features of malicious domains, which can help
to identify new malicious domains and to raise alarms at the
early stage of their deployment. The results also showed peaks
in malicious domain registrations in March 2020, indicating
bulk registrations that accounted for 73.2% of all malicious
domains. The first registered domain was ‘clientdoc.us’, which
hosted multiple COVID‐19 related phishing subdomains like
‘banking.covid19.hsbc.clientdocs.us’ and ‘covid19update.hsbc.
clientdocs.us’. The authors also identified 15 verified attack
campaigns that were used for phishing, malware, and domain
squatting. Finally, similarly to the previous study, also [49]
performed an analysis at Internet‐scale of COVID‐19 domain
name registrations during the early stages of the virus' spread.
The authors leveraged the DomainTools COVID‐19 Threat
List and additional measurements to analyse over 150,000
domains registered between 1 January 2020 and 1 May 2020.
They identified two key rationales for covid‐related domain
registrations: (i) online marketing, by either redirecting traffic
or hosting a commercial service on the domain; and (ii) domain
parking, by registering domains containing popular COVID‐19
keywords, presumably anticipating a profit when reselling the
domain later on.
3.2.3 | Studies based on questionnaires, surveys
and interviews
Another remarkable body of work about phishing and other
cyber‐attacks in relation to COVID‐19 relied on the use of
questionnaires, surveys and interviews as tools for assessing
the perception, readiness and effect of such attacks on those
that experienced them. As part of this literature, the idea of
the study presented in [73] was to examine how teleworking
affected employee perceptions of organizational efficiency
and cybersecurity, before and during the COVID‐19
pandemic. The research was based on an analytical and
empirical approach. The quantitative approach involved the
design of a structural equation model, one of the most
widely‐used approaches to causal inference [74], on a sample
of 1101 respondents from the category of employees in
Montenegro. Within the model, the authors examined
simultaneously the impact of the employees' perceptions on
the risks of teleworking, changes in cyber‐attacks during teleworking,
organisations' capacity to respond to cyber‐attacks,
key challenges in achieving an adequate response, as well as
the perceptions of key challenges related to cybersecurity.
Perhaps surprisingly, the main findings of the research were
that teleworking had no impact on digital information security,
and that teleworking had a positive and significant impact
on organizational efficiency perceptions. Similar conclusions
were reached in [24], where authors discussed how the
pandemic impacted the IT industry in terms of the IT security
implications, the impact on global IT, and the increase
in COVID‐19 phishing attacks and malware. The authors
used a survey to demonstrate how the industry was able, for
the most part, to cope with and address the challenges
brought by the COVID‐19 crisis. With similar techniques, the
analysis carried out in [75] evaluated the cybersecurity culture
readiness of organisations from different countries and
business domains, when teleworking became a necessity due
to the COVID‐19 crisis. The authors designed a targeted
questionnaire and conducted a web‐based survey addressing
employees while working from home during the COVID‐19
spread over the globe. The questionnaire contained 23
questions and was available for almost a month, between
April and May 2020. During that period, 264 participants
from 13 European countries spent approximately 8 min to
answer it. Gathered data were analysed from different perspectives,
allowing to find answers regarding the information
security readiness and the resilience of both individuals and
organisations. Some of the results of the research showed
that 53% of employees reported to not having received any
cybersecurity guidance with regards to working from home,
44.44% had no possibility of working from home, and about
15% reported having faced some kind of cyber‐threat. Still
related to perceptions, the research in [76] examined the
relationship between teleworking cybersecurity protocols
during the COVID‐19 era and employee perception of their
efficiency and performance predictability. The premise of this
research project was that teleworking could transform employees
into unintentional insider threats. Interviews were
conducted through video conferencing with nine employees
in Virginia, USA to examine the problem and collect data.
The data from the interviews was then analysed using
narrative analysis to unpack some of the common themes
from the interviews [77]. The major findings demonstrated
that employees were trusting the cybersecurity protocols that
their organisations implemented, but that they also believed8
https://covid19scam.github.io/
334 - AL‐QAHTANI AND CRESCI
#
#they were vulnerable, and that the protocols were not as
reliable as in‐person working arrangements. While the respondents
perceived that the cybersecurity protocols lend to
performance predictability, they also appeared to think it
disrupted their efficiency.
Other studies focussed instead on the effects of cyber‐
attacks and of the specific techniques used to carry them
out. The experiment described in [78] examined the effects of
persuasive appeals in phishing messages on judgements of
credibility. Participants were tasked with reading a combination
of legitimate and phishing e‐mails to determine whether each
message was legitimate or a scam. When phishing messages
included more appeals to authority and likability, phishing
susceptibility increased. However, as the number of fear and
urgency appeals in the message increased, phishing susceptibility
decreased, as it was easier for participants to detect the
phishing attempt. Interestingly, results showed that appeals to
authority and likability increased credibility, while appeals to
fear, urgency, and social proof decreased judgements of credibility.
Moving on, in [79], the authors investigated how the
pandemic affected rates of cyber‐victimization. The study
considered the pandemic as a natural experiment, thus allowing
the comparison between pre‐pandemic rates of victimization
and post‐pandemic ones, leveraging datasets originally
designed to track cyber‐crime. In particular, the authors built
two samples that they used to conduct a survey: (i) one related
to the pre‐COVID‐19 situation consisting of 1109 participants,
and (ii) another one for the post‐COVID‐19 situation counting
1021 participants. After considering how the pandemic may
have altered routines and affected cyber‐victimization, the
study found that the pandemic did not radically alter cyber‐
routines nor changed cyber‐victimization rates.
The last study that we reviewed in relation to attacks made
use of a simulation to evaluate the vulnerability of different
groups of employees to phishing during COVID‐19 [80]. In
particular, the authors performed a comparative study of
cybersecurity awareness of employees working in different
departments within the same organisation in Bangkok,
Thailand. In their experiment, they exposed different employees
to simulated phishing attacks and evaluated their ac-
tions. After data collection and analysis, the authors found
significant differences in the cybersecurity awareness levels
between Thai employees from technology‐based departments
(e.g., IT department) and social‐based departments (e.g., HR
department) within the same organisation, with the latter
group that showed to be more vulnerable to phishing attacks
than the former one. Simulations such as the one described in
[80] have recently been regarded as a promising tool for
training staff in preparation for future cyber‐attacks. For
instance, in the context of healthcare professionals, [81] proposed
to carry out cybersecurity campaigns in which members
of the IT departments send out fake phishing emails to the rest
of the staff and provide further training to those who fail to
identify the phishing emails. However, in spite of the widespread
awareness of cybersecurity limitations of the healthcare
sector [25, 26] and of the advices, such as those of [81], given
several months before the outbreak of COVID‐19, few
enterprises enacted significant changes, which worsened the
impact of the massive wave of phishing attacks occurred in the
aftermath of the pandemic.
4 | COUNTERMEASURES
While the previous section focussed on the drivers and the
characteristics of phishing attacks occurred during the
COVID‐19 pandemic, this section discusses the proposed
defenses and countermeasures to such attacks.
4.1 | Overview and synthesis
The multitude of ways in which phishing attacks were
mounted demanded the development of a broad array of
different solutions. Each solution surveyed and described
here exploits some characteristics of COVID‐19 phishing
attacks, such as those that we discussed in Section 3. First, we
summarise the main approaches adopted for detecting
phishing attacks during COVID‐19. Then, we focus on the
key factors that influence the effectiveness of machine
learning solutions, that is: data, methods (i.e., algorithms) and
features. Hence, we highlight the available datasets for this
task, as well as the methods and the features used for
developing detectors. Table 3 supports and complements this
discussion by presenting a detailed classification and comparison
of the techniques that were recently proposed for
detecting COVID‐19 phishing attacks.
4.1.1 | Approaches
Among all solutions that were recently proposed to defend
from phishing attacks, the vast majority was aimed at detecting
phishing websites, as also shown in Table 3. This finding is
perhaps unsurprising, considering that emails and websites
were the most frequent attack vectors exploited during the
COVID‐19 pandemic [1]. The most straightforward way to
tackle the task of detecting COVID‐19 phishing websites is by
analysing website contents. Approaches of this kind typically
revolve around assessing the presence or absence of covid‐
specific keywords in website names and contents (e.g., coronavirus,
COVID‐19, masks, n95, and more) [52]. Another
frequent approach to the detection of phishing websites is
based on the analysis of the website's URL. To this end, it was
observed that attackers frequently used cybersquatting and
typosquatting techniques, or techniques to obtain homograph
domain names, to make COVID‐19 themed malicious websites
mimic legitimate ones [94], which highlights the importance
and usefulness of detecting such modified URLs. Other approaches
focus instead on the website's age, since malicious
websites tend to be more recent than authoritative ones [91].
The works in Table 3 that target phishing websites represent
notable examples of the combination of the aforementioned
approaches.
AL‐QAHTANI AND CRESCI - 335
#
#The second most‐common approach for detecting phishing
attacks grounds on the analysis of emails, another
frequently used attack vector. Similarly to systems for detecting
phishing websites, also many systems for phishing email
detection are based on the analysis of email contents. For
instance, covid‐related keywords – such as those related to
cures, guidelines, or offers – can be searched in subject lines
and in the textual contents [52]. Instead, other techniques
based on email content analysis focus on the links contained in
the email, or on its attachments. The former systems typically
analyse the URL of the links by means of the same techniques
already described for the analysis of website's URLs. The latter
are instead aimed at assessing the harmfulness of any file
attached to the email, for instance by means of static and dynamic
analyses of the file's content. Finally, another common
approach to the detection of COVID‐19 phishing emails is
based on spotting email spoofing or masquerading attacks.
Here, the analysis is aimed at verifying the identity of the
sender, for example, by analysing the headers of the email [86].
COVID‐19 themed malicious apps are another common
vector for phishing attacks. A set of approaches for defending
against this threat is based on computer vision techniques that
assess the visual similarity of new app logos with those of
legitimate existing apps [52]. Other techniques are instead
based on static and/or dynamic analyses of the apps, in order
to detect malicious ones (e.g., repackaged apps). A minority of
approaches also aims at detecting spoofed app names, for
example, by computing string edit distances between the
names of new apps with respect to existing and popular ones.
Smishing and vishing attacks represent a minority of all
phishing attacks occurred during the pandemic. As such, only
few works specifically targeted these attacks, as also shown in
Table 3. Textual analyses of the content of the messages—in
the case of smishing, or of the call transcripts—in the case of
vishing, is by far the most common approach for detecting
these types of attacks. Such analyses can be carried out by the
adoption of natural language processing techniques, for
instance with the goal of spotting suspicious content, such as
TABLE 3 Detailed classification and comparison of some recently proposed techniques for detecting COVID‐19 phishing, smishing and vishing attacks
Reference Year Focus Dataset Target Methoda Features Evaluationb
Mishra & Soni [82] 2021 Smishing [83] + pinterestc SMSs Deep learning, RF, NB, DT SMS text Test accuracy = 0.98
Biswal [84] 2021 Vishing [85] Calls SVM, LR, MP Call transcript text Test accuracy = 0.65
Wu & Guo [86] 2021 Phishing Own (unreleased) Emails Document embeddings, anomaly
detection
SMTP headers Case‐study and comparison
against commercial
solutions
Sarma et al. [87] 2021 Phishing Mendeleyd Websites kNN, RF, SVM, LR URL, website
content, website
metadata
Test F1 = 0.98
Mukhopadhyay &
prajwal [88]
2021 Phishing Own (unreleased) Emails,
websites,
malware
Blacklists, heuristics IP, URL, email
attachments
Case‐study and comparison
against commercial
solutions
Ispahany & Islam
[89]
2021 Phishing DomainToolse URLs SVM, kNN, NB URL Test accuracy = 0.99
Xia et al. [34] 2021 Phishing Own (unreleased) Websites,
URLs
Knowledge graphs, graph
representation learning,
graph clustering
IP, URL Qualitative and case‐study
Tawalbeh et al. [90] 2020 Phishing Own (unreleased) Malware Deep learning Email attachments Training accuracy = 0.85
Saha et al. [91] 2020 Phishing Kagglef Websites MP IP, URL, website
metadata
Test accuracy = 0.93
Basit et al. [92] 2020 Phishing UCI machine
learning
repositoryg
Websites Ensemble of classifiers (RF,
kNN, DT)
URL Test accuracy = 0.97
Pritom et al. [93] 2020 Phishing CheckPhishb,h
DomainToolse
Websites RF, kNN, DT, LR, SVM URL, website
metadata
Test accuracy = 0.98
aDT, decision tree; kNN, K‐nearest neighbours; LR, logistic regression; MP, multilayer perceptron; NB, naïve Bayes; RF, random forest; SVM support vector machine.
bIn case the reference paper reported multiple evaluation results, here we list only the best one.
chttps://in.pinterest.com/seceduau/smishing‐dataset.
dhttps://doi.org/10.1016/j.procs.2020.03.294.
ehttps://www.domaintools.com/resources/blog/free‐covid‐19‐threat‐list‐domain‐risk‐assessments‐for‐coronavirus‐threats.
fhttps://www.kaggle.com/akashkr/phishing‐website‐dataset.
ghttps://archive.ics.uci.edu/ml/datasets/phishing+websites.
hhttps://checkphish.ai/coronavirus‐scams‐tracker.
336 - AL‐QAHTANI AND CRESCI
#
#the presence of spoofed URLs, special characters, and
COVID‐19 themed keywords [52]. Other sophisticated approaches
are also based on natural language processing, but
this time the aim is that of detecting persuasive messages that
make use of propaganda techniques [28] or other social engineering
techniques [95]. These latter works lay at the
intersection of cyberpsychology and natural language processing
[96].
4.1.2 | Datasets
High quality and reference datasets represent an important
resource to foster research and experimentation on novel scientific
issues [97]. However, building such resources is noto-
riously challenging and time‐demanding [98]. In the case of
COVID‐19 phishing attacks, publicly available reference
datasets are few and far between. In addition to the aforementioned
generic challenges, scholars interested in building a
scientific dataset for covid‐themed phishing attacks also had to
account for the recency and unpredictability of the pandemic
(and its associated scamdemic), and for its rapidly evolving
nature. As a result, at the time of writing no reference dataset
for COVID‐19 phishing attacks exists and scholars tackling
phishing detection either had to build their own dataset or to
rely on existing, yet older, ones.
The only partial exceptions to the above consideration are
the datasets released by DomainTools9 and CheckPhish.10
Both companies were extremely rapid to intervene against the
deluge of malicious domains that plagued the Web during the
first months of the pandemic. They curated and periodically
updated lists of scam covid‐themed websites and made such
lists publicly available. As also shown in Table 3, datasets from
DomainTools and CheckPhish were used by a subset of the
papers that proposed website and URL COVID‐19 phishing
detection systems, such as [89, 93]. Unfortunately, as of now
both the DomainTools and the CheckPhish datasets appear to
be no longer publicly available. To partially ameliorate this
issue, DomainTools suggested another publicly available
dataset,11 curated by the COVID‐19 Cyber Threat Coalition.
To the best of our knowledge, no scientific study has been
conducted on such dataset.
The novelty of the issue and the lack of reference datasets
forced many scholars interested in experimenting with
COVID‐19 phishing detection to build their own dataset. For
instance, this route has been chosen for the development of
the HOLMES [86] and EDITH [88] systems, and for the
systems presented in [34, 90]. This approach has however
several drawbacks. First, none of the datasets built in this way
were made publicly available by the respective authors, thus
hindering replicability and future research along this direction.
Second, the datasets are related to very specific issues and have
been collected with ad‐hoc methodologies. As a practical
example, the dataset used in [86] was obtained from the SMTP
server of an unspecified firm. As a consequence of these
limitations, datasets built ad‐hoc for a specific study are often
small, which raises concerns about the validity and generality of
the results obtained from their analysis.
An orthogonal approach to building an ad‐hoc dataset
involves the use of well‐known existing datasets. For instance,
the datasets originally used in [83, 85] were also used to train
and evaluate the systems recently proposed in [82, 84]. Similarly,
other scholars used data published in well‐known scien-
tific repositories such as Mendeley, Kaggle and the UCI
collection of machine learning datasets. However, also this
solution presents an important drawback. In this case, some
systems were designed with COVID‐19 phishing attacks in
mind, but the lack of specific reference datasets forced authors
to evaluate their proposed system against other attacks. In
many cases, the attacks contained in the used datasets occurred
way before the start of the COVID‐19 pandemic. Again, the
concern is about the reliability of the results of such systems –
some of which are remarkably good, as visible in Table 3 – that
were designed and proposed for the COVID‐19 scenario, but
were evaluated otherwise.
4.1.3 | Methods and features
The previous section highlighted the limitations of current
research with respect to the choice of datasets for training and
evaluating detectors. Similar considerations also apply to the
choice of machine learning algorithms and features. Indeed,
the choice of a machine learning algorithm strongly depends
on the characteristics of the available data [99]. To this regard,
the most powerful and advanced analytical methods currently
available are based on deep learning. Deep learning algorithms,
however, require massive datasets for training, which are not
yet available for the task of COVID‐19 phishing detection. As
such, and also due to the relatively limited time passed since
the start of the pandemic, the vast majority of existing detectors
are based on simpler, general‐purpose and off‐the‐shelf
classification algorithms. Table 3 shows that nearly all traditional
classification algorithms were tested for the detection of
COVID‐19 phishing attacks. These include algorithms such as
decision trees and random forests, logistic regression, k‐nearest
neighbours and support vector machines. Clearly, these
represent the quickest and most straightforward way of tackling
a classification task, such as that of phishing detection.
Simplicity, scalability and mild data requirements however
come at the cost of predictive power and generalisability. The
adoption of more complex methods, such as those based on
deep learning that were used in [82, 90], is still largely overlooked.
A minority of systems are also based on ensembles of
supervised classifiers—such as [92], or on unsupervised machine
learning—such as [86].
The machine learning features used by the existing detectors
are mainly based on the textual content of the item
under investigation (e.g., a website, email, SMS, etc.). In fact,
9
https://www.domaintools.com/resources/blog/free‐covid‐19‐threat‐list‐domain‐risk‐
assessments‐for‐coronavirus‐threats
10
https://checkphish.ai/coronavirus‐scams‐tracker
11
https://www.cyberthreatcoalition.org/blocklist
AL‐QAHTANI AND CRESCI - 337
#
#the text has long been the most widely used data modality in
many detection tasks [100]. In the context of phishing attacks,
textual content can be found in emails, websites, OSNs, text
messages (e.g., SMSs or any other message in instant messaging
apps), call transcripts and app information. In addition, the
analysis of URLs can also be considered as a form of text
analysis. Because of the ubiquity of text, almost all COVID‐19
phishing detection systems leverage textual features. The current
state‐of‐the‐art for extracting textual features is based on
deep learning, and particularly, on artificial intelligence
methods for natural language understanding [101]. However,
the solutions exploited in the surveyed phishing detection
systems are again largely based on more traditional and less
powerful approaches. For example, bag‐of‐words features or
simple sequences of characters and words (i.e., character and
word n‐grams) were used as text features in [84]. As such, the
application of more recent and powerful text feature extraction
techniques is still unexplored, with the exception of the
HOLMES system that uses unsupervised word embeddings as
text features [86]. The issue related to the use of simple and
‘shallow’ features also emerges when surveying systems that
also leverage other data modalities. For example, many
different features can be used for the detection of phishing
websites, thus going beyond the mere analysis of the textual
content of the website. Among such features are images, links,
the HTML code and CSS documents of the website, JavaScript
features, ActiveX Objects and forms [16]. However, the website
classification systems reported in Table 3 almost exclu-
sively rely on the analysis of the website's URL and on the
assessment of the presence of certain covid‐related keywords.
Similarly, assessing the validity of URLs could involve querying
DNS services and retrieving WHOIS and web traffic data [16],
which is seldom done in the case of the analysed COVID‐19
phishing detectors.
4.2 | Detailed literature review
The literature discussing countermeasures to phishing attacks
is mainly organised in two large bodies of work. The first body
of work proposes general and long‐known recommendations,
and discusses their application to the specific and novel situation
caused by the COVID‐19 pandemic. Part of the literature
in this body of work overlaps, or is anyway similar, to the
introductory works already discussed in Section 3.2.1. Instead,
the second category of papers take an orthogonal approach to
the problem of phishing attacks during COVID‐19 and proposes
ad‐hoc technical solutions, the majority of which is based
on machine learning, for automatically and promptly detecting
such attacks.
4.2.1 | Works proposing general
recommendations
Our analysis of the papers that provided actual recommendations
to defend against phishing attacks reveals that
the majority of works suggested a combination of the
following three general strategies: (i) increasing user awareness
of phishing attacks, which was suggested in [57, 61–63,
102]; (ii) resorting to multi‐factor authentication, proposed
in [57, 59–61, 65, 102]; and (iii) resorting to the use of
VPNs, which was proposed in [59–61, 65, 102]. Among
these works, the authors of [61, 102] provided all three
aforementioned recommendations. In particular, [102] first
conducted a survey to investigate the types of cyber‐attacks
that users suffered during COVID‐19, as well as the level
of knowledge and the technical challenges faced by users
who switched to remote services during the pandemic. The
survey highlighted phishing emails as the most common
type of attack, corroborating previous findings [1, 7]. Part
of the survey was also targeted at understanding victim
behaviours when they were attacked. Surprisingly, as much
as 62.5% of respondents admitted that they did not take
any specific countermeasure because of a lack of awareness
and understanding of the type of attack. Results such as
those presented in this study motivate this body of research
– namely, studies that analysed the initial situation of the
pandemic and that rapidly intervened to provide simple, yet
relatively effective, recommendations such as those listed
above.
In addition to the previous ‘horizontal’ works that provided
general recommendations, some scholars also carried
out ‘vertical’ analyses by focussing on specific issues and
relevant case‐studies. As a notable example of this kind, [103]
investigated the task of measuring cyber‐resilience, a preliminary
– yet mandatory – step towards the development of
better countermeasures to cyber‐attacks. The paper highlighted
common misunderstandings in the definition and
notion of cyber‐resilience, which impair our capacity to
measure it. They stressed the importance of considering
systems' abilities to recover and to adapt, and not just to
resist to cyber‐attacks. The paper also proposed different
methods for measuring cyber‐resilience, taking into account
cyber‐security implementations as well as adversarial models.
Still related to the analysis of cyber‐resilience, [104] analysed
how a global financial institution (GFI) dealt with the
cybersecurity challenges posed by COVID‐19. Authors conducted
semi‐structured in‐depth interviews with 11 key actors
from the GFI and leveraged Hollnagel's four abilities for
resilient performance as a theoretical lens for their evaluation
[105]. Among the main findings of the research was that the
organisation performed well in terms of cyber‐resilience, in
the sense that the number and impact of cyber‐incidents did
not significantly increase after the COVID outbreak. The
interviews also revealed that all four abilities of resilience
were formally developed prior to the COVID‐19 outbreak.
The analysis however also showed that the favourable performance
was obtained through many actions undertaken
reactively rather than proactively, as it is instead advisable for
a number of cybersecurity issues [106]. As such, [104] leaves
open the question as to whether the four potentials should be
developed beforehand, in order to perform resiliently during
crises.
338 - AL‐QAHTANI AND CRESCI
#
#4.2.2 | Works proposing technical solutions
The general advices discussed in the previous section can be
beneficial in reducing the frequency of successful phishing
attacks [58]. This is the reason why so many researchers and
practitioners rushed to make these recommendations in the
first months of the COVID‐19 pandemic. However, at the
same time, none of these countermeasures is capable of
completely solving the problem. For instance, studies that
analysed advanced phishing attacks made via sophisticated
phishing toolkits or via phishing‐as‐a‐service, showed that
such attacks are capable of evading two‐factor authentication
schemes [107]. The same result can also be achieved simply by
mounting more elaborate social engineering attacks.12 As such,
the need for technical and intelligent systems for detecting
such phishing attacks remains. In the remainder of this section
we discuss relevant works that provided this kind of
contribution.
As anticipated, the majority of technical countermeasures
to phishing attacks is based on machine learning. As such, the
main goal of the work discussed in [108] was to identify and
propose ways in which machine learning techniques could be
deployed for the detection of diverse types of cyber‐crimes,
such as phishing, identify theft, hacking, distributed denial of
service, email bombing, and digital stalking. Authors discussed
different types of machine learning‐based implementations in
cyber‐crime mitigation, including the discussion of ways in
which machine learning could contribute to phishing detection,
with particular reference to the detection of phishing emails via
analysis of the headers and body of the emails. The techniques
proposed in [108] are effectively used in the following systems.
In [86], the authors introduced a novel AI‐based anomalous
email detector – HOLMES – that can effectively tackle the
challenge of anomalous email detection. HOLMES uses the
email headers as input for the machine learning algorithm.
Furthermore, it combines word embeddings with novelty
detection to discover anomalous behaviours from a high volume
of mirrored SMTP traffic in a large‐scale enterprise
environment. Its performance was measured in a limited
number of case‐studies, and its detection capability was
compared with several well‐known commercial detectors. The
evaluation showed that HOLMES significantly outperformed
those commercial products in all considered attack scenarios.
During the development of the system, emphasis was also put
with respect to its efficiency and capacity to run in environments
characterised by a limited availability of computational
resources. Also the EDITH system, proposed in [88], was
designed to detect phishing emails. Specifically, EDITH (the
Email Disintegration Intrusion‐Detection of Trojan Hacktool)
aims at identifying the embedded malware files and fake
websites that are often present in phishing emails. EDITH
takes emails exported from Thunderbird or Gmail and scans
for URLs or attachments. It compares them to the VirusTotal
database and applies a blacklist approach and heuristics to
detect possible phishing and malicious emails. The peculiarity
of this system is its capacity to simultaneously scan for
phishing links and malware attachments. However, from the
analytical perspective the system relies on rather simple
methods (i.e., blacklists and heuristics). For the future it could
thus be advisable to adopt a similar approach for the detection
of phishing links and malware, but to consider the adoption of
more powerful methods based on machine learning and AI.
Similarly to [88], also the system proposed in [90] is designed to
detect malicious emails. This time however, only the content of
email attachments are analysed and, as such, the system is
specifically focussed on detecting malware. Authors of [90]
proposed to rely on deep learning for performing the detection.
However, some important details of their methodology
are undisclosed, including the type of deep learning architecture
and the types of features used by their system.
Several systems were also developed to detect phishing
websites. To this end, the analysis presented in [87] experimented
with various machine learning classifiers, including k‐
nearest neighbors (kNN), random forest, support vector machines,
and logistic regression. Authors relied on a public
dataset available on Mendeley, comprising 5000 phishing
websites and 5000 real websites, described by 48 machine
learning features mostly based on website content and metadata.
Results of the evaluation campaign in [87] showed that
the random forest classifier achieved the best performance,
with F1‐score = 0.98. Comparable approaches were discussed
in [92, 93]. In particular, [92] proposed an ensemble method to
effectively detect website phishing attacks. The authors
selected three well‐known machine learning classifiers such as
artificial neural network (ANN), kNN, and DT, to use in an
ensemble method together with a random forest classifier (RF).
The authors used a dataset from the UCI machine learning
repository with 11,055 instances and 30 features. Similarly to
[87], also in this case the dataset is almost balanced, with 4898
legitimate instances and 6157 phishing instances. The results
show that the ensemble with kNN + RF achieved the best
results, with accuracy = 0.97 and TP rate = 0.983, followed by
the ANN + RF with TP rate = 0.981 and by the DT + RF with
TP rate = 0.977. In [91] the authors proposed an ANN model
that categorizes websites into either 1 of 3 categories: (i)
phishing websites, (ii) suspicious websites, and (iii) legitimate
websites. To perform the detection, the system leverages a
publicly available Kaggle dataset comprising more than 10,000
instances of legitimate and phishing websites, described by
features extracted from the IP address, the website's URL and
its metadata. The ANN model used is the multilayer perceptron,
a very simple kind of ANN architecture. As such, better
results are foreseeable by the adoption of more sophisticated
classification algorithms or ANN architectures. A somewhat
simpler approach to the detection of phishing websites is the
detection of phishing URLs. For this latter task, only the URL
string of a website is considered, which inevitably leads to a
much narrower array of possible features to leverage for the
detection. Among the systems that tackled this task, is [89].
The authors proposed a classification approach that exploits
12
https://www.agari.com/email‐security‐blog/phishing‐attacks‐two‐factor‐
authentication/
AL‐QAHTANI AND CRESCI - 339
#
#only 5 features extracted from URLs. In addition to traditional
and largely used features such as the length of the URL and
features counting the number hyphens, [89] also used a feature
computed as the Shannon entropy of the URL. Experimental
results involved the use of support vector machines, kNN and
naïve Bayes classifiers. The best classification results were
achieved by kNN with accuracy = 0.99 on the test‐set. Surprisingly,
the authors measured no gain in detection perfor-
mance when adding the entropy feature to the set of more
traditional features – a finding that contrasted with earlier results
[109, 110]. The reason for this result could however be
due to the simplicity of the task tackled in [89], which could
already be addressed with remarkable accuracy by only
leveraging traditional URL features.
The work presented in [34] dealt with the proliferation of
malicious domains campaigns. Differently from previous
works that tackled the classification of individual websites, the
goal of this work was the detection of malicious campaigns.
Authors defined malicious domains campaigns as groups of
related malicious websites. At first, they demonstrated the
widespread presence of such campaigns, especially in the first
months of the pandemic which were characterized by the surge
of covid‐related domain registrations. Then, they also proposed
a detection strategy. The proposed solution is based on 3
steps: (i) the construction of a knowledge graph of domains,
where related domains are linked together; (ii) the graph representation
learning step, where an informative representation
is computed for each node in the graph (i.e., each domain), in
the form of a feature vector; and (iii) the graph clustering step,
where similar domains are clustered together, based on their
representation. In [34], the clustering step was used to group
together the domains belonging to the same malicious
campaign, thus effectively leading to discover and characterize
malicious campaigns. Based on its characteristics, [34] represents
one of the most advanced solutions to the detection of
phishing (websites) in the context of COVID‐19. First of all, it
employs state‐of‐the‐art methods, such as knowledge graphs,
graph representation learning and graph clustering, instead of
traditional classification algorithms. Then, it proposes a solution
based on unsupervised machine learning, which was
recently proven to be more resilient to the inevitable evolution
of cyber‐attacks [31, 111]. Finally, it focuses on the detection of
groups of malicious websites, rather than individual websites,
thus leveraging the inherent relationships between phishing
websites and the additional information available in this way.
Again, focusing on group analyses instead of the classification
of individual entities is a promising direction of research in
several areas of cybersecurity [31]. Among the other advantages
of this work is the construction of large and detailed
dataset, which however has not been publicly released to the
scientific community.
To conclude our detailed analysis of proposed phishing
countermeasures, we discuss systems for defending against
smishing [82] and vishing [84] attacks. In detail, [82] proposed
the DSmishSMS system, targeted at the detection of smishing
SMSs. The system aimed to address some of the typical challenges
related to the task of smishing detection, including the
brevity of text messages which limits the number of available
features, and the scarcity of labeled datasets to use for training
a detector. To overcome these limitations, DSmishSMS only
leveraged 5 features extracted from the text of the SMSs,
including features aimed at encoding the authenticity of the
URLs contained in the analyzed SMSs. The classification was
obtained by leveraging an ANN trained with the backpropagation
algorithm, which achieved accuracy = 0.98.
Classifications from the ANN were also compared to those
obtained with traditional algorithms, such as random forest,
naïve Bayes and DT. The comparison showed that the ANN
beat competitors by a tiny margin, at the expense of a slightly
longer execution time. The RIVPAM system is instead aimed at
the detection of vishing attacks [84]. Specifically, RIVPAM
(Real‐Time Vishing Prediction and Awareness Model) was
designed to alert potential unwary vishing targets in real‐time,
during vishing attacks. The system uses a combination of
natural language processing and machine learning to analyze
conversations in real‐time and is capable of issuing warning
messages in case it detects a possible ongoing attack. The
classification is performed by leveraging algorithms such as
support vector machine, logistic regression and multilayer
perceptron, which analyze some simple linguistic features (e.g.,
n‐grams) extracted form the conversations. Vishing detection
results achieved by RIVPAM are rather low, with the best reported
accuracy = 0.65 on the test‐set. Similarly to other
surveyed systems that adopted shallow features and traditional
classification algorithms, better results are foreseeable by the
adoption of more advanced techniques for both the feature
extraction and the classification steps.
5 | DISCUSSION: CHALLENGES AND
FUTURE DIRECTIONS
Thoroughly investigating a problem represents the first step
for reaching a satisfactory solution. This simple consideration
and the relatively limited time passed since the start of the
pandemic motivate and explain the first finding of our
literature review. That is, the landscape of research on
COVID‐19 phishing attacks and their countermeasures is
made of a majority of studies aimed at investigating attacks,
with only a relative minority of works that proposed specific
solutions to them. The analysis of the literature that investigated
attacks revealed that scholars already explored
different directions of research and evaluated different aspects
of the attacks. For instance, while some papers pro-
vided a general (i.e., horizontal) overview of the cyber‐attacks
that occurred during COVID‐19, out of which phishing
represents the utmost example, others carried out more
constrained yet detailed (i.e., vertical) analyses of specific issues.
Among them are papers that investigated (i) the causes
of vulnerability to phishing attacks during COVID‐19 [7, 10],
(ii) the rise of malicious domain registrations [34, 49], (iii) the
economic impact of phishing attacks [7], (iv) the responses
enacted by some countries to fight the rampaging COVID‐19
scamdemic [64–66], (v) the peculiar cybersecurity challenges
340 - AL‐QAHTANI AND CRESCI
#
#faced by the healthcare sector [10, 20, 22, 25], and more. As
such, the body of research on covid‐related phishing attacks
appears to be diversified, dense and overall already mature.
On the contrary, our detailed analysis of the proposed
countermeasures to such attacks revealed a number of challenges
and drawbacks.
5.1 | Current challenges
In Section 4 we identified a lack of reference datasets and we
highlighted that the majority of proposed COVID‐19 phishing
detectors are based on simple and traditional classification algorithms
and on small sets of shallow features. The first issue –
that is, the limited availability of reference datasets – can be
traced back to a combination of long‐known and covid‐related
challenges. Firstly, building high‐quality scientific datasets have
always represented a very demanding task [98]. In addition, the
impact and the recency of the pandemic left even less time and
resources for scholars to tackle this task. As such, a general lack
of extensive, high‐quality data on the novel problem of covid‐
related phishing attacks is somehow expected at this stage.
Nonetheless, this is causing several problems to the scholars
working in this field. One general problem is that this lack of
resources inevitably hinders the research on covid‐related
cyber‐attacks. Moreover, another problem is related to the
capability of training and evaluating automatic systems for
phishing detection. In particular, the current situation where
each detector was evaluated on a different dataset, many of
which are small and not publicly available, inevitably raises
concerns about the validity and generality of the evaluations
reported in the existing papers.
The second issue unveiled by our analysis is related to the
use of traditional (i.e., not state‐of‐the‐art) machine learning
algorithms and of shallow features. As shown, the majority of
proposed phishing detectors was based on classification algorithms
such as decision trees, random forests and support
vector machines, instead of more recent and better performing
solutions, such as those based on deep learning [112]. The
same considerations can be made for the choice of machine
learning features, which is not on par with current state‐of‐the‐
art solutions [113]. Notably, the issue with the choice of algorithms
and features strictly depends on the lack of reference
datasets. This is particularly true for the possible application of
deep learning to the task of phishing detection, for which large
datasets are needed in order to train and optimize deep neural
networks that easily involve millions of parameters [114].
5.2 | Future directions of research
As anticipated, the body of research on covid‐related phishing
attacks is overall mature. However, some specific areas could
nonetheless benefit from additional research. One of such
areas is that related to the quantification of the effects (or
impact) of the attacks. This task has been mostly left to
cybersecurity firms and governmental agencies, but it could
instead see a deeper academic involvement. Notably, measuring
the effects of cyber‐attacks currently represents an open and
promising research direction that goes beyond phishing
and COVID‐19. In fact, quantifying effects is meaningful and
needed in all those areas of cybersecurity that deal with relatively
new types of attacks (e.g., fake news and all forms of
online information manipulation [31]) and countermeasures
[115]. Here, a better assessment and quantification of the
consequences of phishing attacks during a major crisis could
inform decisions for a broad array of stakeholders, including
policymakers, law enforcement personnel, as well as all those
scholars and practitioners actively involved in developing
effective countermeasures.
Since each challenge comes with opportunities, the area
related to the development of countermeasures to COVID‐19
phishing attacks is the one that currently presents the majority
of opportunities for future research. For example, the aforementioned
lack of reference datasets for training and validating
detectors, mandates additional work in this important direction.
In fact, works aimed at collecting, developing and sharing
scientific resources – including datasets, but also tools and
software as well as benchmark platforms/frameworks – are
much needed and are likely to have a strong impact in the
scientific community. As such, this scientific endeavour represents
a low‐hanging fruit. Then, with more and better data it
is foreseeable that more sophisticated and powerful detectors
will be developed. In other words, we envision that the greater
availability of resources will bootstrap the next wave of
research on covid‐related cyber‐attacks, including the experimentation
with those algorithms and techniques whose appli-
cation was daunting or infeasible until now. Notably, not only
does this direction of research involve new experimentation
with deep learning‐based methods for feature extraction and
attack detection, but it also opens up the possibility to experiment
with feature selection techniques [116] and with tech-
niques for combining simple classifiers, such as ensemble
methods [3]. All these techniques have seen very limited
application until now, because of the limitations that we previously
discussed. However, they have already proven their
efficacy in related tasks and are thus likely to provide favourable
results also for the detection of COVID‐19 phishing
attacks.
Another challenge that we highlighted in the previous
section is the difficulty at assessing the validity of the experimental
results of phishing detectors. To this regard, another
much needed direction of research is the one related to the
development of systematic evaluation campaigns of the existing
detectors. As it typically happens with many detection tasks
[31], the majority of efforts are devoted to developing new
detectors and only a small minority of works focus on evaluating
and comparing the different detectors. With the foreseen
increase in the development of state‐of‐the‐art phishing detectors,
the latter task will become even more important.
Systematic evaluations of the existing detectors should not only
involve comparisons between the detectors, but should also
include experiments aimed at evaluating the generalizability of
the different detectors – that is, their capacity to detect attacks
AL‐QAHTANI AND CRESCI - 341
#
#for which they were not trained. The latter test in particular has
proven valuable in other tasks for identifying detectors'
generalization deficiencies and for estimating their capacity to
thwart future and unforeseen attacks [111].
5.3 | Final remarks
COVID‐19 has been one of the deadliest pandemics in the
history of humanity and the first to occur in a massively
digitized and hyperconnected world. Withstanding its spread
and impact required drastic changes that gave rise to a plethora
of problems. One of such problems – phishing attacks – has
been the subject of this survey.
The long‐term effects of the pandemic on our society
are still unclear. However, it is already evident that some
changes are bound to stay. As an example, the sudden shift
to remote work represented a unique opportunity to
reimagine and reorganise businesses, jobs and work habits.
The world after COVID‐19 will never be the same. Moreover,
more and worse pandemics are expected to strike in
the coming years [117].
What all of this means is that at least some of the problems
that we faced during COVID‐19 will remain for a long time
and will probably reappear and intensify over and over again.
Gunther Eysenbach – the father of infodemiology – stressed in
2009 the need to ‘build tools now to manage future infodemics’
[118]. In retrospect, we clearly see that his warning call
went unheeded [119]. For all of these reasons, it is of the
utmost importance to capitalize on the lessons learnt in this
pandemic, for such experiences will be decisive to withstand
the future infodemics and scamdemics.
6 | CONCLUSIONS
In this survey we focussed on the most frequent type of cyber‐
attack perpetrated during the COVID‐19 pandemic: phishing.
We systematically analysed and discussed both scientific
studies, as well as reports by cybersecurity firms and governmental
agencies that investigated phishing attacks or that
proposed solutions against them.
Our analysis highlighted that many works investigated the
drivers and the characteristics of phishing attacks. Instead, only
a minority of scholars worked to build and share resources for
the community (e.g., reference datasets) and to propose specific
solutions against phishing. Moreover, the existing solu-
tions are mostly based on traditional machine learning
techniques, thus largely overlooking the state‐of‐the‐art
methods for both the classification and feature extraction
steps.
Given this picture, the most favourable directions for
future research and experimentation revolve around building
and sharing resources to the community, such as large datasets
and evaluation campaigns. Once more resources will be
available, efforts should be directed towards applying state‐
of‐the‐art techniques, such as those based on deep learning,
for the task of phishing detection. The lessons learnt from
contrasting phishing and other cyber‐attacks during the
COVID‐19 pandemic will be valuable for responding to the
increasing cybersecurity concerns that are rising with each
passing year.
ACKNOWLEDGEMENTS
The publication of this article was funded by the Qatar
National Library (QNL), Doha, Qatar and partially supported
by award QRLP10‐G‐1803022 from the Qatar National
Research Fund (QNRF), member of Qatar Foundation.
CONFLICT OF INTEREST
The authors declare that have no conflicts of interest.
DATA AVAILABILITY STATEMENT
Data sharing is not applicable to this article as no new data
were created or analyzed in this study.
ORCID
Stefano Cresci https://orcid.org/0000-0003-0170-2445
REFERENCES
1. Hijji, M., Alam, G.: A multivocal literature review on growing social
engineering based cyber‐attacks/threats during the COVID‐19
pandemic: challenges and prospective solutions. IEEE Access. 9,
7152–7169 (2021). https://doi.org/10.1109/access.2020.3048839
2. Di Pietro, R., et al.: New dimensions of information warfare. Adv Inf
Sec, vol. 84. (2021)
3. Valiyaveedu, N., et al.: Survey and analysis on AI based phishing
detection techniques. The 2021 International Conference on Communication,
Control and Information Sciences (ICCISC’21), vol. 1,
pp. 1–6. IEEE (2021)
4. Zarocostas, J.: How to fight an infodemic. Lancet. 395(10225), 676
(2020). https://doi.org/10.1016/s0140‐6736(20)30461‐x
5. Ferrara, E., Cresci, S., Luceri, L.: Misinformation, manipulation, and
abuse on social media in the era of COVID‐19. J Comput Soc Sci. 3(2),
271–277 (2020). https://doi.org/10.1007/s42001‐020‐00094‐5
6. Szurdi, J., et al.: Studying how cybercriminals prey on the COVID‐19
pandemic. Unit42 (2020)
7. Lallie, H.S., et al.: Cyber security in the age of COVID‐19: a timeline
and analysis of cyber‐crime and cyber‐attacks during the pandemic.
Comput. Secur. 105, 102248 (2021). https://doi.org/10.1016/j.cose.
2021.102248
8. Jalali, M.S., et al.: Why employees (still) click on phishing links: investigation
in hospitals. J. Med. Internet Res. 22(1), e16775 (2020). https://
doi.org/10.2196/16775
9. Schneck, P.A.: Cybersecurity during COVID‐19. IEEE Ann. Hist. Comput.
18(06), 4–5 (2020). https://doi.org/10.1109/msec.2020.3019678
10. He, Y., et al.: Health care cybersecurity challenges and solutions under
the climate of COVID‐19: scoping review. J. Med. Internet Res. 23(4),
e21747 (2021). https://doi.org/10.2196/21747
11. United Nations Department of Global Communications: UN
Tackles infodemic of Misinformation and Cybercrime in COVID‐19
Crisis (2020). https://www.un.org/en/un‐coronavirus‐communications‐
team/un‐tackling‐‘infodemic’‐misinformation‐and‐cybercrime‐covid‐19.
Accessed 31 May 2022
12. Basit, A., et al.: A comprehensive survey of AI‐enabled phishing attacks
detection techniques. Telecommun. Syst. 76(1), 139–154 (2021).
https://doi.org/10.1007/s11235‐020‐00733‐2
13. Salloum, S., et al.: Phishing email detection using natural language
processing techniques: a literature survey. Procedia Comput. Sci. 189,
19–28 (2021). https://doi.org/10.1016/j.procs.2021.05.077
342 - AL‐QAHTANI AND CRESCI
#
#14. Alkhalil, Z., et al.: Phishing attacks: recent comprehensive study and a
new anatomy. Front. Comput. Sci. 3, 6 (2021). https://doi.org/10.
3389/fcomp.2021.563060
15. Hakak, S., et al.: Have you been a victim of COVID‐19‐related cyber
incidents? Survey, taxonomy, and mitigation strategies. IEEE Access. 8,
124134–124144 (2020). https://doi.org/10.1109/access.2020.3006172
16. Korkmaz, M., Sahingoz, O.K., Diri, B.: Feature selections for the
classification of webpages to detect phishing attacks: a survey. In: The
2nd International Congress on Human‐Computer Interaction, Optimization
and Robotic Applications (HORA’20), pp. 1–9. IEEE (2020)
17. Ghafir, I., Prenosil, V.: Advanced persistent threat attack detection: an
overview. Int J Adv Comput Networks Secur. 4(4), 5054 (2014)
18. Ahmad, A., et al.: Strategically‐motivated advanced persistent threat:
definition, process, tactics and a disinformation model of counterattack.
Comput. Secur. 86, 402–418 (2019). https://doi.org/10.1016/j.cose.
2019.07.001
19. Kumaran, N., Lugani, S.: Protecting Businesses against Cyber Threats
during COVID‐19 and beyond, Google Cloud ‐ Identity and Security
(2020)
20. Williams, C.M., Chaturvedi, R., Chakravarthy, K.: Cybersecurity risks in
a pandemic. J. Med. Internet Res. 22(9), e23692 (2020). https://doi.org/
10.2196/23692
21. Boddy, A., et al.: A study into data analysis and visualisation to increase
the cyber‐resilience of healthcare infrastructures. In: The 1st International
Conference on Internet of Things and Machine Learning
(IML’17), pp. 1–7. ACM (2017)
22. Offner, K., et al.: Towards understanding cybersecurity capability in
Australian healthcare organisations: a systematic review of recent
trends, threats and mitigation. Intell. Natl. Secur. 35(4), 556–585 (2020).
https://doi.org/10.1080/02684527.2020.1752459
23. Ronquillo, J.G., et al.: Health IT, hacking, and cybersecurity: national
trends in data breaches of protected health information. JAMIA Open.
1(1), 15–19 (2018). https://doi.org/10.1093/jamiaopen/ooy019
24. Weil, T., Murugesan, S.: IT risk and resilience—cybersecurity response
to COVID‐19. IT Prof. 22(3), 4–10 (2020). https://doi.org/10.1109/
mitp.2020.2988330
25. Sardi, A., et al.: Cyber risk in health facilities: a systematic literature
review. Sustainability. 12(17), 7002 (2020). https://doi.org/10.3390/
su12177002
26. Kim, D.‐w., Choi, J.‐y., Han, K.‐h.: Risk management‐based security
evaluation model for telemedicine systems. BMC Med. Inf. Decis.
Making. 20(1), 1–14 (2020). https://doi.org/10.1186/s12911‐020‐
01145‐7
27. Naidoo, R.: A multi‐level influence model of COVID‐19 themed
cybercrime. Eur. J. Inf. Syst. 29(3), 306–321 (2020). https://doi.org/10.
1080/0960085x.2020.1771222
28. Da San Martino, G., et al.: A survey on computational propaganda
detection. In: The 29th International Joint Conference on Artificial
Intelligence, pp. 4826–4832. IJCAI’20 (2020)
29. Iuga, C., Nurse, J.R., Erola, A.: Baiting the hook: factors impacting
susceptibility to phishing attacks. Human‐centric Comput Inf Sci. 6(1),
1–20 (2016). https://doi.org/10.1186/s13673‐016‐0065‐2
30. MalwareBytes, Cybercriminals impersonate World Health Organization
to Distribute Fake Coronavirus E‐Book (2020). https://blog.
malwarebytes.com/social‐engineering/2020/03/cybercriminals‐imperso
nate‐world‐health‐organization‐to‐distribute‐fake‐coronavirus‐e‐book/.
Accessed 31 May 2022
31. Cresci, S.: A decade of social bot detection. Commun. ACM. 63(10),
72–83 (2020). https://doi.org/10.1145/3409116
32. The Daily Mail: Cyber criminals create a spoof copy of the NHS
website in the midst of the coronavirus pandemic to trick users into
downloading dangerous malware that can steal their passwords and
credit card data. https://www.dailymail.co.uk/sciencetech/article‐
8250737/Kaspersky‐detects‐fake‐NHS‐site‐steals‐credit‐card‐data.html
(2020). Accessed 31 May 2022
33. Krebs on Security.: Live coronavirus map used to spread malware.
https://krebsonsecurity.com/2020/03/live‐coronavirus‐map‐used‐to‐
spread‐malware/ (2020). Accessed 31 May 2022
34. Xia, P., et al.: Identifying and characterizing COVID‐19 themed malicious
domain campaigns. In: The 11th ACM Conference on Data and
Application Security and Privacy, pp. 209–220. CODASPY’21 (2021)
35. O’Donnell, L.: Skype Phishing Attack Targets Remote Workers’ Passwords
(2020). https://threatpost.com/skype‐phishing‐attack‐targets‐
remote‐workers‐passwords/155068/. Accessed 31 May 2022
36. Rodger, J.: The school meals coronavirus text scam which could trick
parents out of thousands. https://www.birminghammail.co.uk/news/
midlands‐news/school‐meals‐coronavirus‐text‐scam‐17975311 (2020).
Accessed 31 May 2022
37. Pilkey, A.: Coronavirus Email Attacks Evolving as Outbreak Spreads
(2020). https://blog.f‐secure.com/coronavirus‐email‐attacks‐evolving‐
as‐outbreak‐spreads/. Accessed 31 May 2022
38. Patranobis, S.: Indian hackers targeting Chinese medical institutes amid
coronavirus outbreak, says report (2020). https://www.hindustantimes.
com/world‐news/indian‐hackers‐targetting‐chinese‐medical‐institutes‐
amid‐coronavirus‐outbreak‐says‐report/story‐piDHQeY4UfTVy8BWa2
GG3O.html. Accessed 31 May 2022
39. Vergelis, M.: Phishers Are Using the Wuhan Coronavirus as Bait, Trying
to Hook E‐Mail Credentials (2020). https://www.kaspersky.com/blog/
coronavirus‐phishing/32395/. Accessed 31 May 2022
40. Walter, J.: Threat Intel Cyber Attacks Leveraging the COVID‐
19/coronavirus Pandemic (2020). https://www.sentinelone.com/labs/
threat‐intel‐cyber‐attacks‐leveraging‐the‐covid‐19‐coronavirus‐pandemic/.
Accessed 31 May 2022
41. Doffman, Z.: Chinese Hackers ‘weaponize’ Coronavirus Data for New
Cyber Attack: Here’s what They Did (2020). https://www.forbes.com/
sites/zakdoffman/2020/03/12/chinese‐hackers‐weaponized‐coronavirus‐
data‐to‐launch‐this‐new‐cyber‐attack. Accessed 31 May 2022
42. Henderson, S., et al.: Vietnamese Threat Actors APT32 Targeting
Wuhan Government and Chinese Ministry of Emergency Management
in Latest Example of COVID‐19 Related Espionage (2020). https://
www.mandiant.com/resources/apt32‐targeting‐chinese‐government‐in‐
covid‐19‐related‐espionage. Accessed 31 May 2022
43. Del Rosso, K.: New Threat Discovery Shows Commercial Surveillanceware
Operators Latest to Exploit COVID‐19 (2020). https://www.
lookout.com/blog/commercial‐surveillanceware‐operators‐latest‐to‐take‐
advantage‐of‐covid‐19. Accessed 31 May 2022
44. Greig, J.: Global Shipping Industry Attacked by Coronavirus‐Themed
Malware (2020). https://www.techrepublic.com/article/global‐shipping‐
industry‐attacked‐by‐coronavirus‐themed‐malware/. Accessed 31 May
2022
45. Smithers, R.: Fraudsters Use Bogus NHS Contacttracing App in
Phishing Scam (2020). https://www.theguardian.com/world/2020/
may/13/fraudsters‐use‐bogus‐nhs‐contact‐tracing‐app‐in‐phishing‐scam.
Accessed 31 May 2022
46. Nurse, J.R.: Cybercrime and you: how criminals attack and the human
factors that they seek to exploit. In: The Oxford Handbook of
Cyberpsychology, pp. 663–690. Oxford University Press (2019)
47. Check Point: Coronavirus Cyber‐Attacks Update: Beware of the Phish
(2020). https://blog.checkpoint.com/2020/05/12/coronavirus‐cyber‐
attacks‐update‐beware‐of‐the‐phish/. Accessed 31 May 2022
48. Williams, E.J., Polage, D.: How persuasive is phishing email? The role of
authentic design, influence and current events in email judgements.
Behav. Inf. Technol. 38(2), 184–197 (2019). https://doi.org/10.1080/
0144929x.2018.1519599
49. Pletinckx, S., et al.: Cash for the register? Capturing rationales of early
COVID‐19 domain registrations at Internet‐scale. In: The 12th International
Conference on Information and Communication Systems
(ICICS’21), pp. 41–48. IEEE (2021)
50. Krebs on Security: Sipping from the Coronavirus Domain Firehose
(2020). https://krebsonsecurity.com/2020/04/sipping‐from‐the‐corona
virus‐domain‐firehose/. Accessed 31 May 2022
51. BAE Systems: The COVID Crime Index 2021 (2021). https://www.
baesystems.com/en‐financialservices/insights/the‐covid‐crime‐index.
Accessed 31 May 2022
52. Pritom, M.M.A., et al.: Characterizing the landscape of COVID‐19
themed cyberattacks and defenses. In: The 18th IEEE International
AL‐QAHTANI AND CRESCI - 343
#
#Conference on Intelligence and Security Informatics (ISI’20), pp. 1–6.
IEEE (2020)
53. Pols, P.: Tech. rep. In: The Unified Kill Chain. Leiden University (2017)
54. Eian, I.C., et al.: Cyber Attacks in the Era of COVID‐19 and Possible
Solution DomainsPreprints (2020). https://doi.org/10.20944/preprints
202009.0630.v1
55. Plachkinova, M.: Exploring the shift from physical to cybercrime at the
onset of the COVID‐19 pandemic. Int J Cyber Forensics Adv Threat
Invest. 2(1), 50–62 (2021). https://doi.org/10.46386/ijcfati.v2i1.29
56. Khan, N.A., Brohi, S.N., Zaman, N.: Ten Deadly Cyber Security Threats
amid COVID‐19 Pandemic. TechRxiv (2020). https://doi.org/10.
36227/techrxiv.12278792.v1
57. Parani, S., Raikwar, M.: A study on phishing attack during the COVID‐
19 lockdown. Int. J. Comput. Appl. 174(21), 50–54 (2021). https://doi.
org/10.5120/ijca2021921086
58. Malecki, F.: Overcoming the security risks of remote working. Comput.
Fraud Secur. (7), 10–12 (2020). https://doi.org/10.1016/s1361‐3723
(20)30074‐9
59. Sarginson, N.: Securing your remote workforce against new phishing
attacks. Comput. Fraud Secur. 2020(9), 9–12 (2020). https://doi.org/10.
1016/s1361‐3723(20)30096‐8
60. Ahmad, T.: Corona Virus (COVID‐19) Pandemic and Work from
Home: Challenges of Cybercrimes and Cybersecurity
61. Pranggono, B., Arabo, A.: COVID‐19 pandemic cybersecurity issues.
Internet Technol Lett. 4(2), e247 (2021). https://doi.org/10.1002/
itl2.247
62. Başeskioğlu, M.Ö., Tepecik, A.: Cybersecurity, computer networks
phishing, malware, ransomware, and social engineering anti‐piracy reviews.
In: The 3rd International Congress on Human‐Computer
Interaction, Optimization and Robotic Applications (HORA’21),
pp. 1–5. IEEE (2021)
63. Mathew, A.R.: Cybersecurity pros warn–COVID‐19 pandemic as a tool.
Int. J. Eng. Adv. Technol. 9(4), 2441–2443. https://doi.org/10.35940/
ijeat.d8305.049420
64. Furnell, S., Shah, J.N.: Home working and cyber security–an outbreak of
unpreparedness. Comput. Fraud Secur. 2020(8), 6–12 (2020). https://
doi.org/10.1016/s1361‐3723(20)30084‐1
65. Škiljić, A.: Cybersecurity and remote working: Croatia’s (non‐) response
to increased cyber threats. Int Cybersecur Law Rev. 1(1), 51–61 (2020).
https://doi.org/10.1365/s43439‐020‐00014‐3
66. Amarullah, A.H., Runturambi, A.J.S., Widiawan, B.: Analyzing cyber
crimes during COVID‐19 time in Indonesia. In: The 3rd International
Conference on Computer Communication and the Internet (ICCCI’21),
pp. 78–83. IEEE (2021)
67. Chapman, P.: Are your IT staff ready for the pandemic‐driven insider
threat. Netw. Secur. (4), 8–11 (2020). https://doi.org/10.1016/s1353‐
4858(20)30042‐8
68. Bitaab, M., et al.: Scam pandemic: how attackers exploit public fear
through phishing. In: The 2020 Symposium on Electronic Crime
Research (eCrime’20), pp. 1–10. APWG (2020)
69. Akdemir, N., Yenal, S.: How phishers exploit the coronavirus pandemic:
a content analysis of COVID‐19 themed phishing emails. Sage Open.
11(3), 21582440211031879 (2021). https://doi.org/10.1177/215824402
11031879
70. Wang, L., et al.: Beyond the virus: a first look at coronavirus‐themed
Android malware. Empir. Software Eng. 26(4), 1–38 (2021). https://
doi.org/10.1007/s10664‐021‐09974‐4
71. Xia, P., et al.: Don’t Fish in TroubledWaters! Characterizing Coronavirus‐
Themed Cryptocurrency Scams. arXiv preprint arXiv:2007.13639
72. Nizzoli, L., et al.: Charting the landscape of online cryptocurrency
manipulation. IEEE Access. 8, 113230–113245 (2020). https://doi.
org/10.1109/access.2020.3003370
73. Mihailović, A., et al.: COVID‐19 and beyond: employee perceptions of
the efficiency of teleworking and its cybersecurity implications. Sustainability.
13(12), 6750 (2021). https://doi.org/10.3390/su13126750
74. Pearl, J.: Causal inference in statistics: an overview. Stat. Surv. 3(none),
96–146 (2009). https://doi.org/10.1214/09‐ss057
75. Georgiadou, A., Mouzakitis, S., Askounis, D.: Working from home
during COVID‐19 crisis: a cyber security culture assessment survey.
Secur. J. 35(2), 1–20 (2021). https://doi.org/10.1057/s41284‐021‐
00286‐2
76. Turner, C., Turner, C.B., Shen, Y.: Cybersecurity concerns & teleworking
in the COVID‐19 era: a socio‐cybersecurity analysis of orga-
nizational behavior. J Adv Res Soc Sci. 3(2), 22–30 (2020). https://doi.
org/10.33422/jarss.v3i2.502
77. Schutt, R.K.: Investigating the Social World: The Process and Practice
of Research. SAGE publications (2018)
78. Baryshevtsev, M., McGlynn, J.: Persuasive appeals predict credibility
judgments of phishing messages. Cyberpsychol Behav. Soc. Netw. 23(5),
297–302 (2020). https://doi.org/10.1089/cyber.2019.0592
79. Hawdon, J., Parti, K., Dearden, T.E.: Cybercrime in America amid
COVID‐19: the initial results from a natural experiment. Am. J. Crim.
Justice. 45(4), 546–562 (2020). https://doi.org/10.1007/s12103‐020‐
09534‐4
80. Daengsi, T., et al.: A comparative study of cybersecurity awareness
on phishing among employees from different departments in an
organization. In: The 2nd International Conference on Smart
Computing and Electronic Enterprise (ICSCEE’21), pp. 102–106.
IEEE (2021)
81. Gordon,W.J., et al.: Evaluation of a mandatory phishing training program
for high‐risk employees at a US healthcare system. J. Am. Med. Inf.
Assoc. 26(6), 547–552 (2019). https://doi.org/10.1093/jamia/ocz005
82. Mishra, S., Soni, D.: DSmishSMS‐A system to detect smishing SMS.
Neural Comput. Appl., 1–18 (2021). https://doi.org/10.1007/s00521‐
021‐06305‐y
83. Almeida, T.A., Hidalgo, J.M.G., Yamakami, A.: Contributions to the
study of SMS spam filtering: new collection and results. In: The 11th
ACM Symposium on Document Engineering, pp. 259–262. DocEng’11
(2011)
84. Biswal, S.: Real‐time intelligent vishing prediction and awareness model
(RIVPAM). In: The 7th International Conference on Cyber Situational
Awareness, Data Analytics and Assessment (CyberSA’21), pp. 1–2.
IEEE (2021)
85. Jones, K.S., et al.: How social engineers use persuasion principles during
vishing attacks. Inf.Comput. Secur. 29(2), 314–331 (2020). https://doi.
org/10.1108/ics‐07‐2020‐0113
86. Wu, P., Guo, H., Holmes: An Efficient and Lightweight Semantic Based
Anomalous Email Detector. arXiv preprint arXiv:2104.08044
87. Sarma, D., et al.: Comparative analysis of machine learning algorithms
for phishing website detection. In: Inventive Computation and Information
Technologies, pp. 883–896. Springer (2021)
88. Mukhopadhyay, A., Prajwal, A.: Edith ‐ a robust framework for prevention
of cyber attacks in the COVID era. In: The 2nd International
Conference for Emerging Technology (INCET’21), pp. 1–8. IEEE
(2021)
89. Ispahany, J., Islam, R.: Detecting malicious COVID‐19 URLs using
machine learning techniques. In: The 19th IEEE International Conference
on Pervasive Computing and Communications Workshops
(PERCOM’21 Workshops), pp. 718–723. IEEE (2021)
90. Tawalbeh, L., et al.: Predicting and preventing cyber attacks during
COVID‐19 time using data analysis and proposed secure IoT layered
model. In: The 4th International Conference on Multimedia
Computing, Networking and Applications (MCNA’20), pp. 113–118.
IEEE (2020)
91. Saha, I., et al.: Phishing attacks detection using deep learning approach.
In: The 3rd International Conference on Smart Systems and Inventive
Technology (ICSSIT’20), pp. 1180–1185. IEEE (2020)
92. Basit, A., et al.: A novel ensemble machine learning method to detect
phishing attack. In: The 23rd International Multitopic Conference
(INMIC’20), pp. 1–5. IEEE (2020)
93. Pritom, M.M.A., et al.: Data‐driven characterization and detection of
COVID‐19 themed malicious websites. In: The 18th IEEE International
Conference on Intelligence and Security Informatics (ISI’20),
pp. 1–6. IEEE (2020)
344 - AL‐QAHTANI AND CRESCI
#
#94. Ahmad, H., Erdodi, L.: Overview of phishing landscape and homographs
in Arabic domain names. Secur.Priv. 4(4), e159 (2021). https://
doi.org/10.1002/spy2.159
95. Montañez, R., Golob, E., Xu, S.: Human cognition through the lens of
social engineering cyberattacks. Front. Psychol. 11, 1755 (2020).
https://doi.org/10.3389/fpsyg.2020.01755
96. Guitton, M.J.: Cyberpsychology research and COVID‐19. Comput.
Hum. Behav. 111, 106357 (2020). https://doi.org/10.1016/j.chb.2020.
106357
97. Cockburn, A., et al.: Threats of a replication crisis in empirical computer
science. Commun. ACM. 63(8), 70–79 (2020). https://doi.org/10.
1145/3360311
98. Assenmacher, D., et al.: Benchmarking crisis in social media analytics: a
solution for the data sharing problem. Soc. Sci. Comput. Rev. https://
doi.org/10.1177/08944393211012268
99. Domingos, P.: A few useful things to know about machine learning.
Commun. ACM. 55(10), 78–87 (2012). https://doi.org/10.1145/
2347736.2347755
100. Alam, F., et al.: A Survey on Multimodal Disinformation Detection.
arXiv preprint arXiv:2103.12541
101. Liu, X., et al.: Multi‐task deep neural networks for natural language
understanding. In: The 57th Annual Meeting of the Association for
Computational Linguistics, pp. 4487–4496. ACL’19 (2019)
102. Al‐Turkistani, H.F., Ali, H.: Enhancing users wireless network cyber
security and privacy concerns during COVID‐19. In: The 1st International
Conference on Artificial Intelligence and Data Analytics
(CAIDA’21), pp. 284–285. IEEE (2021)
103. Kott, A., Linkov, I.: To improve cyber resilience, measure it. Computer.
54(2), 80–85 (2021). https://doi.org/10.1109/mc.2020.3038411
104. Groenendaal, J., Helsloot, I.: Cyber resilience during the COVID‐19
pandemic crisis: a case study. J. Contingencies Crisis Manag. 29(4),
439–444 (2021). https://doi.org/10.1111/1468‐5973.12360
105. Hollnagel, E.: RAG – Resilience Analysis Grid, Introduction to the
Resilience Analysis Grid (RAG)
106. Cresci, S., et al.: From reaction to proaction: unexplored ways to the
detection of evolving spambots. In: The Web Conference 2018
(WWW’18 Companion), pp. 1469–1470 (2018)
107. Hyslip, T.S.: Cybercrime‐as‐a‐Service Operations, pp. 815–846. The
Palgrave Handbook of International Cybercrime and Cyberdeviance
(2020)
108. Arshey, M., Viji, K.A.: Thwarting cyber crime and phishing attacks with
machine learning: a study. In: The 7th International Conference on
Advanced Computing and Communication Systems (ICACCS’21), vol.
1, pp. 353–357. IEEE (2021)
109. Verma, R., Das, A.: What’s in a URL: fast feature extraction and malicious
URL detection. In: The 3rd ACM International Workshop on
Security and Privacy Analytics, pp. 55–63. IWSPA’17 (2017)
110. Patil, D.R., Patil, J.B.: Malicious URLs detection using decision tree
classifiers and majority voting technique. Cybern. Inf. Technol. 18(1),
11–29 (2018). https://doi.org/10.2478/cait‐2018‐0002
111. Echeverrìa, J., et al.: LOBO: evaluation of generalization deficiencies in
Twitter bot classifiers. In: The 34th Annual Computer Security Applications
Conference (ACSAC’18), pp. 137–146. ACM (2018)
112. Yi, P., et al.: Web phishing detection using a deep learning framework.
Wireless Commun. Mobile Comput., 1–9 (2018). https://doi.org/10.
1155/2018/4678746
113. Yang, P., Zhao, G., Zeng, P.: Phishing website detection based on
multidimensional features driven by deep learning. IEEE Access. 7,
15196–15209 (2019). https://doi.org/10.1109/access.2019.2892066
114. Sun, C., et al.: Revisiting unreasonable effectiveness of data in deep
learning era. In: The 15th IEEE International Conference on Computer
Vision, pp. 843–852 (2017)
115. Trujillo, A., Cresci, S.: Make Reddit Great Again: Assessing Community
Effects of Moderation Interventions on r/The_Donald. arXiv:
2201.06455
116. Gandotra, E., Gupta, D.: An efficient approach for phishing detection
using machine learning. In: Multimedia Security, pp. 239–253. Springer
(2021)
117. Intergovernmental science‐policy platform on biodiversity and
ecosystem services, IPBES workshop report on biodiversity and pandemics.
Tech. Rep. (2020)
118. Eysenbach, G.: Infodemiology and infoveillance: framework for an
emerging set of public health informatics methods to analyze search,
communication and publication behavior on the internet. J. Med.
Internet Res. 11(1), e1157 (2009). https://doi.org/10.2196/jmir.1157
119. Cinelli, M., et al.: The COVID‐19 social media infodemic. Sci. Rep.
10(1), 1–10 (2020)
How to cite this article: Al‐Qahtani, A.F., Cresci, S.:
The COVID‐19 scamdemic: a survey of phishing
attacks and their countermeasures during COVID‐19.
IET Inf. Secur. 16(5), 324–345 (2022). https://doi.org/
10.1049/ise2.12073
AL‐QAHTANI AND CRESCI - 345
COVID-19-related online
misinformation in Bangladesh
Md. Sayeed Al-Zaman
Journalism and Media Studies, Jahangirnagar University, Dhaka, Bangladesh
Abstract
Purpose –This paper aims to understand the popular themes of coronavirus disease 2019 (COVID-19)-related
online misinformation in Bangladesh and to provide some suggestions to abate the problem.
Design/methodology/approach – This paper discusses online COVID-19-related misinformation in
Bangladesh. Following thematic analyses, the paper discusses some dominant misinformation themes based
on the data collected from three fact-checkingwebsites of Bangladesh run bymedia professionals and scholars.
Findings – COVID-19-related online misinformation in Bangladesh has six popular themes: health, political,
religious, crime, entertainment and miscellaneous. To curb misinformation, many initiatives have been taken
so far that have produced little success. This paper briefly proposes the implementation of an experimental
two-way misinformation prevention technique for a better result.
Originality/value – Acknowledging previous initiatives, this paper discusses the major themes and offers
additional solutions to reduce online misinformation which would benefit academics as well as policymakers.
Keywords Misinformation, COVID-19, Public health communication, Bangladesh
Paper type Commentary
Introduction
“Highly concentrated alcohol could disinfect the body and kill the virus.” This single piece of
misinformation claimed at least 800 livesworldwide. Additionally, 5,876 peoplewere admitted
to hospital and 60 more developed complete blindness caused by another source of
misinformation related to a cure for coronavirus disease 2019 (COVID-19) [1, 2]. Both incidents
hint at how misinformation can make public health conditions more critical during the
pandemic. The health-care system in Bangladesh has been suffering from inadequatemedical
facilities and health-care corruption for the last few years, and the COVID-19 pandemic has
exacerbated this problem [3–6]. Increasing pandemicmisinformation also impedes health-care
services, producing anxiety and misleading the public. Bangladesh is a country with 0.1 bn
Internet users, and a large share of them has a lack of proper digital literacy [7]. Further, like
many other South Asian countries, the netizens of Bangladesh rely on the Internet for healthrelated
information during the pandemic [8, 9]. Therefore, COVID-19 misinformation reached
a greater audience, making the COVID-19 situation more critical. Misinformation,
disinformation, rumor and fake news are used interchangeably in scientific literature due
to their conceptual proximity [10–12] but result in the same negative consequences.
In this viewpoint piece, following a thematic analysis, the author discusses a fewdominant
themes of COVID-19 misinformation, drawing from the data of BD FactCheck, Rumor
Scanner and Jachai, three leading Bangladeshi fact-checking websites.
Themes of online misinformation in Bangladesh
Theme identification of misinformation is difficult because topics of public interest may vary
from region to region, causing a generalization problem. For example, religious
JHR
35,4
364
© Md. Sayeed Al-Zaman. Published in Journal of Health Research. Published by Emerald Publishing
Limited. This article is published under the Creative Commons Attribution (CC BY 4.0) licence. Anyone
may reproduce, distribute, translate and create derivative works of this article (for both commercial and
non-commercial purposes), subject to full attribution to the original publication and authors. The full
terms of this licence may be seen at http://creativecommons.org/licences/by/4.0/legalcode
The current issue and full text archive of this journal is available on Emerald Insight at:
https://www.emerald.com/insight/2586-940X.htm
Received 22 September 2020
Revised 14 December 2020
Accepted 28 December 2020
Journal of Health Research
Vol. 35 No. 4, 2021
pp. 364-368
Emerald Publishing Limited
e-ISSN: 2586-940X
p-ISSN: 0857-4421
DOI 10.1108/JHR-09-2020-0414
#
#misinformation in India is more common during the pandemic [13], whereas medicine- and
policy-relatedmisinformation ismore common inmany other countries around theworld [14].
Popular online misinformation in Bangladesh is mostly native, produced by locals. Based on
the relevant misinformation data from three fact-checking websites, a thematic analysis of
popular COVID-19 misinformation produced six dominant themes: health, political, religious,
crime, entertainment and miscellaneous (Table 1). Health-related misinformation deals with
medicinal and health-care issues, such as fake prescriptions, misleading measures and
pandemic denial. Of them, fake medicine can cause health hazards, while pandemic denial
hinders proper protective measures against viral transmission. Political misinformation
mainly deals with political figures and events that are somehow related to the pandemic, such
as a political leader’s fake speech about COVID-19. Religious misinformation
instrumentalizes people’s faith and sentiment, often to serve political purposes. The first
COVID-19-related online misinformation in Bangladesh was a religious one that claimed that
eating Thankuni (Indian pennywort) three times a day uttering Bismillah (in the name of
Allah) would be a protection from COVID-19 infection [15]. This misinformation went viral
instantly through Facebook and YouTube, and many believers started searching their
surroundings for the mentioned plant. Addressing COVID-19 as a curse of Allah, a few
Islamic clerics also incited Muslim devotees to protest the shutdown of mosques in March
Themes and claims of misinformation
Health
“Kalazira (fennel flower) prevents coronavirus”
“Hot water mixed with salt-vinegar can cure corona infection”
“A medical team of 37 Chinese has arrived in Bangladesh”
“Coronavirus is a myth mainly to control people”
“Drink boiling water to prevent corona infection”
Political
“Italy’s PM says: ‘The only solution of the pandemic is in the sky’”
“Putin deploys 800 tigers and lions in the street of Russia to force people to stay at home”
“86 countries will file a case against China for spreading coronavirus”
“Rizvi dreamt that coronavirus will vanish from Bangladesh if Khaleda Zia is released”
Religious
“Quarantine was first invented by Mohammad (SM): US researcher”
“Saudi government canceled the Hajj this year due to the pandemic”
“Two million Chinese people accepted Islam in six months because of coronavirus”
“‘Allahu Akbar’ is written on the roads of Europe to avoid corona”
“Coronavirus is a curse of Allah for the infidels and real Muslims will be exempted from it”
“Life and death, both are in the hands of Allah, coronavirus can do nothing about it”
Crime
“More than 20 million people are missing in China during the pandemic”
“COVID-19 positive dead bodies are found in the street of Italy”
“Local public vandalized a Saudi immigrant’s house suspecting corona positive”
Entertainment
“Due to COVID-19 Netflix will give out three months free subscription”
“COVID-19 positive vs COVID-19 negative football match in Mymensingh”
Miscellaneous
“Italy’s doctor couple kiss in the public place during the pandemic”
“Bank and NGO’s three-month loan installment is deferred for COVID-19”
Note(s): The claims of misinformation were collected from three Bangladesh-based fact-checking websites:
BD Fact-check (http://bdfactcheck.com), Jachai (http://jachai.org) and Rumor Scanner (http://rumorscanner.
com) and translated from Bangla to English afterward
Table 1.
Major themes of
COVID-19-related
online misinformation
in Bangladesh
COVID-19related
online
misinformation
365
#
#2020 [16]. Crime misinformation mainly circulates false claims related to COVID-19 death
tolls, health-care corruption and lockdown crimes. Fake high death tolls may create panic
among the people. Entertainment misinformation refers to fake events and celebrities, and
miscellaneous misinformation refers to business, consumerism and other issues. As the
COVID-19 pandemic is accompanied by an infodemic, it is normal for netizens to be
bombarded and deceived by unverified as well as false information. In such cases, it is
important to devise appropriate solutions to this problem.
Online misinformation prevention strategies
Previous research suggests various online misinformation prevention techniques, such as
preparing myth busters, digital surveillance, reliable information circulation, medical
information dissemination, accuracy nudges and the use of artificial intelligence [17–21].
Furthermore, strict implementation of the law brings success in some countries [22]. In
Bangladesh, law enforcers have been arresting online rumor spreaders throughout the
pandemic but with little success [23]. Among other governmental and voluntary initiatives to
curb online misinformation, some include creating COVID-19 information hubs like National
Corona Portal, corona chatbots like COVID-response Bangladesh, mobile applications like
Corona Tracer BD and fact checkers like Rumor Scanner [6, 24]. Despite their potential to
reduce misinformation, these initiatives nevertheless struggled to effectively contain online
misinformation. As a result, COVID-19 misinformation is still a routine phenomenon in
Bangladesh [25, 26] that remains unacknowledged in many previous studies [6, 24].
Observing this, the government now plans to launch a countrywide anti-misinformation
campaign Asol Chini (Let’s Know the Truth) to counter the ongoing monsoon of COVID-19
misinformation [25]. It also requested Facebook to close downmore than a hundred Facebook
pages that were actively spreading COVID-19 misinformation [27]. Along with previous and
upcoming measures, a two-way misinformation prevention technique was also thought to
lead to a better result. In this process, sources of misinformation were first identified with
their production and dissemination processes and channels. Second, curtailing this
misinformation would create an information vacuum that had to be filled by verified
information, otherwise unreliable information would replace the vacuum once again.
To identify both misinformation sources and information vacuum, a form of surveillance
over social media platforms may be needed, as suggested in previous studies [18, 21]. This
paper provides a brief overview of misinformation themes, issues of public interest and
examples of prevalent misinformation in Bangladesh, which may be a guide in this process.
In this case, however, the notion of free speech and the necessity for surveillance may be
viewed as contradictory. For example, to what extent can the surveillance be enacted? Who
should have the right to patrol online media? What would be the modus operandi of online
surveillance as well as its probable consequences? Contentions between ideals would linger
on. Many may argue that the “Digital Security Act 2018” has already curbed the online
community’s freedom of speech to a certain extent [28]. Additionally, a few recent and
controversial arrests in the name of rumor prevention [29, 30] could lead to distrust of the
government’s control over online communication.
Conclusion
With the increase in the numbers of netizens, onlinemisinformation becomes commonplace in
Bangladesh, having a potential threat to public health communication during the COVID-19
pandemic. Although online misinformation is a global phenomenon, its impact could be more
intense in countries like Bangladesh that have comparatively less efficient communication
infrastructures and/or more digital illiterate netizens [7]. Therefore, controlling
misinformation production and simultaneously supplying a reliable information flow
would help to effectively reduce the extent ofmisinformation during the COVID-19 pandemic.
JHR
35,4
366
#
#References
1. Coleman A. Hundreds dead’ because of Covid-19 misinformation. BBC NewsWorld. [Internet].
2020 Aug 12 [cited 2020 Aug 14]. Available at: https://www.bbc.com/news/world-53755067.
2. Islam MS, Sarkar T, Khan SH, Mostofa Kamal AH, Hasan SMM, Kabir A, et al. COVID-19-related
infodemic and its impact on public health: a global social media analysis. Am J Trop Med Hyg.
2020 Oct; 103(4): 1621-9. doi: 10.4269/ajtmh.20-0812.
3. Andaleeb SS, Siddiqui N, Khandakar S. Patient satisfaction with health services in Bangladesh.
Health Policy Plan. 2007 Jul; 22(4): 263-73. doi: 10.1093/heapol/czm017.
4. Al-Zaman MS. Healthcare crisis in Bangladesh during the COVID-19 pandemic. Am J Trop Med
Hyg. 2020; 103(4): 1357-9. doi: 10.4269/ajtmh.20-0826.
5. Ahmed S, Tarique KM, Arif I. Service quality, patient satisfaction and loyalty in the Bangladesh
healthcare sector. Int J Health Care Qual Assur. 2017 Jun; 30(5): 477-88. doi: 10.1108/ijhcqa-012017-0004.
6. Haque A. The COVID-19 pandemic and the public health challenges in Bangladesh: a
commentary. J Health Res. 2020; 34(6): 563-7. doi: 10.1108/JHR-07-2020-0279.
7. Hossain Z, Hashmi Y, Mezbah-ul-Islam M. ICT facilities and literacy in rural non-government
secondary school libraries of Bangladesh. Sch. Libr. Worldw. 2019; 25(2): 66-80. doi:
10.14265.25.2.005.
8. Mayhew F. Poll suggests shift from print to digital will last beyond pandemic. Press Gazette.
[Internet]. 2020 Aug 12 [cited 2020 Aug 14]. Available at: https://www.pressgazette.co.uk/pollmore-people-consuming-news-digitally-than-in-print-during-pandemic-lockdown/.
9. Keelery S. COVID-19 impact on media consumption by type of media 2020. [cited 2020 Aug 3].
Available at: https://www-statista-com.proxy.lib.sfu.ca/statistics/1113485/india-coronavirusimpact-on-media-consumption-by-media-type/.
10. Duffy A, Tandoc E, Ling R. Too good to be true, too good not to share: the social utility of fake
news. Inf Commun Soc. 2020; 23(13): 1965-79. doi: 10.1080/1369118X.2019.1623904.
11. Tandoc EC, Lim D, Ling R. Diffusion of disinformation: how social media users respond to fake
news and why. Journalism. 2019; 21(3): 381-98. doi: 10.1177/1464884919868325.
12. Tandoc EC, Lim ZW, Ling R. Defining ‘fake news’. Digital Journalism. 2018; 6(2): 137-53. doi: 10.
1080/21670811.2017.1360143.
13. Sutaria S. Coronavirus misinformation in India is not limited to health misinformation. [updated
2020 Jul 20; cited 2020 Sep 20]. Available at: https://meedan.com/reports/coronavirusmisinformation-in-india-is-not-limited-to-health-misinformation/.
14. Brennen JS, Simon FM, Howard PN, Nielsen RK. Types, sources, and claims of COVID-19
misinformation. [cited 2020 Sep 20]. Available at: https://reutersinstitute.politics.ox.ac.uk/sites/
default/files/2020-04/Brennen - COVID 19 Misinformation FINAL %283%29.pdf.
15. Rumor Scanner Bangladesh. Rumor: Thankuni will prevent coronavirus. [updated 2020 Mar 18;
cited 2020 Aug 27]. Available at: https://rumorscanner.com/fact-check/archives/750.
16. Rashid H. Disinformation in the time of coronavirus outbreak. The Business Standard. [Internet].
2020 Mar 18 [cited 2020 Sep 20]. Available at: https://tbsnews.net/international/coronaviruschronicle/disinformation-time-coronavirus-outbreak-57889.
17. Naeem SB, Bhatti R. The Covid-19 ‘infodemic’: a new front for information professionals. Health
Info Libr J. 2020; 37(3): 233-9. doi: 10.1111/hir.12311.
18. Laato S, Islam AKMN, Islam MN, Whelan E. What drives unverified information sharing and
cyberchondria during the COVID-19 pandemic?. Eur J Inf Syst. 2020; 29(3): 288-305. doi: 10.1080/
0960085X.2020.1770632.
19. Erku DA, Belachew SA, Abrha S, Sinnollareddy M, Thomas J, Steadman KJ, et al. When fear and
misinformation go viral: pharmacists’ role in deterring medication misinformation during the
COVID-19related
online
misinformation
367
#
#‘infodemic’ surrounding COVID-19. Res Social Adm Pharm. 2021; 17(1): 1954-63. doi: 10.1016/j.
sapharm.2020.04.032.
20. Gradon K. Crime in the time of the plague: fake news pandemic and the challenges to lawenforcement
and intelligence community. Soc Regist. 2020; 4(2): 133-48. doi: 10.14746/sr.2020.4.
2.10.
21. Orso D, Federici N, Copetti R, Vetrugno L, Bove T. Infodemic and the spread of fake news in the
COVID-19-era. European Journal of Emergency Medicine. 2020 Oct; 27(5): 327-8. doi: 10.1097/MEJ.
0000000000000713.
22. Alvarez-Risco A, Mejia CR, Delgado-Zegarra J, Del-Aguila-Arcentales S, Arce-Esquivel AA,
Valladares-Garrido MJ, et al. The Peru approach against the COVID-19 infodemic: insights and
strategies. Am J Trop Med Hyg. 2020 Aug; 103(2): 583-6. doi: 10.4269/ajtmh.20-0536.
23. UNB News. 79 arrested for spreading rumour over COVID-19. United News of Bangladesh.
[Internet]. 2020 May 6 [cited 2020 Aug 30]. Available at: https://unb.com.bd/category/Bangladesh/
79-arrested-for-spreading-rumour-over-covid-19/51155.
24. Islam MN, Islam AKMN. A systematic review of the digital interventions for fighting COVID-19:
the Bangladesh perspective. IEEE Access. 2020; 8: 114078-87. doi: 10.1109/ACCESS.2020.3002445.
25. Govt launches campaign to make people aware of fake info. Daily Prothom Alo. [Internet]. 2020
Sep 8 [cited 2020 Sep 17]. Available at: https://en.prothomalo.com/science-technology/govtlaunches-campaign-to-make-people-aware-of-fake-info.
26. Drinking tube well water would prevent corona. Bangla Tribune. [Internet]. 2020 Sep 12 [cited
2020 Sep 17]. Available at: https://www.banglatribune.com/country/news/641975/.
27. Shawki A. Govt. asks Facebook to delete pages spreading Covid-19 rumours. The Business
Standard. [Internet]. 2020 Apr 30 [cited 2020 Aug 7]. Available at: https://tbsnews.net/coronaviruschronicle/covid-19-bangladesh/govt-asks-facebook-delete-pages-spreading-covid-19-rumours.
28. Bangladesh: new law will silence critics. Human Rights Watch. [Internet]. 2018 Sep 24 [cited 2020
Aug 7]. Available at: https://www.hrw.org/news/2018/09/24/bangladesh-new-law-will-silencecritics.
29. Shams S. Bangladeshi lecturer arrested over Facebook coronavirus post. DW. [Internet]. 2020 Jun
14 [cited 2020 Aug 7]. Available at: https://www.dw.com/en/bangladeshi-lecturer-arrested-overfacebook-coronavirus-post/a-53803383.
30. Bangladesh: alarming crackdown on freedom of expression during coronavirus pandemic. Article 19.
[Internet]. 2020 May 19 [cited 2020 Aug 7]. Available at: https://www.article19.org/resources/
bangladesh-alarming-crackdown-on-freedom-of-expression-during-coronavirus-pandemic/.
Corresponding author
Md. Sayeed Al-Zaman can be contacted at: msalzaman@juniv.edu
For instructions on how to order reprints of this article, please visit our website:
www.emeraldgrouppublishing.com/licensing/reprints.htm
Or contact us for further details: permissions@emeraldinsight.com
JHR
35,4
368
A Policy Framework to Support Shared Decision-Making
through the Use of Person-Generated Health Data
Carolyn Petersen1 Margo Edmunds2 Deven McGraw3 Elisa L. Priest4 Jeffery R.L. Smith5
Eagan Kemp6 Hugo Campos7
1Mayo Clinic, Rochester, Minnesota, United States
2AcademyHealth, Washington, Dist. Of Columbia, United States
3Ciitizen, Corp., Palo Alto, California, United States
4Baylor Scott and White Health, Dallas, Texas, United States
5Office of the National Coordinator for Health IT, U.S. Department of
Health and Human Services, Washington, Dist. Of Columbia, United
States
6Public Citizen, Washington, Dist. Of Columbia, United States
7All of Us Research Program and California Precision Medicine
Consortium (CaPMC), Oakland, California, United States
ACI Open 2021;5:e104–e115.
Address for correspondence Carolyn Petersen, MS, MBI, FAMIA, Mayo
Clinic, 200 First Street, SW, Rochester, MN 55905, United States
(e-mail: petersen.carolyn@mayo.edu).
Keywords
► health policy
► policy making
► privacy
► patient engagement
► health care reform
► data management
Abstract Background Individuals increasingly want to access, contribute to, and share their
personal health information to improve outcomes, such as through shared decisionmaking
(SDM) with their care teams. Health systems’ growing capacity to use person-
generated health data (PGHD) expands the opportunities for SDM. However, SDM not
only lacks organizational and information infrastructure support but also is actively
undermined, despite public interest in it.
Objectives This work sought to identify challenges to individual–clinician SDM and
policy changes needed to mitigate barriers to SDM.
Methods Two multi-stakeholder group of consumers, patients, caregivers; health
services researchers; and experts in health policy, informatics, social media, and user
experience used a consensus process based on Bardach’s policy analysis framework to
identify barriers to SDM and develop recommendations to reduce these barriers.
Results Technical, legal, organizational, cultural, and logistical obstacles make data
sharing difficult, thereby undermining use of PGHD and realization of SDM. Stronger
privacy, security, and ethical protections, including informed consent; promoting
better consumer access to their data; and easier donation of personal data for research
are the most crucial policy changes needed to facilitate an environment that supports
SDM.
Conclusion Data protection policy lags far behind the technical capacity for third
parties to share and reuse electronic information without appropriate permissions,
while individuals’ right to access their own health information is often restricted
unnecessarily, poorly understood, and poorly communicated. Sharing of personal
information in a private, secure environment in which data are shared only with
individuals’ knowledge and consent can be achieved through policy changes.
received
August 16, 2020
accepted after revision
September 9, 2021
DOI https://doi.org/
10.1055/s-0041-1736632.
ISSN 2566-9346.
© 2021. The Author(s).
This is an open access article published by Thieme under the terms of the
Creative Commons Attribution License, permitting unrestricted use,
distribution, and reproduction so long as the original work is properly cited.
(https://creativecommons.org/licenses/by/4.0/)
Georg Thieme Verlag KG, Rüdigerstraße 14, 70469 Stuttgart,
Germany
Original Article
THIEME
e104
Published online: 2021-11-15
#
#Introduction
Shareddecision-making (SDM) isaprocessofengagement that
balances patient preferences and values with clinical evidence
in the service of better delivering person-centered health
care.1–3 Despite the growing importance of SDM strategies
and tools to patients and health care systems, structural and
cultural barriers related to datamanagement hinder proactive
use of such strategies and tools. Patients have long relied upon
clinicians to manage personal information generated during
clinical encounters, andwill continuetodosoevenas theygain
access to personal health information (PHI) through eHealth
tools such as patient portals and application programming
interfaces (APIs). The growing use of consumer-friendly devices
(e.g., wearables) is making it possible for patients to
generate health-related data that both they and their care
teams can use to facilitate better health. However, technical
barriers, legislative and regulatory constraints, and market
forces currently prevent realization of the potential of persongenerated
health data (PGHD). This paperdescribes conditions
in the United States (U.S.) that currently limit the use of PGHD
in SDM and recommends strategies to remove these barriers.
Definition of Terms
Person-Generated Health Data (PGHD) are health-related
data created, recorded, gathered, or inferred by or
from patients or their designees to help address a health
concern.70
Shared Decision Making (SDM) is an approach where
clinicians and patients share the best available evidence
when faced with the task of making decisions, and where
patients are supported to consider options, to achieve
informed preferences.71
Approach to This Work
With funding from a Eugene Washington Patient-Centered
Outcomes Research Institute (PCORI) Engagement Award to
AcademyHealth, 50 diverse stakeholders with expertise in
consumer-generated health data were invited to help develop
aunifying framework for using PHI in SDMwith care teams. The
purpose of the framework was to develop a consumer-driven,
consensus-based research agenda to guide future studies that
would: (1) strengthen the evidence base on using consumergenerated
information in SDM; (2) identify policy barriers and
changes needed to facilitate SDM; and (3) describe the tools,
culture, and organizational changes needed to support and
encourage SDM (e.g., addressing time pressures). This approach
was based on Eugene Bardach’s widely-used policy analysis
framework, which describes a systematic, multidisciplinary
process for evidence-based, consensus-based decision-making
about complex multisector issues.4,5
Participants
Guided by a representative advisory group, twomulti-stakeholder
workshops were held in the spring of 2019 in Wash-
ington, DC, and at the University of California-Davis in
Sacramento, California. Stakeholders were invited by the
advisory group based on their expertise and experience
with consumer informatics and digital health; conducting,
funding, and participating in research on patient-centered
outcomes and SDM; health policy; and health communications.
They included consumers, patients, caregivers, clini-
cians, policy experts, health services researchers,
epidemiologists, and experts in social media, graphic design,
and user experience and came from academia, non-profit
research and membership organizations, health care delivery
systems, and public and private sector funders. Several
had previously participated in PCORI-funded studies or
convenings on consumer informatics and electronic health
data and were suggested through advisory group members,
the project team, and PCORI. They also reflected diversity in
racial, ethnic, and gender identities.
The advisory group (named in the Acknowledgments)was
co-chaired by a consumer and an informatics researcher; its
members included three consumer-caregivers, three health
systems researchers, and an experience design strategist. All
of the consumers and patients had lived experience with
managing health conditions and using electronic health data
through web portals, registries, and PGHD (e.g., wearables,
implants), and several were active in social media. The
advisory group members reflected diversity in
racial/ethnic and gender identities. Honorariawere provided
and travel expenses were paid for participating consumers
and advisory group members.
Method
The approach followed the steps described in Bardach’s policy
analysis framework: (1) Define the problem; (2) Assemble the
evidence; (3) Construct policy alternatives; (4) Select the
criteria for decision-making; (5) Project the outcomes; (6)
Confront trade-offs; (7) Make decisions/recommendations;
and (8) Share the results of the process. The 1.5 day workshop
structure included background pre-readings based on peerreviewed
literature selected by the project team and advisory
group; facilitated, interactive group participation in large and
small groups to define the problem and set priorities for
discussion; and real-time development and sharing of written
outlines andvisualdesigns to illustratekeyconceptsduring the
workshop. A collaborative consensus approach was used in
which a variety of stakeholder perspectives were represented
and all perspectives were discussed and valued equally during
the deliberations.
Through facilitated discussions and interactive group
participation, all participants engaged in group prioritysetting
by consensus and came to agreement about high-
level priorities for four key topics for recommendations: gaps
in the evidence base on SDM; the informatics tools used in
PGHD; policy facilitators and barriers to sharing PHI; and the
cultural changes that would be needed to support implementation
of SDM on a larger scale.
ACI Open Vol. 5 No. 2/2021 © 2021. The Author(s).
Shared Decision-Making through the Use of Person-Generated Health Data Petersen et al. e105
#
#After the workshops, four volunteer cross-sector writing
teams were formed to write papers for submission to peerreviewed
journals. Each paper was led by an advisory group
member and included at least one consumer/caregiver representative
along with interested research, policy, and tech-
nology experts for each of the four topics. The entire advisory
group continued to meet on a monthly basis for 5 months to
refine the conceptual framework; standardize definitions
across the papers; review illustrative graphic designs; and
discuss cross-cutting themes in the recommendations. The
four papers were submitted separately to peer-reviewed
journals in late fall 2019.
Background
Consumer interest in using technology to monitor and treat
health conditions is growing,6,7 along with interest in
connecting and sharing PHI with other individuals undergoing
similar experiences.8–10 Most often, consumers prefer
to share their information in a secure environment in which
their information will be available to them and other
authorized users, such as their clinicians and caregivers,
when needed but protected from unauthorized use and
breaches.11
When important and relevant health information is not
readily available through clinical channels, or when information
is incomplete or inaccessible, consumers often turn
to social media platforms to learn more about a condition,
share information, find out about new clinical trials and
treatments, rate clinicians, and generate a myriad other
types of information that is not collected, studied, or otherwise
available through clinical sources.11–14Data fromonline
sources such as Facebook, Google, and Twitter are now used
for research on health habits, treatment preferences, and
recruitment into clinical studies, providing important sources
of PGHD but raising questions about informed consent,
privacy, and other ethical issues.12,13,15
The importance of electronic health records and decision
support tools in improving patient safety and health carewas
one of the main drivers for the 2009 Health Information
Technology for Economic and Clinical Health (HITECH) legislation,
which provided financial incentives for clinicians,
hospitals, and health systems to adopt electronic health
records.16 At the time, it was hoped that building a standard-based,
interoperable electronic information infrastruc-
ture would make it easier to share electronic information
among providers to coordinate care for patients as they
moved through the continuum of care.17
There are some prominent examples of health system
changes to promote electronic information exchange. More
than 200 health systems are nowparticipating inOpenNotes,
which allows 40 million patients to access their clinicians’
visit notes electronically through patient portals.18,19
Patients who access their clinical visit notes are more likely
to manage their medications appropriately, experience
greater trust in their providers, and feel more satisfied
with their quality of care.20–23 However, the majority of
health systems lack the capacity or inclination to share
clinical information with other health systems, out of technical,
legal, or business concerns.24
Data Sources and Potential Sites of Delivery
The rapid emergence of smartphones and other new technologies,
particularly consumer-facing technologies such as
wearable health/fitness trackers, has resulted in a rapidly
evolving landscape of both data sources and data types.25 The
majority of health data no longer reside in electronic health
records,26 andmany of these data can be used to infer health
or factors that influence health. Data that are generatedwith
intention by consumer devices, such as exercise and sleep
data, are well-known and familiar, but other less familiar
types of data include patient-reported outcome measures
used to record experiences with investigational therapies
and/or established treatments, signals from in-homemotion
detection systems that indicate when individuals are not
undertaking their usual activity levels, mobile health apps
that remind users to take their medication and record that
they have done so, Web search metrics that suggest the
spread of infectious disease, social media posts that reveal an
increase in mental health concerns in response to public
events, and others.27
Besides these recognized forms of data, the use of devices
leaves behind a trail—so-called “digital dust.” Invisible and
often unknown to users, these data trails can be analyzed by
third parties to show where users have been digitally and in
real life, and what they were doing during that time.28 Such
data can be used to indirectly measure device users’ health
habits, such as howoften they visit fast food restaurants, how
often they use a gym of which they are a member, and
whether they commute by car or bicycle. These types of data
can be aggregated, analyzed, and repackaged for sale, often
under the description of “social determinants of health.”
For example, one company has created a set of health
scores built on “hundreds of clinically-validated socioeconomic
attributes” compiled from more than 10,000 sources
of public and proprietary records.29 These scores can predict
health-related outcomes such as medication adherence and
hospital readmissions, and both the data and the risk scores
are marketed to payers and providers (e.g., physicians,
pharmacists),29–31 all without the consumers’ awareness
or informed consent. Other business models market risk
scores to employers interested in engaging employees in
health and wellness programs.32
PGHD originate from many sources and locations, and
take many forms, and there are few guidelines within health
care to address how they may or should be collected,
managed within archives, used, re-used for other purposes,
shared with other entities within and outside health care,
and transmitted to others.14,33 Many, if not most, patients
and consumers have little to no awareness of just how fluid
the data stream between data sources and data users may be.
Theymayhave even less understanding of whether (and if so,
how) they might manage the flow of the 10,000þ sources
that could have their personal data.34Because 61% ofworkers
with employer-based coverage are enrolled in health plans
that are completely or partially self-funded by their
ACI Open Vol. 5 No. 2/2021 © 2021. The Author(s).
Shared Decision-Making through the Use of Person-Generated Health Data Petersen et al.e106
#
#employers, employers benefit from ensuring that workers
understand whether and how employers and insurers could
use these data, including whether employees are responding
to financial incentives to participate in screening and wellness
activities.35 However, employees who would prefer not
to share their sensitive PHI with their employers may not
have a way to opt out of this tracking, which can result in
higher health risk ratings, higher premiums, or even job loss
for no apparent reason.
Although informed consent processes are intended to
provide guidance for participating in clinical procedures or
research, in practice such documents are often highly technical,
wordy, or difficult for patients to understand, and
patients may feel pressured, with little time or opportunity
to review such legal agreements and ask questions.28 Individuals
commonly experience these challenges in consent
forms for clinical procedures and participation in research, as
well as when downloading health-related smartphone
apps.36–39 If patients agree to data sharing they do not fully
understand, by the time they realize that they prefer a more
restrictive approach it may be too late to “unshare” data
about them.33 Often patients must decide between the
potential for services that may be valuable and the risk
that data generated during those services will be used
contrary to their preferences and even safety.
Facebook activities offer multiple examples of potential
problems that can affect patients adversely. The social media
platform has:
• Shared with third parties PHI of members of several
closed support groups for people with genetic mutations
that predispose them to cancer, which members believed
to be private based on Facebook’s terms of use.40,41
• Collected data directly from health apps even if the user
does not have a Facebook account.42
• Approached hospitals about combining clinical patient
data with Facebook data, de-identifying data, and providing
it for research purposes.43 Because 99.8% of Americans
could be re-identified in any dataset using 15 demographic
attributes,44 it is unlikely that Facebook’s proposed
research data would truly be de-identified.
Although Facebook leadership has publicly called for
combining social media data with medical record information
to gain insights into social determinants of health
through an article in the Journal of the American Medical
Association,45 Facebook has neither pledged an intent to
protect PHI nor delineated a plan for how it can and—more
importantly—will do so. Facebook’s intent to merge social
media datawithmedical information covered by U.S. privacy
laws, with the support of medical professionals,46 increases
the possibility that information will be shared in ways the
individuals do not intend and have not agreed to, even if the
individuals wished to keep it confidential and/or are unaware
that the information exists (e.g., insights gained via
analysis using artificial intelligence).
As these examples indicate, business practices that support
popular data-generating tools may not handle PGHD as
individuals intend or expect, creating a need for more
transparency about the flow of data.
Risks of Unintended, Unexpected, and Unconsented
Data Sharing
Erosion of trust is among the most significant risks arising
from the lack of privacy and security protections for data
generated by individuals. If individuals do not believe that
the information they are populating into mobile apps and
devices and sharing within health social media groups is safe
and protected, or if they see this information being easily
bought and sold without their knowledge or consent, they
may not use those tools or may censor their use.47–50 This
maymean they forego the benefits of use for themselves and
also discourages ethical uses, such as sharing their data for
research of rare diseases.
If “digital dust,” often collected without an individual’s
knowledge (much less consent) while they are online, is
unexpectedly used to make decisions about them or about
subpopulations with which they identify, they may stop
using Internet or online tools to search for health information
for themselves or others, or to interact with otherswho share
their health needs and interests. Their mistrust is magnified
if they perceive these data to be readily available to everyone
—health care providers and their business associates as a
matter of course, as well as for commercial gain by third
parties—except the individuals themselves. In addition, they
may be categorized into subpopulations that do not accurately
describe them or to which they do not regard them-
selves as belonging. The downstream effects of these
misclassifications (e.g., received direct marketing obviously
intended for members of a different group) may further
erode their trust, understandably so.
Health systems and payors may lack trust in PGHD from
these tools if the data are perceived to be inaccurate or less
than complete (due to individual reluctance to fully utilize
them, understand them, or to disclose fully and honestly) or
unreliable (because data being collected are based on inferences
or due to doubts about the origins of, or security
protections, for this data).51,52 Inaccurate and/or incomplete
data can result in incorrect decisions by clinicians, leading to
poor health outcomes in patients, as well as skewed research
results, loss of reimbursement for care and/or research
funding, malpractice litigation, and other undesirable outcomes
for health systems, so health systems approach new
technologies with caution.
Systems and payorsmay be reluctant to trust the outcome
of algorithms whose inputs may be potentially biased as to
racial andgender identities; are not transparent; or that have
other potential negative outcomes that are difficult to predict
or mitigate.53,54 Conversely, they may simply trust what
they believe the algorithmically analyzed data tell them and
make inaccurate diagnoses, thereby setting themselves up
for non-beneficial and/or dangerous or discriminatory data
use by third parties, such as targeting patients for ads for
products and services that may be unnecessary or even
harmful.
ACI Open Vol. 5 No. 2/2021 © 2021. The Author(s).
Shared Decision-Making through the Use of Person-Generated Health Data Petersen et al. e107
#
#Every data breach has implications for public trust of
technology, health systems, and health research. As long as
policies on collecting, storing, using, and sharing data are not
transparent and consequences for misinformation and violations
are not disclosed or enforced, knowledgeable con-
sumers may be reluctant to agree to share or donate data
about them.
►Fig. 1 illustrates the range of possible combinations of
levels of risk disclosed and discussed by participants in the
workshops. The illustration reflects the vast number of
sources of data and the variations in the amount of control
individuals are able to exert over their own personal health
data. Individuals may experience multiple adverse effects
simultaneously, as described in the accompanying vignette
“How Bad Can It Get?”
Current Policy Context
The U.S. approach to health data privacy is fragmented and
the most foundational policies are more than 20 years old,
having been written long before the digitization of health
care.55 Subsequent policy “patches” are narrowly tailored to
specific data types, such as genetic information, or organizational
settings, such as care delivery or research operations.
This patchwork is insufficient to protect PGHD and is illsuited
for the evolving technology landscape and scope of
health data. ►Table 1 illustrates the policy patchwork and
summarizes the provisions of key U.S. federal laws governing
health information that are relevant to PGHD.
Despite two decades of continuous legislative and regulatory
activity intended to promote the information-sharing
interests of patients and clinicians, technology advances and
business practices have far outpaced adjustments in federal
policy,56 resulting in unnecessary risk and frustration for
everyone while still not supporting real-time access to key
information. The privacy, security, and breach notification
regulations under the Health Insurance Portability and Accountability
Act (HIPAA) govern the use and disclosure of
identifiable health information (known as protected health
information)—but only when that information is held by
covered entities (most health care providers and health
plans) and their contractors (otherwise known as “business
associates”). PHI is routinely shared outside of HIPAA’s
coverage, including when patients upload information into
a mobile health app of their choosing. Commercial entities
collecting personal information are required by federal law
(the Federal Trade Commission Act or FTCA) to adopt reasonable
security safeguards and uphold their commitments
to consumers regarding how data are accessed, used, and
shared. But notwithstanding these baseline protections,
unauthorized transmission of digital data- or data leakage
—occurs.57
New U.S. policies that require certified electronic medical
records to make PHI available to consumer apps via APIs
Fig. 1 The continuum of risk for person-generated health data. The Health Data Landscape illustrates the relative likelihood that various types of
personal information (e.g., demographic, socioeconomic, health-related, financial) will be collected and/or shared without individuals’
knowledge and/or permission. Figure designed by Hugo Campos for Improving the Care Experience: A Collaborative Consensus Project.
ACI Open Vol. 5 No. 2/2021 © 2021. The Author(s).
Shared Decision-Making through the Use of Person-Generated Health Data Petersen et al.e108
#
#Table 1 Current Legal and Regulatory Landscape: Privacy Protections for PGHD used for shared decision-making. This table
summarizes key federal legislation and regulations in the U.S. related to patient/individual data privacy and PGHD for shared
decision-making. The table clarifies patients’ rights with regard to data access and control; the obligations of entities covered by the
law (for example, under HIPAACovered Entity [CEs], Business Associates [BAs], and health care researchers); coverage of consumerfacing
technology companies that manages or uses patient/individual data (if any); and prohibitions (if any) that limit how
patient/individual data may be used.
Rights Obligations Prohibitions and limitations
HIPAA Privacy,
Security, and
Breach
Notification
Rules
• Grants patients the right to:
• Access their protected health
information (PHI) maintained by or
for a CE.
• Access laboratory test reports from
Clinical Laboratory Improvement
Amendments (CLIA)-certified or
CLIA-exempt laboratories.
• Transmit their PHI to a 3rd party.
• Receive an accounting of parties
who have received their PHI for
purposes other than treatment,
payment or health care operations
(Accounting of Disclosures).
• Decide whether to share their PHI
for purposes of research through
an authorization or informed
consent (unless the authorization
requirement is waived by an
institutional review board [IRB] or
Privacy Board).
• Control whether and how their PHI
is used and disclosed for marketing
purposes.
• Control whether their PHI can be
sold.
• Request amendments to their PHI.
• Request restrictions on how their
PHI can be used and disclosed
(up to the entity on whether to
honor, except as noted below).
Demand their PHI not be disclosed to
payers if patients pay out of pocket in
full for services.
• Be notified of breaches of their health
information by a covered entity.
Requires CEs and BAs to protect PHI by
maintaining reasonable:
• Administrative safeguards
• Physical safeguards
• Technical safeguards.
Requires CEs to:
• Garner a patient’s prior written
authorization to use or disclose PHI
for any purpose not expressly
permitted by the regulations—for
example, sales of PHI and uses or
disclosures of PHI for marketing
purposes.
• Assure in writing (e.g., via contract)
that its BAs will appropriately
safeguard the protected health
information it receives or creates
on behalf of the CE.
• Notify individuals (and the federal
government) in the event of
breaches of PHI.
Requires researchers who work for CEs
to obtain an individual’s authorization
for research containing non-de-identified
PHI or a waiver of authorization is
granted by an IRB or Privacy Board.
All things not permitted by HIPAA may
be done with the authorization of the
individual through a HIPAA-compliant
authorization.
Consumer-based technology
companies have no obligations with
regard to data management and/or use
practices in HIPAA.
HITECH Act Grants patients the right to be notified
of a breach of their PHI by a personal
health record vendor (e.g., an mHealth
app).
Requires personal health record vendors
to notify individuals and the Fed-
eral Trade Commission of breaches of
their identifiable information.
None
21st Century
Cures Act
Extends the rights of individuals to
access their complete medical record
and all electronic health information
(EHI) held by a provider or a health
information network through
prohibitions on information blocking.
Provides patients with the right to
access key aspects of their health
information through the app of their
choice via application programming
interfaces (APIs) in provider electronic
medical records (effective in
2022–2023).
Requires providers and health
information networks to provide
patients (upon request) with key
aspects of their health information.
More information to be provided to
patients (or an app chosen by the
patient) via APIs by 2022.
Applies only to providers, certified
electronic health record vendors, and
health information networks.
Applies only to information in the U.S.
Core Data Set for Interoperability,
which is not all information the patient
has the right to under HIPAA.
Part 2 Grants patients’ rights to consent prior
to disclosures of their identifiable
health information, in most
circumstances (including for treatment
purposes).
Requires individual consent to share for
most purposes, including treatment.
Entities covered by HIPAA and Part 2
may disclose data for research purposes
consistent with HIPAA (for example,
with authorization, with a waiver of
authorization, or under broad consent
provisions).
Prohibits law enforcement access to
identifiable data without a court order.
Applies only to federally supported
substance abuse treatment facilities
and programs (although identifiable
data from a Part 2-covered program
continues to be covered by Part 2 even
if it is lawfully disclosed to another
entity).
No express individual rights to access
data.
No consent required to share data for
treatment purposes in a “bona fide”
medical or national emergency.
Of note: the Coronavirus Aid, Relief,
and Economic Security (CARES) Act
made some changes to these
regulations
(Continued)
ACI Open Vol. 5 No. 2/2021 © 2021. The Author(s).
Shared Decision-Making through the Use of Person-Generated Health Data Petersen et al. e109
#
#further complicate the landscape, given the interest such
vendors may have in acquiring and using PHI for commercial
purposes. In examining the increasing reliance on patientfacing
APIs to promote patient access to their clinical infor-
mation, a U.S. Office of the National Coordinator for Health
Information Technology Task Force noted that an app may
need to comply with several federal laws, including HIPAA (if
the app is being offered by a covered entity or business
associate), the Federal Trade Commission Act (FTC Act), and
the FTC’s Health Breach Notification Rule, among others.58
Recent guidance from the U.S. Food and Drug Administration
also attempts to clarify what apps are regulated medical
devices that require FDA review and approval for safety and
efficacy (FDA does not regulate the privacy of information
collected by apps).59 But not withstanding that some laws do
apply to these apps, they do not provide the comprehensive
framework for privacy and security that may be needed to
build and maintain consumer trust in these tools.60
Although the focus of this article is to assure protections
for data that fall outside of the coverage of HIPAA, HIPAA
itself is not without its flaws. Health care organizations seek
to comply with state and federal statutes, but struggle to do
so despite the availability of detailed guidance from regulators
because of differences in legal interpretations.61–63 As
a result, organizations often are reluctant to release any
patient information over fear of violating the law and potentially
incurring financial penalties.64 Information blocking, a
set of practices undertaken by vendors and providers that
restrict access to patient information, has been characterized
as a revenue enhancement strategy.65A2016 studyof the top
20 hospitals rated byU.S. News andWorld Report foundmany
were not meeting federal requirements for medical records
formats and timeliness of processing.66 Also, notwithstanding
clear mandates in HIPAA since 2001 to provide patients
with easy access to their health information, patients still
struggle to get copies of their health data.
However, changemay be on thehorizon. TheHHSOffice for
Civil Rights—which enforces HIPAA—announced in February
2019 that it would be launching a new HIPAA Right of
Access enforcement initiative. OCR reached itsfirst settlement
in this new initiative in early September 2019,67whichmay be
a harbinger of more active federal oversight and enforcement
Table 1 (Continued)
Rights Obligations Prohibitions and limitations
Common Rule Grants patients the right to choose
whether or not to allow their
identifiable health information to be
accessed for research (either specific
research projects or broadly defined)
when research is performed by an
entity subject to the Rule.
Requires researchers covered by the
Rule to:
• Receive approval from an IRB or
Privacy Board prior to conducting a
study that will use PHI.
• Either seek consent or obtain
waiver of consent from an
Institutional Review Board prior to
using identifiable health
information for research purposes.
Does not apply to data that are not
identifiable to researchers (not
considered to be human subjects
research).
FTC Act None None Applies to most consumer
technologies.
Prohibits deceptive or unfair acts or
practices in or affecting commerce,
including those relating to privacy and
data security, and those involving false
or misleading claims about apps’
handling of personal data, safety, or
performance.
Genetic
Information
Nondiscrimination
Act
Individuals have a right to pursue
private litigation if they feel they have
been discriminated against in
employment.
Health insurance discrimination on the
basis of genetic information may be a
violation of the Affordable Care Act or
other civil rights laws.
Prohibits health insurers from:
• Using genetic information to make
eligibility, coverage, underwriting
or premium-setting decisions.
• Requesting or requiring individuals
or their family members to
undergo genetic testing or to
provide genetic information.
• Using genetic information to make
any decisions about health
insurance benefits, eligibility for
benefits, or the calculation of
premiums under a health plan.
Prohibits employers from using genetic
information in employment decisions
such as hiring, firing, promotions, pay,
and job assignments
Prohibits employers or other CEs
(employment agencies, labor
organizations, joint labor-management
training programs, and apprenticeship
programs) from requiring or requesting
genetic information and/or genetic
tests as a condition of employment.
ACI Open Vol. 5 No. 2/2021 © 2021. The Author(s).
Shared Decision-Making through the Use of Person-Generated Health Data Petersen et al.e110
#
#on behalf of consumer access to data. Another approach is a
public scorecard recently implemented by Ciitizen, in which
health care provider organizations receive scores on their
responses to actual patient requests for their own records.68
Regulations penalizing “information blocking,” which were
issued by theHHS Office of the National Coordinator pursuant
to the 21st Century Cures Act, will go into effect as of
April 2021,69 and provider electronic medical record technology
will be equipped with patient-facing APIs no later than
mid-2022. These initiativeswill spurgreater accessbypatients
to comprehensive clinical data—but also increase the pressure
on policymakers to assure privacy and security protections for
data that fall outside of HIPAA protections.
Building a Better Future: Policy
Recommendations
A new policy framework is needed to facilitate a data-rich
environment that leads to better SDM, where individuals
trust that they can actively use online and mobile tools to
collect, use, and share PGHD, both in pursuit of their own
individual health andwellness goals aswell as to improve the
health and wellness of others. The following recommendations
originated as policy priorities in workshop discussions
and were refined in subsequent discussions by the coauthors.
They are consensus-based, consumer-driven, and
outline some key best practices for the collection, use, and
sharing of PGHDnot covered under HIPAA, except as noted to
support HIPAA compliance.
Recommendation 1: Policymakers should assure privacy,
security, and ethical protections for PGHD (all person-generated
data used for health and wellness purposes) are in
place, regardless of which entity is holding the data.
• As highlighted in ►Table 1, data used for health and
wellness purposes are not protected in all settings.
• Policymakers should act to fill gaps in current protections
and, for existing laws, regulatory agencies should enact and
robustly enforce regulations (including penalties) of sufficient
weight to promote compliance and enable a trusted
environment for the collection, use, and sharing of PGHD.
Additional action is needed by Congress to make this
possible.
• Protections for PGHD should be sufficiently comprehensive
to, at a minimum, facilitate recommendations 2 to 5.
Recommendation 2: Individuals must be able to access
PGHD data about them electronically, promptly after it is
generated (“real-time”), at their convenience using their
preferred method(s).
• Policymakers should ensure that individuals are able to
identify, access, and manage their PGHD—including the
ability to request correction of inaccurate information—in
a timely manner and at a reasonable cost. Without a
guarantee of access to their data upon request supported
by appropriate regulation, individuals have no reason to
trust data-gathering institutions, whether such organizations
are health care-based or commercial.
• This access should include all PGHD, including data covered
by HIPAA (such as that generated through clinical
care and payment encounters) as well as through commercial
transactions, such as purchasing over-the-count-
er medications and visiting a gym or a website.
Recommendation 3: Collection of PGHD should include a
robust, consistent, and transparent process for ensuring that
informed consent occurs whenever andwherever the potential
for data sharing arises.
• Any agreement, contract, or terms of service offered to an
individual should include a straightforward delineation of
the individual’s rights regarding their access to data about
themaswell as how those data are stored andmay be used
or shared by companies, researchers, and organizations.
Agreements also should clearly state the costs that individuals
must bear, if any.
• To the extent feasible, this process should include mechanisms
(e.g., opt-in approaches) through which individu-
als can identify potential data uses they are consenting to
when data about them are collected and restrict access to
this data (in terms of use or in terms of duration) without
limiting its use for either the individual or the public good.
• This informed consent process must include information
on how individuals can contact the organization or company
storing data about them and a process bywhich they
can change or update their ongoing consent.
• Individualsshouldbeable toupdatetheirpreferencesatany
time, including the right towithdrawaccessbyentitieswith
which they have previously shared their information.
• Individuals should be able to choose the tool(s) they use to
share data about them.
Recommendation 4: Individuals should be able to ensure
that researchers can use data about them for meaningful
medical research.
• Individuals often wish to contribute data about them for
medical research, and they should be able to donate this
data, as well as require others who hold this data to
contribute it for research purposes (including when those
data are held by entities covered by HIPAA). After consent
has been given, researchers should cover any costs associated
with PGHD use.
• Policies and regulations in the PGHD context should
ensure that individuals are aware of the past and present
uses of data about them for research purposes and consent
to future data use. The circumstances under which
the data could be used for research, how long the data can
be retained by research organizations, and the circumstances
under which individuals (and their families) could
later rescind the continued use of data about them for
research should be clearly disclosed.
Recommendation 5: Uses of PGHDmust be ethical and fair
to individuals and populations.
• Non-health-related uses of PGHD (e.g., for marketing of
non-health-related products such as cigarettes or alcohol)
ACI Open Vol. 5 No. 2/2021 © 2021. The Author(s).
Shared Decision-Making through the Use of Person-Generated Health Data Petersen et al. e111
#
#that could cause harm to individuals, subpopulations, or
populations (e.g., law enforcement access without a warrant
or insurer use to redline individuals or populations
out of benefits) should be prohibited.
• Tracking of data about individuals, vulnerable groups (e.g.,
elderly persons, people with disabilities), and historically
targeted groups (e.g., people of color) based on data
collected for specific purposes beneficial to the public
(such as public health) should be limited to health-related
uses that benefit individuals, subpopulations, and/or
populations.
• Ongoing, objective, ethical reviewof initiatives focused on
analysis of PGHD should be undertaken to ensure that
such analytics are conducted in a way that promotes
learning by individuals, care providers, and health care
systems and minimizes harm to individuals and populations.
Review bodies that include the full range of affected
stakeholders—including patients, caregivers, consumers,
and community representatives—can promote trust by
ensuring transparency around ongoing work and forcing
robust public discussion of proposed new efforts.
Conclusion
From the perspective of individuals, there are many opportunities
to improve health data management policy and its
implementation across health care organizations, health
information exchanges, and consumer-facing health management
tools. With a SDM process, PGHD have the potential
to inform which treatment(s) patients choose and how they
are given, decisions that confer power to shape the future
directions and goals of research.
In addition, the ability to re-use clinical data offers
significant opportunities for the advancement of practice,
the reduction of health disparities, and the promotion of
health equity—if we develop and implement at scale improved
processes supported by realistic, appropriate, and
transparent policies. Health policies that more closely align
to individuals’ needs and goals, as well as their expectations
with regard to consent, data sharing, and transparency about
use of data about them, will encourage SDM and facilitate
more positive experiences for individuals, clinicians, and
care teams alike.
How Bad Can It Get?
John, a 36-year-old manwith a knee injury and diagnosed
anxiety, works at a large corporation that self-insures. His
employer offers a wellness program that uses algorithms
to identify the best ways to incentivize employee selfcare.
John’s credit card transaction data reflect regular visits
to fast food restaurants and insurance claim-eligible
purchases revealing his anxiety diagnosis. Through these
and other data sources, he is invited to participate in
counseling for healthy eating offered via his employer’s
wellness program. He declines, because he is concerned
about what his employer will do with information about
where he eats and that he has anxiety. It already makes
him uncomfortable that he gets served ads for antianxiety
medications on his phone and his computer. One
time he handed his phone to a friend to show her his
vacation photos, and one of those ads popped up. He also
doesn’t think it’s any of his employer’s business where he
eats. Because he declined the program to try to keep his
information more private, his monthly health insurance
premium increases $100/month the following year.
When John’s knee pain worsens, he is referred for an
MRI. At the follow-up appointment, John’s orthopedist
tells him that hehas an elevated surgical risk score created
from clinical and “social factors.” John is unsure what
“social factors” are but decides not to pursue surgery as a
result.
Still dealing with his unresolved and painful knee
condition, several months later John receives a phone call
from a debt collector for an unpaid medical bill. He logs
into his insurance Web portal and finds claims from
doctors he did not visit. He assumes it’s amistake and calls
the clinic he visited for his knee pain. John explains his
situation, and it becomes clear that he is a victim of
medical identity fraud. When he requests a copy of his
own medical record to correct it, the clinic refuses access.72
The customer service representative wrongly tells
John that the clinicmust preserve the identity thief’s right
to privacy, even though the medical record is John’s.
Shocked and confused, he declines to pursue the matter.
Six months later John receives a notice that a local
radiology clinic had a breach of patient protected health
information (PHI) when scans weremade available on the
Internet,73 and that he is among those whose PHI was
exposed. He realizes that his knee MRI led to the theft of
his medical identity, and that he is now among the 25% of
Americans who have had health information stolen.74
After 2 years, hundreds of hours, and more than
$10,000 in legal fees and incorrect medical bills, John
continues working to repair the damage to his identity
and his health.75 He continues to pay higher insurance
premiums because of his concerns about the data collection
required for his participation in the wellness
program. His knee also still continues to bother him.
Clinical Relevance Statement
The evolution of patient–clinician relations from paternalistic
to more egalitarian engagement has resulted in broader
use of SDM as the basis for treatment planning and created a
need for re-assessment of factors influencing adoption of
SDM. At the same time, the increasing availability and
usability of PGHD are opening up new opportunities for
care management and treatment monitoring for clinicians
ACI Open Vol. 5 No. 2/2021 © 2021. The Author(s).
Shared Decision-Making through the Use of Person-Generated Health Data Petersen et al.e112
#
#and patients engaging in SDM. This article provides policy
recommendations for removing barriers to the collection,
use, and sharing of PGHD for health care management and
research.
Note
This work is one of four papers co-authored by participants
in a collaborative consensus project to develop a
framework for using person-generated health data in
shared decision-making. Each writing team included at
least one consumer or caregiver representative alongwith
other research, policy, and technology experts, and a user
experience designer. The work was overseen by a diverse
multisector advisory group co-chaired by Hugo Campos
and Katherine Kim,with Jeffrey Corkran, Patricia Franklin,
Sarah Greene, Megan O’Boyle, and Carolyn Petersen.
Advice and consultation with Dana Lewis, Liz Salmi, and
John Wilbanks and assistance and support from Lauren
Adams and Tamara Infante are gratefully acknowledged.
All statements in this report, including its findings and
conclusions, are solely those of the authors and do not
necessarily reflect the views of the Patient-Centered Outcomes
Research Institute (PCORI) or its Board of
Governors.
Authors’ Contributions
M.E., H.C., and C.P. planned theworkshop. Manuscript was
approved by C.P. All the authors developed the recommendations
and drafted the manuscript.
Protection of Human and Animal Subjects
No human subjects were involved in this project and
institutional review board approval was not required.
Funding
Financial support for this work was provided by a Eugene
P. Washington PCORI engagement award to Margo
Edmunds at AcademyHealth.
Conflict of Interest
D.M. reports other relevant activities from Ciitizen Corporation,
personal fees from All of U.S. Research Program
IRB, personal fees from Verily’s Project Baseline Advisory
Board, and other relevant activities from Datavant, outside
the submittedwork. J.R.L.S. was an AmericanMedical
Informatics Association employee during the periodwhen
this manuscript was being developed; his views are his
own and not those of HHS or the Office of the National
Coordinator for Health IT. The remaining authors report
no conflict of interest.
References
1 Beach MC, Sugarman J. Realizing shared decision-making in
practice. JAMA 2019;322(09):811–812
2 Elwyn G, Frosch D, Thomson R, et al. Shared decision making: a
model for clinical practice. J Gen Intern Med 2012;27(10):
1361–1367
3 Office of the National Coordinator for Health Information Technology.
Shared decision-making fact sheet; 2013. Accessed
October 9, 2019 at https://www.healthit.gov/sites/default/files/
nlc_shared_decision_making_fact_sheet.pdf
4 Bardach E. A Practical Guide for Policy Analysis: the Eightfold Path
to More Effective Problem Solving. New York: Chatham House
Publishers; 2000
5 Engelman A, Case B, Meeks L, Fetters MD. Conducting health
policy analysis in primary care research: turning clinical ideas
into action. Fam Med Community Health 2019;7(02):e000076
6 Davis S, Roudsari A, Raworth R, Courtney KL, MacKay L. Shared
decision-making using personal health record technology: a
scoping review at the crossroads. J Am Med Inform Assoc 2017;
24(04):857–866
7 Edmunds M. Promoting consumer engagement in health and
healthcare. InEdmunds M, Hass C, Holve E, eds. Consumer
Informatics and Digital Health: Solutions for Health and Healthcare.
Berlin, Germany: Springer; 2019:3–24
8 Lai AM, Hsueh PS, Choi YK, Austin RR. Present and future trends in
consumer health informatics and patient-generated health data.
Yearb Med Inform 2017;26(01):152–159
9 Smith TG, Dunn ME, Levin KY, et al. Cancer survivor perspectives
on sharing patient-generated health data with central cancer
registries. Qual Life Res 2019;28(11):2957–2967
10 Hartzler AL, TaylorMN, Park A, et al. Leveraging cues frompersongenerated
health data for peer matching in online communities. J
Am Med Inform Assoc 2016;23(03):496–507
11 Petersen C. Through patients’ eyes: regulation, technology, privacy,
and the future. Yearb Med Inform 2018;27(01):10–15
12 Arigo D, Pagoto S, Carter-Harris L, Lillie SE, Nebeker C. Using social
media for health research: methodological and ethical considerations
for recruitment and intervention delivery. Digit Health
2018;4:2055207618771757
13 Terrasse M, Gorin M, Sisti D. Social media, e-health, and medical
ethics. Hastings Cent Rep 2019;49(01):24–33
14 Petersen C, DeMuro P. Legal and regulatory considerations associated
with use of patient-generated health data from social
media and mobile health (mHealth) devices. Appl Clin Inform
2015;6(01):16–26
15 Pagoto S, Nebeker C. How scientists can take the lead in establishing
ethical practices for social media research. J Am Med Inform
Assoc 2019;26(04):311–313
16 Mennemeyer ST, Menachemi N, Rahurkar S, Ford EW. Impact of
the HITECH Act on physicians’ adoption of electronic health
records. J Am Med Inform Assoc 2016;23(02):375–379
17 Health IT and Patient Safety: Building Safer Systems for Better
Care. Committee on Patient Safety and Health Information
Technology; Institute of Medicine. Washington, DC: National
Academies Press (US); 2011
18 DesRoches CM, Bell SK, Dong Z, et al. Patients managing medications
and reading their visit notes: a survey of OpenNotes
participants. Ann Intern Med 2019;171(01):69–71
19 OpenNotes. More Than 40 Million Patients Can Access Their
Clinicians’ Visit Notes Via Secure Portals at 200 Health Systems.
Accessed September 22, 2019 at: https://www.opennotes.org/news/more-than-40-million-patients-can-access-their-
clinicians-visit-notes-via-secure-portals-at-200-health-systems/
20 Gerard M, Fossa A, Folcarelli PH, Walker J, Bell SK. What patients
value about reading visit notes: a qualitative inquiry of patient
experiences with their health information. J Med Internet Res
2017;19(07):e237
21 Blumenthal D, Abrams MK. Ready or not, we live in an age of
health information transparency. Ann Intern Med 2019;171(01):
64–65
22 Vodicka E, Mejilla R, Leveille SG, et al. Online access to doctors’
notes: patient concerns about privacy. J Med Internet Res 2013;15
(09):e208
ACI Open Vol. 5 No. 2/2021 © 2021. The Author(s).
Shared Decision-Making through the Use of Person-Generated Health Data Petersen et al. e113
#
#23 Wright E, Darer J, Tang X, et al. Sharing physician notes through an
electronic portal is associated with improved medication adherence:
quasi-experimental study. J Med Internet Res 2015;17(10):
e226
24 EdmundsM, PeddicordD, FrisseME. 2016. The evolution of health
information technology policy in the United States. InBall M,
Weaver C, Kiel C, eds. Healthcare Information Management
Systems: Cases, Strategies, and Solutions. Cham, Switzerland:
Springer; 2016
25 Pifer R. Q&A: ONC chief Don Rucker on bringing the app economy
into healthcare. HealthcareDive. Accessed September 15, 2019 at:
https://www.healthcaredive.com/news/qa-onc-chief-donrucker-on-bringing-the-app-economy-into-healthcare/561436/
26 Singhal S, Carlton S. The era of exponential improvement in health-
care? Accessed September 26, 2019 at: https://www.mckinsey.com/
industries/healthcare-systems-and-services/our-insights/the-era-ofexponential-improvement-in-healthcare
27 Kim KK, Jalil S, Ngo V. Improving Self-Management and Care
Coordination with Person-Generated Health Data and Mobile
Health. Consumer Informatics and Digital Health: Solutions for
Health and Healthcare. Berlin, Germany: Springer; 2019
28 Wilbanks J. Ethical issues in consumer informatics and online
content. InEdmunds M, Hass C, Holve E, eds. Consumer Informatics
andDigital Health: Solutions for Health andHealthcare. Berlin,
Germany: Springer; 2019
29 LexisNexis. Socioeconomic health score. Accessed September 15,
2019at: https://risk.lexisnexis.com/products/socioeconomic-healthscore
30 LexisNexis. Socioeconomic health attributes for providers. Accessed
September 15, 2019 at: https://risk.lexisnexis.com/products/socioeconomic-health-attributes
31 AllenM. Health insurers are vacuuming up details about you—and
it could raise your rates. ProPublica. Accessed September 14, 2019
at: https://www.propublica.org/article/health-insurers-are-vacuuming-up-details-about-you-and-it-could-raise-your-rates
32 Welltok. Total wellbeing solution for employers. Accessed on
September 15, 2019 at: http://www.welltok.com/employers/
33 Petersen C. User-focused data sharing agreements: a foundation
for the genomic future. JAMIA Open 2019;2(04):402–406
34 Haug CJ. Whose data are they anyway? Can a patient perspective
advance the data-sharing debate?. N Engl J Med 2017;376(23):
2203–2205
35 Kaiser Family Foundation. Self-Funded Plans: 2018 Employer
Health Benefits Survey. Accessed on September 14, 2019 at:
https://www.kff.org/report-section/2018-employer-health-benefits-survey-summary-of-findings/
36 Eltorai AE, Naqvi SS, Ghanian S, et al. Readability of invasive
procedure consent forms. Clin Transl Sci 2015;8(06):830–833
37 Perrenoud B, Velonaki VS, Bodenmann P, Ramelet AS. The effectiveness
of health literacy interventions on the informed consent
process of health care users: a systematic review protocol. JBI
Database Syst Rev Implement Reports 2015;13(10):82–94
38 Foe G, Larson EL. Reading level and comprehension of research
consent forms: an integrative review. J Empir Res Hum Res Ethics
2016;11(01):31–46
39 Fowler LR, Gillard C, Morain SR. Readability and accessibility of
terms of service and privacy policies for menstruation-tracking
smartphone applications. Health Promot Pract 2020;21(05):
679–683
40 Downing A. Howa patient advocate discovered amassive security
problemwith closed groups on Facebook and became awhite hat
hacker. Accessed September 1, 2019 at: https://bravebosom.org/
2018/07/04/sicgrl/
41 Federal Trade Commission. United States of America v. Facebook,
Inc., a corporation. Accessed September 1, 2019 at: https://www.
ftc.gov/enforcement/cases-proceedings/092-3184/facebook-inc
42 Schechner S, Secada M. You give apps sensitive personal information.
Then they tell Facebook. The Wall Street Journal. Accessed
September 15, 2019 at: https://www.wsj.com/articles/you-giveapps-sensitive-personal-information-then-they-tell-facebook-
11550851636
43 Farr C. Facebook sent a doctor on a secret mission to ask hospitals
to share patient data. CBNC. Accessed September 15, 2019 at:
https://www.cnbc.com/2018/04/05/facebook-building-8-explored-data-sharing-agreement-with-hospitals.html
44 Rocher L, Hendrickx JM, deMontjoye YA. Estimating the success of
re-identifications in incomplete datasets using generative models.
Nat Commun 2019;10(01):3069
45 Abnousi F, Rumsfeld JS, Krumholz HM. Social determinants of
health in the digital age: determining the source code for nurture.
JAMA 2019;321(03):247–248
46 Estabrooks PA, Boyle M, Emmons KM, et al. Harmonized patientreported
data elements in the electronic health record: support-
ing meaningful use by primary care action on health behaviors
and key psychosocial factors. J Am Med Inform Assoc 2012;19
(04):575–582
47 Hewitt C, Lloyd KC, Tariq S, et al. Patient-generated data in the
management of HIV: a scoping review. BMJ Open 2021;11(05):
e046393
48 Iott BE, Campos-Castillo C, Anthony DL. Trust and privacy: how
patient trust in providers is related to privacy behaviors and
attitudes. AMIA Annu Symp Proc 2020;2019:487–493
49 Peek ME, Gorawara-Bhat R, Quinn MT, Odoms-Young A, Wilson
SC, Chin MH. Patient trust in physicians and shared decisionmaking
among African-Americans with diabetes. Health Com-
mun 2013;28(06):616–623
50 Ruotsalainen P, Blobel B. Health information systems in the digital
health ecosystem—problems and solutions for ethics, trust and
privacy. Int J Environ Res Public Health 2020;17(09):3006
51 Bietz MJ, Bloss CS, Calvert S, et al. Opportunities and challenges in
the use of personal health data for health research. J Am Med
Inform Assoc 2016;23(e1):e42–e48
52 Zhu H, Colgan J, Reddy M, Choe EK. Sharing patient-generated
data in clinical practices: an interview study. AMIA Annu Symp
Proc 2017;2016:1303–1312
53 Isakadze N, Martin SS. How useful is the smartwatch ECG? Trends
Cardiovasc Med 2019
54 Rajakariar K, Koshy AN, Sajeev JK, Nair S, Roberts L, Teh AW.
Accuracy of a smartwatch based single-lead electrocardiogram
device in detection of atrial fibrillation. Heart 2020;106(09):
665–670
55 Bari L, O’Neill DP. Rethinking patient privacy in the era of digital
health. Accessed May 23, 2021 at: https://www.healthaffairs.org/do/10.1377/hblog20191210.216658/full/
56 Rosenbloom ST, Smith JRL, Bowen R, Burns J, Riplinger L, Payne
TH. Updating HIPAA for the electronic medical record era. J Am
Med Inform Assoc 2019;26(10):1115–1119
57 LeomMD, Choo KR, Hunt R. Remotewiping and secure deletion on
mobile devices: a review. J Forensic Sci 2016;61(06):1473–1492
58 Office of the National Coordinator for Health Information Technology
Application Programming Interface (API) Task Force. API
Task Force Recommendations. Accessed September 15, 2019 at:
https://www.healthit.gov/sites/default/files/facas/SingleSourceofTruth-APITFRecom-
mendations.pdf
59 Food and Drug Administration. Policy for Device Software Functions
andMobileMedical Applications: Guidance for Industry and
Food and Drug Administration Staff. Accessed September 15,
2019 at: https://www.fda.gov/media/80958/download
60 McGrawD, Petersen C. Fromcommercialization to accountability:
responsible health data collection, use, and disclosure for the 21st
century. Appl Clin Inform 2020;11(02):366–373
61 Office of the National Coordinator for Health Information Technology.
Permitted Uses and Disclosures: Exchange for Health Care
Operations. Accessed September 15, 2019 at: https://www.
healthit.gov/sites/default/files/exchange_health_care_ops.pdf
ACI Open Vol. 5 No. 2/2021 © 2021. The Author(s).
Shared Decision-Making through the Use of Person-Generated Health Data Petersen et al.e114
#
#62 Office of the National Coordinator for Health Information Technology.
Permitted Uses and Disclosures: Exchange for Treatment.
Accessed September 15, 2019 at: https://www.healthit.gov/sites/default/files/exchange_treatment.pdf
63 Office of the National Coordinator for Health Information Tech-
nology. Permitted Uses and Disclosures: Exchange for Public
Health Activities. Accessed September 15, 2019 at: https://
www.healthit.gov/sites/default/files/12072016_hipaa_and_public_health_fact_sheet.pdf
64 Office of the National Coordinator for Health Information Tech-
nology. 2016Report to Congress onHealth IT Progress: Examining
the HITECH Era and the Future of Health IT. Accessed September
15, 2019 at: https://dashboard.healthit.gov/report-to-con-
gress/2016-report-congress-examining-hitech-era-futurehealth-information-technology.php#executive-summary
65 Adler-Milstein J, Pfeifer E. Information blocking: is it occurring
and what policy strategies can address it? Milbank Q 2017;95
(01):117–135
66 Lye CT, Forman HP, Gao R, et al. Assessment of US hospital
compliance with regulations for patients’ requests for medical
records. JAMA Netw Open 2018;1(06):e183014
67 Health and Human Services. OCR Settles First Case in HIPAA Right
of Access Initiative. Accessed September 9, 2019 at: https://www.
hhs.gov/hipaa/for-professionals/compliance-enforcement/agreements/bayfront/index.html
68 Ciitizen. The Patient Record Scorecard:What It Is andWhyWe Did
It. Accessed September 29, 2019 at: https://www.ciitizen.com/thepatient-record-scorecard-what-is-it-and-why-we-did-it/
69 Office of the National Coordinator for Health Information Tech-
nology. ONC Interim Final Rule. Accessed May 23, 2021 at:
https://www.healthit.gov/cures/sites/default/files/cures/202010/IFC_FactSheet_Information_Blocking.pdf
70 Office of the National Coordinator for Health Information Technol-
ogy. Patient-Generated Health Data. March 2014. Accessed September
18, 2019 at: https://www.healthit.gov/sites/default/files/
patient_generated_data_factsheet.pdf
71 Elwyn G, Edwards A, Kinnersley P, Grol R. Shared decisionmaking
and the concept of equipoise: the competences of involving
patients in healthcare choices. Br J Gen Pract 2000;50(460):
892–899
72 Federal Trade Commission. What To Do Right Away. Accessed
September 15, 2019 at: https://www.identitytheft.gov/Steps
73 Gillum J, Kao J, Larson J. Millions of American’s Medical images
and data are available on the internet. Anyone can take a peek.
ProPublica. Accessed September 17, 2019 at: https://www.
propublica.org/article/millions-of-americans-medical-imagesand-data-are-available-on-the-internet
74 Accenture. One in FourUS ConsumersHaveHad Their Healthcare Data
Breached, Accenture Survey Reveals. Accessed on September 16, 2019
at:https://newsroom.accenture.com/subjects/technology/one-in-fourus-consumers-have-had-their-healthcare-data-breached-accenture-
survey-reveals.htm
75 Ponemon Institute. Fifth Annual Study on Medical Identity Theft.
Accessed September 18, 2019 at: http://www.medidfraud.org/wp-content/uploads/2015/02/2014_Medical_ID_-
Theft_Study1.pdf
ACI Open Vol. 5 No. 2/2021 © 2021. The Author(s).
Shared Decision-Making through the Use of Person-Generated Health Data Petersen et al. e115
Journal of Strategic Security
Volume 8
Number 3 Volume 8, No. 3, Special Issue Fall
2015: Intelligence: Analysis, Tradecraft,
Training, Education, and Practical Application
Article 7
The Cyber Intelligence Challenge of
Asyngnotic Networks
Edward M. Roche
Columbia Institute for Tele-Information, Columbia University, emr96@columbia.edu
Michael J. Blaine
John McCreary
Defense Intelligence Agency (ret.)
Follow this and additional works at: http://scholarcommons.usf.edu/jss
pp. 107-136
This Article is brought to you for free and open access by the USF Libraries at Scholar Commons. It has been accepted for inclusion in
Journal of Strategic Security by an authorized administrator of Scholar Commons. For more information, please contact
scholarcommons@usf.edu.
Recommended Citation
Roche, Edward M.; Blaine, Michael J.; and McCreary, John. "The Cyber Intelligence Challenge of Asyngnotic Networks."
Journal of Strategic Security 8, no. 3 (2015): 107-136.
Available at: http://scholarcommons.usf.edu/jss/vol8/iss3/7
#
#The Cyber Intelligence Challenge of Asyngnotic Networks
Abstract
The intelligence community is facing a new type of organization, one enabled by the world’s
information and communications infrastructure. These asyngnotic networks operate without
leadership and are self-organizing in nature. They pose a threat to national security because they are
difficult to detect in time for intelligence to provide adequate warning. Social network analysis and
link analysis are important tools but can be supplemented by application of neuroscience principles to
understand the forces that drive asyngnotic self-organization and triggering of terrorist events.
Applying Living Systems Theory (LST) to a terrorist attack provides a useful framework to identify
hidden asyngnotic networks. There is some antecedent work in propaganda analysis that may help
uncover hidden asyngnotic networks, but computerized SIGINT methods face a number of
challenges.
This article is available in Journal of Strategic Security: http://scholarcommons.usf.edu/jss/vol8/iss3/7
#
# 
 
107 
 
Introduction 
In June of 2014, someone leaked to the press that the infamous computer 
hacking group, Anonymous, was preparing to start operation NO2ISIS to 
strike against supporters of the Islamic State of Syria and al-Sham (ISIS).1   
The anonymous source stated:  
 
“We plan on sending a straightforward message to Turkey, Saudi 
Arabia, Qatar and all other countries that evidently supply ISIS for 
their own gain,” the source said. “In the next few days we will begin 
defacing the government websites of these countries so that they 
understand this message clearly. We are unable to target ISIS because 
they predominately fight on the ground. But we can go after the people 
or states who fund them.”2  
 
One of the motivating factors behind this attack appears to be that the Twitter 
feed @theanonmessage had been taken over by ISIS and used to distribute 
graphic images of violence.  On January 22, 2015, it was reported that the 
Anonymous “Red Cult Team” had taken down ISIS websites in response to 
the murder of the Charlie Hebdo journalists in Paris.  An Anonymous twitter 
feed @OpCharlieHebdo confirmed that the website ansar-alhaqq.net had 
been “taken down,”3 as well as other ISIS websites.4  
 
This faceless cyber-war quickly escalated. By February 8, 2015, Anonymous5 
reported that it had taken control of “dozens of Twitter and Facebook 
                                                     
1   Hamill, J., “Anonymous hacktivists prepare for strike against ISIS ‘supporters,’” (2014) 
Forbes, available at: 
http://www.forbes.com/sites/jasperhamill/2014/06/27/anonymous-hacktivistsprepare-for-strike-against-isis-supporters/.
2   A full text of the Anonymous statement is at the end of this paper. 
3   Vandita, 2015a, Anonymous takes down ISIS websites, confirms leaked government 
documents were real, We Are Anonymous website, available at: 
http://www.anonhq.com.  
4    The list was included in a number of tweets: “#Target joinalqarda.com 144.76.97.176 
#TANGODOWN; #Target alintibana.net 144.76.97.176 #TANGODOWN; #Target 
opcharliehebdo.com 104.28.7.87 (Imposter Website)#TANGODOWN; #Target 
islaam.com 97.74.45.128 #TANGODOWN; #Target Qa3edon.100free.com 
205.134.165.186 #TANGODOWN; #Target daulahisamiyah.net 119.81.24.187 
#TANGODOWN; #Target ansar1.info 79.172.193.108 #TANGODOWN; and #Target 
jhuf.net 104.28.20.19 #TAN- GODOWN”. Note: The term “#TANGODOWN” is 
Anonymous reporting that the site had been taken down.  One assumes that “tango” is the 
call sign for the letter “T” abbreviating “taken”.  
5   Anonymous is a loosely associated international network of activist and hacktivist 
entities.   It does not have a leadership structure.  
Roche et al.: Cyber Intelligence Challenge
Produced by The Berkeley Electronic Press, 2015
#
# 
 
108 
 
accounts” that had been used by ISIS to spread their influence through social 
media.6  Anonymous released a video on YouTube openly threatening ISIS:  
 
“We will hunt you, take down your sites, accounts, emails, and expose 
you. From now on, no safe place for you online . . . You will be treated 
like a virus, and we are the cure . . . We own the Internet . . . We are 
Anonymous; we are Legion; we do not forgive, we do not forget, Expect 
us.”7  
 
It then identified 90 twitter accounts and 12 Facebook accounts that “appear 
to be in close contact with ISIS”.  Shortly after this information was released, 
both Twitter and Facebook took down the accounts.  If this had been a kinetic 
war, these actions would have been equivalent to severely degrading 
command and control by taking out a major telecommunications center.   
 
Only a few days later, February 19th, 2015, Anonymous reported that its 
#OpISIS operation was in “Round 2” and had “exposed thousands of ISIS 
accounts to show that it is not that difficult to fight back against ISIS online”8 
(emphasis added). It also provided a list of 3,030 accounts that it had handed 
over to Twitter with the message “do your job” [and suspend the accounts].9   
Anonymous then released a video stating that its actions were “to show what 
your governments are not doing” (emphasis added). By February 25, it was 
reporting on “#OpISIS Round 4” with the headline “Anonymous Beats United 
States in Combating Terrorism”. Then in a stunning move, it released for 
anyone wishing to view it, a list containing the contact information, IDs, 
usernames, passwords and other information for all members of a leading 
ISIS website.10  
 
                                                     
6   CoNN, 2015. Anonymous “hacktivists” strike a blow against ISIS. We Are Anonymous 
website www.anonhq.com; Blair, L., 2015. Anonymous says ISIS is not Muslim, hacks 
hundreds of ISIS accounts online; says ‘we are Muslims, we are Christians’. The Christian 
Post , np. 
7   Anonymous, 2015. Anonymous #OpISIS continues. YouTube video posting.  
8   Vandita, 2015b. #OpISIs Round 2: Anonymous hacks thousands of ISIS accounts. We 
Are Anonymous website www.anonhq.com. 
9   David, Z., 2015. Anonymous hacks more ISIS accounts than ever, after U.S. 
government and Twitter refuse to act. Counter Current News , np.  
10    You can view the list at: http://http://pastebin.com/rbq8s4GM. The data included 
“id”, “username”, “password”, “category”, “name”, “timeslogin”, “refered”, “credits”, 
“sizedownloaded”, “expirydate”, “registrationdate” for a large number of accounts. See 
Team, A.R.C., 2015, #OpISIS – database hacked by Anonymous Red Cult Team. 
Pastebin.com.  
Journal of Strategic Security, Vol. 8  No. 3
http://scholarcommons.usf.edu/jss/vol8/iss3/7
DOI: http://dx.doi.org/10.5038/1944-0472.8.3.1448
#
# 
 
109 
 
It appears that the #OpISIS operation conducted by Anonymous was a 
success. These operations were conducted with a large amount of technical 
skill, and have been timely. But who is doing the work? Who is Anonymous? 
How is it organized, and are there other groups operating in the same way? 
Can such an organization wage a cyber war more effectively than the world’s 
nation-states? Has it been more effective than the U.S. Cyber Command or its 
counterpart in other countries?11?  
 
This paper argues that the intelligence community is facing a new type of 
organization, one enabled by the world’s information and communications 
infrastructure, and not having the traditional characteristics of any 
organization known before.  These networks of persons operate without 
leadership.  They communicate using both controlled and open un-controlled 
paths for handling information, and they organize themselves in an 
unconscious (un-planned) way.  They are self-organizing networks.  What is 
peculiar is that these networks, such as Anonymous, appear to be highly 
effective, but do not share any of the characteristics of a typical organization.12   
That is, the individual components of a legacy terrorist network such as an 
identifiable leadership structure for command and control is not present.  
 
Apart from Anonymous, we can see this type of organization in other places as 
well.  It is present in broad social movements such as Occupy Wall Street, the 
Arab Spring, Internet policy coalitions, and anti-globalization drives.  It is 
apparent in ISIS recruiting through social media, where we see what appear 
to be random youth in developed Western countries give up their entire life 
and manner of living to join the caliphate and its uncertain future.13  No one 
yet fully understands how these convincing communications take place and 
why unexpected recruits from around the world are joining the caliphate to 
fight, even without explicit orders.  This is because the ISIS phenomenon is 
working in the same self-organizing way.14  
 
                                                     
11   The United States Cyber Command (USCYBERCOM) is an armed forces sub-unified 
command subordinate to the United States Strategic Command.   It is located in Fort 
Meade, Maryland at NSA.  
12   See the related discussion in Cetina (2005) who writes about “microstructures” 
forming networks.  
13   The ISIS phenomenon outside of Iraq and Syria is operating in an asyngnotic way; but 
at home, ISIS is operating with a typical organizational form.  
14   ISIS as an organization operating in Iraq and Syria has a classical organizational form, 
e.g., a leader, assistants, functional specialization, command and control.  It is the wider 
ISIS phenomenon that is characterized by asyngnosis and asyngnotic behavior. 
Roche et al.: Cyber Intelligence Challenge
Produced by The Berkeley Electronic Press, 2015
#
# 
 
110 
 
In order to examine this type of shadowy organization, we propose a simple 
framework that we call ‘Asyngnosis’.  The word is constructed from Greek: α- 
(not) + συνειδιτος- (conscious) + γνοσι- (knowledge, information); and 
describes the undirected emergence of knowledge and other interconnected 
pathways that form around a specific idea or activity.  We would argue that an 
‘Asyngnodic’ (+ δικτυο - grid or network) (an asyngnotic network) may 
describe many complex organizational activities, particularly decision-making 
and operations.  Decisions and strategies may be modeled as not the outcome 
of a complex, structured set of discrete activities or processes, but instead as 
the product of continuous (non-discrete) flows of information, ideas and 
impressions (‘memes’)15 along ever-changing communication pathways tying 
together individuals and organizations.  Much like the pathways between 
neurons in the brain, these networks are characterized by their constant 
formation, strengthening, weakening, and disappearance based on use and 
need, yet as these connections and disconnections take place, the organization 
itself constantly changes.16     
 
Both the continuous flow of memes and the constant reconfiguration of these 
networks takes place not only between individuals, but between individuals 
and organizations, and between the organizations themselves without regard 
to national, cultural, or even linguistic boundaries.  Thus, in its simplest form, 
Asyngnosis combines four concepts: a) memes (information, concepts, 
impressions); b) a continuous nature (non-discrete events); c) networks 
(communication paths; influence paths; sensory paths); d) plasticity 
(spontaneous reconfigurablity) and ad hoc formation and dissolution).  
 
In the remainder of this article, we will review several approaches used in 
intelligence to analyze networks of criminals, terrorists or other persons or 
organizations who threaten national security.  We will show that these 
techniques, although powerful, fail to address the specific challenges of the 
asyngnotic form of network.  Next, we will discuss the nature of asyngnotic 
behavior.  After that, we will present a short example of how an asyngnotic 
network approach could be used to understand the Charlie Hebdo terrorist 
attack in Paris.  Here we use a parallel in Living Systems Theory (LST) to 
make an initial identification of unseen networks.  LST is reviewed briefly in 
the appendix.  
                                                     
15    Ferguson (Ferguson, N., “Networks and hierarchies,” The American Interest (2014) 9, 
16–24) also mentions memes.  
16   To see this pattern in the activities of Anonymous or other hacker groups, it is only 
necessary to examine the shifting pattern of content over the years.  
Journal of Strategic Security, Vol. 8  No. 3
http://scholarcommons.usf.edu/jss/vol8/iss3/7
DOI: http://dx.doi.org/10.5038/1944-0472.8.3.1448
#
# 
 
111 
 
 
We will then discuss the intelligence challenges for automated collection and 
analysis of intelligence data needed to anticipate this type of event. Much of 
our approach is based on neuroscience, because some of its underlying theory 
regarding the brain, including quantitative measurements of action 
potentials, might be used to model the information flow and SIGINT 
characteristics of self-learning leaderless networks.  
 
Current Intelligence Approaches  
Theory of Social Networks  
The analysis of social networks has a rich history starting with the work of 
Ëmile Durkheim, considered to be the father of sociology.  Some of the 
earliest work in diagramming and network analysis focused on derivation of a 
social structure.  It was considered that “the dynamic meaning of chainrelations
in social structure is better understood in view of a network 
hypothesis.”17  Early studies examined how information is passed through 
social networks18 and these models also were useful for understanding 
diffusion of innovation, spread of a disease, cognitive social structures,19 the 
abstract idea of social capital,20 and even sexual behavior.21  Study of the 
strength of ties between persons was found to influence how well a message 
could be propagated (wider propagation with weaker ties).22  Over time, these 
methods became computerized, and the rise of social network analysis may 
have come with the rise of social media.  
 
Analysis of Social Media  
The rapid growth of social media has provided an important platform for 
social network analysis, because an adequate amount of information is 
available openly online.  These giant social networks are easy to view because 
                                                     
17   Moreno, J.L., Jennings, H.H., “Statistics of social configurations,” Sociometry (1938) 
1, pp. 342–374. These first social networks are literally hand-drawn in the journals, very 
different from today’s outputs from computerized mapping software.  
18   Rapoport, A., “Spread of information through a population with socio- structural bias: 
I. assumption of transitivity,” The Bulletin of Mathematical Biophysics (1953) 15, 523–
533; Rapoport, A., “A study of a large sociogram,” Behavioral Science (1961) 6, 279–291. 
19   Krackhardt, D., “Cognitive social structures,” Social Networks (1987) 9, 109 – 134. 
20   Burt, R.S., “The contingent value of social capital,” Administrative Science Quarterly 
(1997) 42, 339. 
21   Laumann, E.O., “A 45-year retrospective on doing networks,” Connections (2006) 27, 
65–90.  
22   Granovetter, M.S., “The strength of weak ties,” American Journal of Sociology (1973) 
78, 1360–1380.  
Roche et al.: Cyber Intelligence Challenge
Produced by The Berkeley Electronic Press, 2015
#
# 
 
112 
 
automation can be used to collect massive amounts of data.  Indeed, social 
networks such as Facebook, Google+, or LinkedIn have publicly visible data 
that reveals every “friend” a person is connected to as well as a record of their 
interactions and indicators of similarity.  Using this information, it is possible 
automatically to data-mine social networks and correlate the derived network 
structure surrounding an individual or group with other known information 
describing the participating individuals, e.g., preferences, location, sex, 
etcetera.  For example, many have studied how social media can be used by 
businesses to develop next generation products23 or assess whether a product 
is appropriate to sell online24.  Other applications include customer 
relationship management,25 identification of promising investments,26 and 
identification of stakeholder groups that may influence corporate policy.27   
Because of its effect on how consumers purchase online,28 much work has 
been done on recommendation systems,29 word-of-mouth evaluations,30 
                                                     
23   Li, Y.M., Chen, H.M., Liou, J.H., Lin, L.F., “Creating social intelligence for product 
portfolio design,” Decision Support Systems (2014) 66, 123 – 134; Lau, R.Y., Li, C., Liao, 
S.S., “Social analytics: Learning fuzzy product ontologies for aspect-oriented sentiment 
analysis,” Decision Support Systems (2014) 65, 80 – 94. Crowdsourcing and Social 
Networks Analysis. 
24   Verbraken, T., Goethals, F., Verbeke, W., Baesens, B., “Predicting online channel 
acceptance with social network data,” Decision Support Systems 63 (2014) 104 – 114. 1. 
Business Applications of Web of Things 2. Social Media Use in Decision Making. 
25   van Dam, J.W., van de Velden, M., “Online profiling and clustering of Facebook 
users,” Decision Support Systems (2015) 70, 60 – 72. 
26   Gottschlich, J., Hinz, O., “A decision support system for stock investment 
recommendations using collective wisdom,” Decision Support Systems (2014) 59, 52 – 
62. 
27   Jiang, S., Chen, H., Nunamaker, J.F., Zimbra, D., “Analyzing firm-specific social 
media and market: A stakeholder-based event analysis framework,” Decision Support 
Systems (2014) 67, 30 – 39. 
28   Gao, J., Zhang, C., Wang, K., Ba, S., “Understanding online purchase decision making: 
The effects of unconscious thought, information quality, and information quantity,” 
Decision Support Systems (2012) 53, 772 – 781. 1) Computational Approaches to 
Subjectivity and Sentiment Analysis 2) Service Science in Information Systems Research : 
Special Issue on {PACIS} 2010.  
29   Li, X., Wang, M., Liang, T.P., “A multi-theoretical kernel-based approach to social 
network-based recommendation,” Decision Support Systems (2014) 65, 95 – 104. 
Crowdsourcing and Social Networks Analysis.; Geiger, D., Schader, M., “Personalized task 
recommendation in crowdsourcing information systems — current state of the art,” 
Decision Support Systems (2014) 65, 3 – 16. Crowdsourcing and Social Networks 
Analysis.; Liao, H.Y., Chen, K.Y., Liu, D.R., “Virtual friend recommendations in virtual 
worlds,” Decision Support Systems (2015) 69, 59 – 69.  
30   Zhang, Z., Li, Q., Zeng, D., Gao, H., “User community discovery from multi-relational 
networks,” Decision Support Systems (2013) 54, 870 – 879.; Chang, H.H., Tsai, Y.C., 
Wong, K.H., Wang, J.W., Cho, F.J., “The effects of response strategies and severity of 
failure on consumer attribution with regard to negative word-of-mouth,” Decision 
Support Systems (2015) 71, 48 – 6.; Bao, T., Chang, T.L., “Finding disseminators via 
electronic word of mouth message for effective marketing communications,” Decision 
 
Journal of Strategic Security, Vol. 8  No. 3
http://scholarcommons.usf.edu/jss/vol8/iss3/7
DOI: http://dx.doi.org/10.5038/1944-0472.8.3.1448
#
# 
 
113 
 
product reviews,31 and the effects of social media on the reputation of a 
business32 or public opinion in general.33   Some work has attempted to 
explore social media as a prediction market.34  
 
Much of this work is done using text mining to uncover information about 
social media users.35  It has been recognized that social media may have a 
strong effect on the behavior of individuals.  Some have explored nonconscious
intentions,36 why people keep coming back to social media,37 
psychological addiction,38 and maladaptive cognition.39  Individuals exhibit 
                                                                                                                                                 
Support Systems (2014) 67, 21 – 29.; Cheung, C.M., Thadani, D.R., “The impact of 
electronic word-of-mouth communication: A literature analysis and integrative model,” 
Decision Support Systems (2012) 54, 461 – 470. 
31   Bao, T., Chang, T.L., “Finding disseminators via electronic word of mouth message for 
effective marketing communications,” Decision Support Systems (2014a) 67, 21 – 29.; 
Yu, H., Shen, Z., Miao, C., An, B., Leung, C., “Filtering trust opinions through 
reinforcement learning,” Decision Support Systems (2014) 66, 102 – 113.; Ku, Y.C., Wei, 
C.P., Hsiao, H.W., “To whom should I listen? Finding Reputable Reviewers in Opinionsharing
Communities,” Decision Support Systems (2012) 53, 534 – 542. 
32    Vavilis, S., Petkovi ́c, M., Zannone, N., “A Reference Model for Reputation Systems,” 
Decision Support Systems (2014) 61, 147 – 154.; Schniederjans, D., Cao, E.S., 
Schniederjans, M., “Enhancing Financial Performance with Social Media: An Impression 
Management Perspective,” Decision Support Systems (2013) 55, 911 – 918. 1. Social 
Media Research and Applications 2. Theory and Applications of Social Networks.; da 
Silva, N.F., Hruschka, E.R., Jr., E.R.H., “Tweet Sentiment Analysis with Classifier 
Ensembles,” Decision Support Systems (2014) 66, 170 – 179. 
33   Tian, R.Y., Liu, Y.J., “Isolation, Insertion, and Reconstruction: Three Strategies to 
Intervene in Rumor Spread Based on Supernetwork Model,” Decision Support Systems 
(2014) 67, 121 – 130. 
34   Qiu, L., Rui, H., Whinston, A., “Social Network-Embedded Prediction Markets: The 
Effects of Information acquisition and communication on predictions,” Decision Support 
Systems (2013) 55, 978 – 987. 1. Social Media Research and Applications 2. Theory and 
Applications of Social Networks. 
35   Lu, H.M., “Detecting short-term cyclical topic dynamics in the user- generated content 
and news,” Decision Support Systems (2015) 70, 1 – 14.  
36    Zhao, K., Stylianou, A.C., Zheng, Y., “Predicting users’ continuance intention in 
virtual communities: The dual intention-formation processes,” Decision Support 
Systems, (2013) 55, 903 – 910. 1. Social Media Research and Applications 2. Theory and 
Applications of Social Networks.  
37   Gwebu, K.L., Wang, J., Guo, L., “Continued usage intention of multi- functional friend 
networking services: A test of a dual-process model using Facebook,” Decision Support 
Systems (2014) 67, 66 – 77.; Al-Debei, M.M., Al-Lozi, E., Papazafeiropoulou, A., “Why 
people keep coming back to Facebook: Explaining and predicting continuance 
participation from an extended theory of planned behavior perspective,” Decision 
Support Systems (2013) 55, 43 – 54.; Sun, Y., Fang, Y., Lim, K.H., “Understanding 
sustained participation in transactional virtual communities,” Decision Support Systems 
(2012) 53, 12 – 22; Cheung, C.M., Lee, M.K., “A theoretical model of intentional social 
action in online social networks,” Decision Support Systems  (2010) 49, 24 – 30. 
38    Wang, C., Lee, M.K., Hua, Z., “A theory of social media dependence: Evidence from 
microblog users,” Decision Support Systems (2015) 69, 40 – 49.  
39  Ibid. 
Roche et al.: Cyber Intelligence Challenge
Produced by The Berkeley Electronic Press, 2015
#
# 
 
114 
 
different leadership styles in online communities,40 they disclose probably too 
much information about themselves,41 and use different habits in making 
decisions.42  Content analysis can be used to discover a variety of social roles 
assumed online.43  Some work has used automated tools to examine the 
effects and operations of social networks including allocation of workflows,44 
span of control as a function of trust,45 and the functions of online support 
communities.46   
 
Finally, a significant amount of work has been done in developing automated 
tools for exploration of the structure of social networks.  Zhang47 used authortopic
data to derive multi-relational networks in social media, and Zhou48 
showed it is possible to discover implicit social networks.  Han49 was able to 
mine Facebook data and uncover individual preferences and interests.  Other 
work in data mining50 has focused on identification of friendships,51 
inferences of shared interests between individuals,52 identification of 
gatekeepers and subgroups,53 organizational structure54 and knowledge 
                                                     
40   Templeton, G.F., Luo, X.R., Giberson, T.R., Campbell, N., “Leader personal influences 
on membership decisions in moderated online social networking groups,” Decision 
Support Systems (2012) 54, 655 – 664. 
41   Chen, R., “Living a private life in public social networks: An exploration of member 
self-disclosure,” Decision Support Systems (2013) 55, 661 – 668.  
42   Sadovykh, V., Sundaram, D., Piramuthu, S., “Do online social networks support 
decision-making?” Decision Support Systems (2015) 70, 15 – 30 
43   Lee, A.J., Yang, F.C., Tsai, H.C., Lai, Y.Y., “Discovering content-based behavioral roles 
in social networks,” Decision Support Systems, (2014) 59, 250 – 261. 
44   Bajaj, A., Russell, R., “Awsm: Allocation of workflows utilizing social network 
metrics,” Decision Support Systems (2010) 50, 191 – 202. 
45   Salas-Fumás, V., Sanchez-Asin, J.J., “Information and trust in hierarchies,” Decision 
Support Systems (2013) 55, 988 – 999. 1. Social Media Research and Applications 2. 
Theory and Applications of Social Networks.  
46   Sutanto, J., Kankanhalli, A., Tan, B.C., “Uncovering the relationship between user 
support networks and popularity,” Decision Support Systems, (2014) 64, 142 – 151 
47   Zhang, Z., Li, Q., Zeng, D., Gao, H., “User community discovery from multi-relational 
networks,” Decision Support Systems (2013) 54, 870 – 879.  
48   Zhou, W., Duan, W., Piramuthu, S., “A social network matrix for implicit and explicit 
social network plates,” Decision Support Systems (2014) 68, 89 – 97.  
49   Han, X., Wang, L., Crespi, N., Park, S., Cuevas, A., “Alike people, alike interests? 
Inferring interest similarity in online social networks,” (2015) Decision Support Systems 
69, 92 – 106. 
50   Chen, Y.L., Wu, Y.Y., Chang, R.I., “From data to global generalized knowledge,” 
Decision Support Systems (2012) 52, 295 – 307.  
51   Liao, H.Y., Chen, K.Y., Liu, D.R., “Virtual friend recommendations in virtual worlds,” 
Decision Support Systems (2015) 69, 59 – 69  
52    Han, X., Wang, L., Crespi, N., Park, S., Cuevas, A., “Alike people, alike interests? 
Inferring interest similarity in online social networks,” Decision Support Systems (2015) 
69, 92 – 106.  
53    Zhu, B., Watts, S., Chen, H., “Visualizing social network concepts,” Decision Support 
Systems (2010) 49, 151 – 161.   
Journal of Strategic Security, Vol. 8  No. 3
http://scholarcommons.usf.edu/jss/vol8/iss3/7
DOI: http://dx.doi.org/10.5038/1944-0472.8.3.1448
#
# 
 
115 
 
flows.55  Although these techniques of analysis at first were developed in the 
commercial sector, particularly advertising, some of the underlying 
methodologies have turned out to be useful frameworks for intelligence, but 
using an extended data set based on SIGINT.  
 
Application of Network Analysis – Link Analysis and Complex 
Correlation  
As a type of applied social network theory, link analysis has been widely 
exploited in intelligence analysis.56  Although earlier intelligence work was 
done manually,57 in criminal intelligence, the concept of network centrality 
has be used to identify vulnerabilities in criminal organization,58 identify false 
identities,59 and find hidden networks and web communities.60  Link analysis 
using a variety of attributes of hate group web sites has been used to map 
affiliations between different groups of radicals,61 guerrillas62 and terrorists.63   
There are important applications in combatting identity theft.64   There are 
                                                                                                                                                 
54   Qiu, J., Lin, Z., “A framework for exploring organizational structure in dynamic social 
networks,” Decision Support Systems (2011) 51, 760 – 771. Recent Advances in Data, 
Text, and Media Mining; Information Issues in Supply Chain and in Service System 
Design.  
55   Liu, D.R., Lin, C.W., Chen, H.F., “Discovering role-based virtual knowledge flows for 
organizational knowledge support,” Decision Support Systems, (2013) 55, 12 – 30.  
56   Senator, T.E., “Link mining applications: Progress and challenges,” (2005) SIGKDD 
Explor. Newsl. 7, 76–83.  
57   Harper, W.R., Harris, D.H., “The application of link analysis to police intelligence,” 
Human Factors (1975) 17, 157–164; Chen, H., Chung, W., Xu, J.J., Wang, G., Qin, Y., 
Chau, M., “Crime data mining: a general framework and some examples,” Computer 
(2004) 37, 50– 56. 
58   Sparrow, M.K., “The application of network analysis to criminal intelligence: An 
assessment of the prospects,” Social Networks, (1991) 13, 251 – 274 
59   Boongoen, T., Shen, Q., Price, C., “Disclosing false identity through hybrid link 
analysis,” Artificial Intelligence and Law (2010) 18, 77–102 
60   Reid, E., 2003. “Using web link analysis to detect and analyze hidden web 
communities.” Information and communications technology for competitive 
intelligence, (2003) 57–84.  
61   Zhou, Y., Reid, E., Qin, J., Chen, H., Lai, G., “US domestic extremist groups on the 
web: link and content analysis,” Intelligent Systems, (2005) IEEE 20, 44–51. 
62   Grau, L.W., “Something Old, Something New. Guerillas, Terrorists, and Intelligence 
Analysis. Technical Report,” Army Combined Arms Center (2004) 
63   Grau, L.W., “Something Old, Something New. Guerillas, Terrorists, and Intelligence 
Analysis. Technical Report,” Army Combined Arms Center (2004); Popp, R., Armour, T., 
Senator, T., Numrych, K., “Countering terrorism with information technology,” 
Communications of the ACM (2004) 47, 36–43.; McCulloh, I.A., Carley, K.M., Webb, M., 
“Social network monitoring of Al-Qaeda,” Network Science (2007) 1, 25–30. 
64   Boongoen, T., Shen, Q., Price, C., “Disclosing false identity through hybrid link 
analysis,” Artificial Intelligence and Law (2010) 18, 77–102. 
Roche et al.: Cyber Intelligence Challenge
Produced by The Berkeley Electronic Press, 2015
#
# 
 
116 
 
other applications in competitive65 intelligence.66   An important product of 
link analysis combined with other methods is information rich 3D graphics.67   
 
Intelligence using social media has been a growth area.68  All of these studies, 
and others not mentioned, have made great progress in using social media to 
find out about the users, and many times about their behavior.  But as we 
shall see next, asyngnotic networks operate in a different way, and so unless 
significantly extended or improved, the techniques developed so far are not 
sufficient to handle the full needs of intelligence analysis.  Without new 
techniques of analysis, asyngnotic networks will remain invisible. 
 
Using Neuroscience Models to Understand Asyngnotic 
Networks 
Asyngnotic networks have several characteristics that are somewhat different 
from other networks. As the structure and behavior of asyngnotic networks 
constantly are in flux, it is difficult to predict how they work or when they will 
strike.  They are self-organizing, so any action may not be driven by a 
detectable dispatch of a leader’s command.  Instead, they act as if triggered by 
an unknown force.   As notions of leadership theory applied to any 
                                                     
65   Vaughan, L., You, J., “Content assisted web co-link analysis for competitive 
intelligence,” Scientometrics (2008) 77, 433–444.; Ramakrishnan, T., Jones, M.C., 
Sidorova, A., “Factors influencing business intelligence (BI) data collection strategies: An 
empirical investigation,” Decision Support Systems (2012) 52, 486 – 496. 
66   The study used web co-link analysis to generate competitive maps in the WiMAX 
industry. Reid (2003) identifies seven techniques of competitor analysis including 
a) advertising analysis; b) alliance networks analysis; c) competitor profiling; 
d) corporate culture analysis; e) futures-based analysis; f) media analysis; and 
g) opportunity assessment.  
67   Risch, J.S., May, R.A., Dowson, S.T., Thomas, J.J., “A virtual environment for 
multimedia intelligence data analysis,” Computer Graphics and Applications, IEEE 
(1996) 16, 33–41; Chin, G., Kuchar, O.P., Whitney, P.D., Powers, M.E., Johnson, K.E.,  
“Graph-based comparisons of scenarios in intelligence analysis, in: Systems, Man and 
Cybernetics,” 2004 IEEE International Conference on, IEEE. pp. 3175–3180.; Chung, H., 
Yang, S., Massjouni, N., Andrews, C., Kanna, R., North, C., 2010. VizCept: Supporting 
synchronous collaboration for constructing visualizations in intelligence analysis., in: 
IEEE VAST, pp. 107–114 
68   There is an important discussion regarding privacy in the United States and how to 
balance it against national security.  See Omand, David, Jamie Bartlett, and Carl Miller. 
"A balance between security and privacy online must be struck." Magdalen House 136 
(2012); Jaeger, Paul T., Charles R. McClure, John Carlo Bertot, and John T. Snead. "The 
USA PATRIOT Act, the Foreign Intelligence Surveillance Act, and information policy 
research in libraries: Issues, impacts, and questions for libraries and researchers." The 
Library 74, no. 2 (2004); and Rovner, Joshua. "Intelligence in the Twitter 
Age." International Journal of Intelligence and CounterIntelligence 26, no. 2 (2013): 
260-271 
 
Journal of Strategic Security, Vol. 8  No. 3
http://scholarcommons.usf.edu/jss/vol8/iss3/7
DOI: http://dx.doi.org/10.5038/1944-0472.8.3.1448
#
# 
 
117 
 
organization hold that leadership is essential to its growth and effectiveness, 
the fact that asyngnotic networks operate without leaders raises the question: 
“If there is no leadership, then how does the asyngnotic network operate and 
organize itself?”  Since there also is no consistency in communication 
pathways informing these networks, they may operate in a way that is 
invisible to traditional link analysis that depends on pre-identified nodes to 
“link” with common characteristics usually found through multi-linear 
regression.  Since their membership is not known, traditional social media 
data mining tools are not as helpful.  There is no “consciousness” in the 
network as it is formed, and network formation is not driven by any strategy.  
Another part of the mystery involves the constant change and propagation of 
asyngnotic networks.  What causes these changes, how do the changes take 
place, and what are the forces that provide the energy for propagation?  
 
Hebbian Plasticity  
As asyngnotic networks operate in a way that is not unlike the phenomenon 
associated with neurons found in the brain, we may look to neuroscience as a 
reference framework for analysis.  By transplanting into the intelligence world 
models that describe the behavior of neurons, we can better locate and 
understand the behavior of asyngnotic networks.  
 
The Hebbian plasticity model69 refers to how the brain learns by 
strengthening links between different neurons by constantly “re-wiring” itself. 
This phenomena occurs naturally without leadership or direction.  At the core 
of this model is the effect of “action potentials”70 flowing from one neuron to 
another.  The more flow, the stronger the link.  Analogously and with 
reference to Hebbian plasticity, an asyngnotic network’s ability constantly to 
change shape and configuration as it remains embedded in society is 
facilitated by the constant flow of action potentials from one network node to 
another.  In neuroscience, these “nodes” are neurons; in an asyngnotic 
network, they are individuals or component organizations that in counterterrorism
parlance typically are referred to as “cells”.  In the world of 
neuroscience, the action potential is measured in millivolts, but in the 
intelligence world, we will need to define these flows with reference to flows of 
communications (emails, messages, telephone calls) or memes.71   The flow of 
                                                     
69   Hebb, D.O., The Organization of Behavior, Wiley (1949) 
70    An action potential is the change in electrical potential associated with the passage of 
an impulse along the membrane of a muscle cell or nerve cell.  
71    Memes are an element of a culture or system of behavior that may be considered to be 
passed from one individual to another by nongenetic means, especially imitation. Its 
 
Roche et al.: Cyber Intelligence Challenge
Produced by The Berkeley Electronic Press, 2015
#
# 
 
118 
 
memes is able to traverse the restrictive boundaries of any fixed 
communication system.  For example, memes can flow through the open 
mass media as a type of signaling (see Figure 1).  
 
Figure 1: Meme Flows through Media.  
 
  
In an asyngnotic network, streams of memes flow through the system at 
unpredictable rates, and are aggregated in each operational cell.  If the level of 
meme stimulation equals or exceeds a threshold ◊, then the cell will trigger an 
event, which would be an attack or any act in furtherance of a conspiracy. 
 
The Hebbian plasticity model compels one to examine asyngnotic networks 
by assessing the strength between different nodes in the network.  Since more 
meme flow means more strength connecting the nodes, and since any node 
can be connected to any other node, it is crucial to find the strongest 
connections.  These stronger connections may highlight enough of the shadow 
of the asyngnotic network to make it possible to detect.  
 
Another application of the plasticity concept is in understanding how 
asyngnotic networks learn and remember.  This is made possible through the 
stimulation and resultant strengthening of links between nodes caused by an 
above average flow of memes.  In the Hebbian concept, the “cell assembly” is 
caused by repeated internal communications.  In the intelligence world, “cell 
assembly” would refer to the organizational structure of the terrorist or 
criminal asyngnotic network.72  Memory, which we can equate to the 
                                                                                                                                                 
origin is μιμεμα (that which is imitated).  
72    See also Kohenen, T., “Self-organization and Associative memory,” 1989 Springer- 
 
Journal of Strategic Security, Vol. 8  No. 3
http://scholarcommons.usf.edu/jss/vol8/iss3/7
DOI: http://dx.doi.org/10.5038/1944-0472.8.3.1448
#
# 
 
119 
 
development of a sense of mission and shared goals by network members, is 
made possible by the repeated stimulation of these connections so that they 
have a type of reverberating activity.  As a consequence, if any of the 
networked nodes are stimulated by a matching meme, then the entire 
network can become activated.73   The fact that stimulation of one highlylinked
node will trigger action potentials in other highly-linked nodes is the 
model of network memory.  The stimulation of a node that is enough to 
trigger action or memory happens when the level of meme flow crosses a 
crucial threshold.  This causes an action potential, which in intelligence terms 
would refer to a part of the asyngnotic network taking specific actions in 
furtherance of a conspiracy or carrying out a terrorist act.74  
 
The Linear-Threshold Model for Network Decision Making  
Another approach can be used to approximate decision-making in these 
networks.  Using the McCulloch-Potts approach75 it is possible to model a 
simple computation (decision to act) made by a network node.  This is done 
by summing up all inflowing memes and expecting the node to take action if a 
threshold were reached.  One complication of this approach from the SIGINT 
point of view is that it does not distinguish between different meme flow rates 
and the fact that nodes receive memes from multiple paths (sources; 
communication channels; media).  
 
To think of this asyngnotic network as a decision making organization, one 
only needs to think of the flowing memes as being “0” or “1” which 
corresponds to false and true or “do not attack” and “attack”.  The linearthreshold
model is useful for taking into consideration the relative strength of 
different pathways that transmit memes to an asyngnotic node.  Since nodes 
in a terrorist network receive messages either by traditional tradecraft routes 
or through general impressions from the mass media, this model allows 
analysis to set a value to multiple types of channels.  
 
If intelligence is continuously monitoring a flow of memes over multiple 
paths, then the stimulus can be graded along a continual scale, rather than 
                                                                                                                                                 
Verlag, Berlin. 
73    This has a strange parallelism to the work by on viral phenomenon in social media in 
which the opinions of one person become good predictors of others in the same network.  
See Chesney, T., 2014. Networked individuals predict a community wide outcome from 
their local information. Decision Support Systems 57, 11 – 21.  
74 See the appendix for more on the mathematical expression of this model. 
75    McCulloch, W.S., “The brain computing machine,” Electrical Engineering (1949) 68, 
492–497. 
Roche et al.: Cyber Intelligence Challenge
Produced by The Berkeley Electronic Press, 2015
#
# 
 
120 
 
using only “true” or “false”.  This would allow, for example, the frequency and 
emotional content of messages to be accounted for.  In addition, since the 
monitoring would be continuous, then the effect on the cell receiving the 
memes can be tracked over time, and thus at each instant.  One nice aspect of 
this approach is that using historical SIGINT data combined with records of 
actions, it should be possible to quantify the θ value for a terrorist cell or any 
other node in an asyngnotic network.  The McCulloch-Pitts approach likely 
would help explain instances of “self-indoctrination” in unexpected places, 
such as the recent scandal at the École d l’Air in France where in April of 
2015, it was discovered that several students of the French Air Force Academy 
were planning to place bombs in a local mosque.  Interviews revealed that 
their motivations were entirely self-generated.   
 
Perceptrons and Neocognitrons  
The perceptron model76 and its progeny were developed as an explanation of 
visual perception.  The model involves multiple layers of neurons, with the 
first layer being the input and the last the output.  In neuroscience, the output 
is equivalent to recognition of an object.  Here we will equate recognition to 
an event trigger for an asyngnotic network node that causes an overt act in 
furtherance of a conspiracy, an enabling action (propaganda, logistics 
support) or a criminal act including terrorism itself.  Incoming memes flow 
through a number of network layers, but no meme in itself is enough to pull 
the trigger.  At each layer in the network, the memes are processed in parallel, 
but as they move through multiple channels.  
 
Because no meme in itself is sufficient to trigger an overt act, no emerging act 
can be anticipated if SIGINT processing does not employ this multi-channel 
cascading model.  One SIGINT advantage to the perceptron model, however, 
is that it is necessary to model and process meme flows in only one direction.  
Each node has orientation selectivity,77 and will pass on a trigger signal if the 
level of memes is sufficient and the total exceeds the threshold.  A crude 
analogy to this is a keyword listening program. These nodes can also be 
thought of as filters; everything but a meme stream carrying the correct 
                                                     
76   Rosenblatt, F., “The perceptron: a probabilistic model for information storage and 
organization in the brain,” Psychological Review (1958) 65, 386; Rosenblatt, F., 
“Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms,” 
Technical Report Report (1961) No. VG-1196-G-8. Cornell Aeronautical Laboratory, Inc.. 
Buffalo, NY.  
77   Hubel, D.H., Wiesel, T.N., “Receptive fields, binocular interaction and functional 
architecture in the cat’s visual cortex,” Journal of Physiology (1962)160, 106–154. 
Journal of Strategic Security, Vol. 8  No. 3
http://scholarcommons.usf.edu/jss/vol8/iss3/7
DOI: http://dx.doi.org/10.5038/1944-0472.8.3.1448
#
# 
 
121 
 
orientation will be filtered out.  There is another analogy to this in decisionmaking
theory in which it is assumed that actors systematically will ignore 
information that disagrees or is inconsistent with their preconceived notion.78 
   
As a consequence, in the perceptron model, the final trigger is fired when 
there is a recognition caused by compatible, and logically inter-locking meme 
streams.  The level of memes passing through to the next layer with a high 
threshold node can either be non-selective or simulate a logical AND 
operation requiring the receipt simultaneously of multiple (but different) 
memes.  If the node has a low threshold, it operates as a logical OR function, 
because any one of a variety of memes can reach the threshold.  Therefore, by 
modeling nodes in a perceptron sequence, it is possible to have AND layers 
followed by OR layers or any combination thereof.79 
 
In order to use this model for intelligence, it is necessary to identify the meme 
components graded by intensity of each trigger and classified according to 
logical interconnectivity.  Learning these recognition patterns can happen 
only by use of historical meme flow data tied to a specific trigger and overt 
act.  The underlying framework of the neocognitron model80 should be useful 
as a starting point for meme-trigger analysis.81  
 
Aggregate Characteristics of Asyngnotic Networks  
Several characteristics distinguish an asyngnotic network from other more 
structured networks, making this phenomenon extremely difficult to monitor 
and control.  Perhaps the most important distinction is that asyngnotic 
networks are self-organizing knowledge networks which enable a “meme” or 
stimulus (i.e., event, idea, or image) to pass through a group or community in 
                                                     
78   Cook, M.B., Smallman, H.S., "Human factors of the confirmation bias in intelligence 
analysis: Decision support from graphical evidence landscapes,” Human Factors: The 
Journal of the Human Factors and Ergonomics Society (2008) 50, 745–754; Nickerson, 
R.S., “Confirmation bias: A ubiquitous phenomenon in many guises,” Review of General 
Psychology (1998) 2, 175. 
79    Note that when nodes are linked together in an asyngnotic network, it is possible to 
produce any function of Boolean logic e.g., AND, OR, NOT, or XOR. See appendix for 
more on the mathematical expression of this model.  
80    Fukushina, K.M., “Neocognitron: a self-organizing neural network model for a 
mechanism of pattern recognition unaffected by shift in position,” Biological Cybernetics 
(1980) 36, 193–202 
81    See also the LeNet approach by LeCun. (LeCun, Y., Boser, B., Denker, J.S., 
Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D., “Backpropagation applied to 
hand-written zip code recognition,” Neural Computing (1989) 1, 541–551). There are a 
number of other neuroscience models that can be considered such as pattern completion, 
interference between different “cell assemblies” and synaptic loops.  
Roche et al.: Cyber Intelligence Challenge
Produced by The Berkeley Electronic Press, 2015
#
# 
 
122 
 
an unstructured way.  As the name implies, these networks provide a vehicle 
for the expression of emerging—often unconscious—ideas, feelings and 
motivations among network members.  Since asyngnotic networks are not 
permanent, they form and dissolve as the stimulus proceeds leaving few 
traces beyond an affective residue in those touched by the process.  Thus, in 
some ways the spread of a meme through an asyngnotic network is like the 
spread of a virus through a population; and like a virus, the meme changes as 
it emerges and has different effects on those it encounters.82 
 
This evolutionary property of asyngnotic networks is one reason why they are 
so difficult to identify and map.  The social media analysis and linking 
techniques discussed above focus on the flow of information (or memes) 
within and between individuals and groups, which assumes a more defined 
target (meme) and more structured relationships than are typically found in 
an evolving asyngnotic network.  This does not, however, mean that these 
techniques are without merit.  Since asyngnotic networks support the 
emergence of sometimes unconscious and latent feelings toward a particular 
idea, image or subject, they are not in themselves vehicles for action.  Instead, 
any triggering action results from the transmission of the meme through 
other, more established subsystems, which may be more amenable to being 
surfaced using current intelligence tools.  The residue of the asyngnotic 
network may become visible when one or more members of an existing 
(online or other) community are triggered by an emerging asyngnotic meme 
so as to actively plan and communicate within the group their pending 
reactions.  Thus, in terms of the neurological models discussed above, an 
asyngnotic network may alter (i.e., lower or raise) the action potential within 
an established community by impacting the willingness of members to engage 
(or dis-engage) in action of some kind.  
 
Until recently, asyngnotic networks would have been considered informal 
social networks for sharing ideas and “news” within a closed community.  As 
such, the techniques discussed above would provide a valuable window into 
the fairly well defined or semi-permanent relationships between cells or 
individuals.  The emergence of these networks, however, is a consequence of 
ongoing advances in telecommunications and social media that allow 
members of any given community to be concurrent members of any number 
of unrelated groups.  As a result, a meme can travel through the global 
population (consciousness) in a discontinuous, unpredictable way, jumping 
                                                     
82   This is not unsimilar to the “telephone” game in which persons pass from one to 
another a sentence, but as it is passed along, it is changed.  
Journal of Strategic Security, Vol. 8  No. 3
http://scholarcommons.usf.edu/jss/vol8/iss3/7
DOI: http://dx.doi.org/10.5038/1944-0472.8.3.1448
#
# 
 
123 
 
from one network to another, leaving multifarious effects within the specific 
communities it touches.  In other words, the same meme may have vastly 
different impacts on the individuals and communities it reaches.  For 
example, it might cause the temporary linkages of networks, even when prior 
to that time there was no planned or realized connection.  This partially 
explains how self-organization works.  
 
The flow of memes can be powerful.  For example, the ISIS beheading videos 
leave an indeterminate affective impression on the individuals who view 
them.  They educate and influence, but without any advance planning of a 
target for their influence.  These videos stimulate a wide range of possible 
reactions from the various established networks exposed to them.  These 
might range from the subsequent actions of Anonymous to the increased 
solidarity of Islamic groups, to a call for peace from religious groups or 
skepticism from the diplomatic community.  
 
In short, asyngnotic networks represent a significant advancement in human 
communication by allowing the real-time transmission of ideas and images 
(memes) simultaneously among billions of persons.  These networks are 
largely resistant to language and other traditional barriers, and are only likely 
to become more important as more people are linked together through the 
mobile Internet and social media.  Given the present global institutional 
structure and political economy, the emergence of a global (mass) 
consciousness largely resistant to overt or covert leadership might pose the 
greatest threat to established interests and the laws that protect them as well 
as pose a challenge to national security.  
 
Practical Application of the Asyngnotic Network Approach: 
Charlie Hebdo  
In Paris, on January 7, 2015, Cherif and Said Kouachi, two Islamic terrorists 
murdered the staff of the satirical magazine Charlie Hebdo using Kalashnikov 
assault rifles.  In leaving the criminals shouted “We have avenged the Prophet 
Muhammad.”  In their escape, the criminals stopped their vehicle to get out 
and shoot a police officer who was lying wounded on the ground. 
Approximately 3 km away, they abandoned their car, which later was found to 
be stuffed with Molotov cocktails and jihadist flags. At the same time in 
south-west Paris, Amedy Coulibaly shot a 32-year-old jogger in the Fontenayles-
Roses park.  About two hours later, the Hebdo terrorists used their 
Roche et al.: Cyber Intelligence Challenge
Produced by The Berkeley Electronic Press, 2015
#
# 
 
124 
 
Kalashnikovs and rocket-propelled grenade launchers to rob an Avia petrol 
station near Villers-Coutterest, north-east of Paris.83  The next day in the 
morning another criminal weilding a machine gun and pistol shot dead 
Clarissa Jean-Philippe, a young policewoman in Montrouge. By this time, the 
police had identified the two Hebdo murderers and issued an arrest warrant. 
One of the terrorists had been jailed in 2008 and had been known for a long 
time for militant activities.   They took refuge in the Creation Tendance 
Decouverte print shop in Dammartin-en-Goele.  
 
At 5:00pm, the Hebdo terrorists ran out of the building, guns blazing, and 
were shot dead.  At the same time, after another shootout, two other terrorists 
Amedy Coulibaly and Hayat Boumeddiene took hostages at a kosher 
supermarket in Porte de Vincennes (in the east of Paris).  They demanded 
freedom for the Hebdo terrorists (thus showing a connection between the two 
events), but were killed approximately 15 minutes after the Kouachi brothers, 
miles away.  The police were given a chance to attack when the terrorists 
paused to pray.  They left in their wake four dead. Hayat Boumeddiene 
escaped to join the genocidal ISIS group in Syria, but later was linked to the 
original Hebdo terrorists by more than 500 telephone calls.  Supposedly the 
Hebdo attack was in response to an incitement to murder issued by a religious 
authority somewhere, but that does not explain killing the young 
policewoman or going after a Jewish supermarket.  Omer el-Hamdoon, 
president of the Muslim Association of Britain defended the murders and 
claimed that publishing cartoons was not covered by freedom of speech, and 
this was echoed in the Egyptian press and elsewhere.84 
 
Only two weeks later, terrorists opened indiscriminate fire on a café in central 
Copenhagen to kill the Swedish cartoonist Lars Vilks.  The two criminals 
escaped in a Volkswagen.  Shortly thereafter, a security guard working at a 
synagogue was murdered, and two police officers wounded by another 
gunman.  This attack followed the same pattern as in Paris: First, murder 
cartoonists; then murder Jews.  One gunman was shot dead by police the next 
morning.85  In Paris, 17 citizens and three terrorists were dead; in 
Copenhagen 2 citizens and one terrorist were dead. In Copenhagen, the 
                                                     
83   Location 49 03’46.92” North; 2 41’38’60” East.  
84   Penketh, A., Weaver, M.,.”Charlie Hebdo: first cover since terror attack depicts 
prophet Muhammad,” The Guardian (2015), Online version. 
85   For a timeline of the cartoon-related violence incidents, see: Staff, R., “Timeline - 
Prophet Mohammad cartoons bring attacks to Scandinavia,” Reuters (2012) Online 
version.  
Journal of Strategic Security, Vol. 8  No. 3
http://scholarcommons.usf.edu/jss/vol8/iss3/7
DOI: http://dx.doi.org/10.5038/1944-0472.8.3.1448
#
# 
 
125 
 
Danish intelligence agency (PET) reported that one of the terrorists (the one 
still at large) had been on their “watch list”. 
 
It is difficult to know what intelligence could have done in advance to stop the 
Hebdo attack.  Immediately speculation started regarding the affiliation of the 
terrorists. Were they part of the so-called Islamic State (ISIS), or part of alQaida
(AQ), or part of a subsidiary of AQ, or perhaps members of AQ in the 
Arabian Peninsula (AQAP)?  Much of the analysis seemed to be after the fact. 
This is the opposite of intelligence, which by definition must complete its 
analysis prior to any event.  Judging by the statements made afterwards, it is 
clear that the extent of analysis was confined to compilation of watch lists of 
people who were not watched and little more.  
 
In order to employ an asyngnotic network approach to develop intelligence on 
terrorist groups, it is necessary to link the abstract and ephemeral nature of 
these networks with the physical reality of terrorist operations on the ground.    
The asyngnotic network approach would start by recognizing that all terrorist 
cells are each but one node in a complex living system that is interconnected 
with other networks, mostly invisible.  For example, it is clear that since the 
terrorists were able to obtain heavy artillery, they were in contact with 
networks that provided financing, other networks that provided training, and 
yet other networks that provided logistics and distribution of terrorist 
equipment.  This is the physical layer of the terrorist organization.    
 
One way to link asyngnotic networks with this physical reality is to utilize 
Living Systems Theory (LST), developed by James Grier Miller, a former OSS 
officer who went on to play a major role in social science.  In Table 1 (above), 
we have listed the twenty categories of subsystems defined by LST and then 
matched them against the types of intelligence technologies and practices that 
might be used to detect and monitor their associated asyngnotic networks.86   
In this way, the LST model may serve as a useful starting point to hypothesize 
asyngnotic networks and uncover the hidden connections that were 
stimulated by the flow of those memes responsible for triggering the attacks.   
In the next section we will discuss a few of the practical intelligence 
challenges in identification of asyngnotic networks.  
 
 
 
                                                     
86   There is a fuller description of LST in the appendix. 
Roche et al.: Cyber Intelligence Challenge
Produced by The Berkeley Electronic Press, 2015
#
# 
 
126 
 
 
 
 
                                                     
87 Based on McCreary, J., 2015. For the night of 7 January 2015. NightWatch Online.  
 
Table 1:  The Charlie Hebdo Attack. Application of the living systems model 
to asyngnotic network analysis. An asyngnotic network as applied to terrorism is 
categorized according to different aspects of a living system.87   
Subsystem Al Qaida Instance SIGINT HUMINT 
Input Transducer (IP) Lookouts, e-mail, radio, television l,c,d,i v 
Ingestor (IN) Banks, mule trains, recruiting camps $,o f,v 
Internal Transducer 
(IT) 
Monitors incoming information and 
interprets 
i X 
Channel and Net (CN) Phone and email receivers l,n,v v 
Decoder (DE) Internet, radio frequencies, messenger l,n,$ c 
Timer Planning cells  f 
Associator (AS) Filing systems, target analysts, 
accountants 
l,d,k,g  
Memory (ME) File cabinets, disk drives, and keepers  c 
Decider (DC) Planners, now distributed over net l,n g,d,k 
Encoder (EN) Computer, video camera, radio/phone l,c,y,i c 
Reproducer (RE) Recruiters and disbursers $ f,v 
Boundary (BO) Guards, sentries, walls i,o a 
Distributor (DI) Communicators, logistics, transport o s 
Converter (CO) Trainers  s,f 
Producer (PR) Bomb-makers  s,f 
Storage (MS) Weapons caches, inventories v v 
Motor (MO) Jets to mules i,o X 
Supporter (SU) Partisans & donors in host countries, 
news media 
k,d,l,v,g s,f,v 
Output Transducer 
(OT) 
Operatives, execution cells; three men 
in Paris 
k,y,l,p s,f,p 
Extruder (EX) Graveyards, jails; under internal 
security 
o m 
HUMINT: Surveillance (s); Infiltration (f); SIGINT collection support (c); Passive 
monitoring (m); SIGINT: Link analysis (l); Content analysis (n); Financial flows ($); 
Mass media (d); Social networks (k); Cyber espionage (y); Biometrics (b); 
Categorization & clustering (a); Database and Big Data (g); Event detection & 
notification (v); Geospatial (o); Predictive modeling (p); Video processing (i).  List 
partially derived from Popp, R., Armour, T., Senator, T., Numrych, K., 2004. 
Countering terrorism with information technology. Communications of the ACM 47, 
36–43.  
Journal of Strategic Security, Vol. 8  No. 3
http://scholarcommons.usf.edu/jss/vol8/iss3/7
DOI: http://dx.doi.org/10.5038/1944-0472.8.3.1448
#
# 
 
127 
 
 
Discussion: Using the Asyngnotic Approach in Intelligence  
The starting point of dealing with asyngnosis as leading to potential bad 
outcomes is to recognize it exists and that it enhances linear police and 
counterintelligence work.  
 
The largest challenge in strategic and other kinds of warning is to “warn left of 
the threat.”  That is to say, to warn that information conditions are promoting 
the emergence of a threat, before the threat has developed.  It is low 
probability warning because at this early stage the threat is one of many 
potential outcomes of visible developments.  It also is the time when the 
emerging threat is easiest to manage.  
 
The practical advantages of tracking asyngnotic networks are several.  The 
historic precedents prove the value of using memes in very early identification 
of emerging threatening behavior before overt action has been detected.  As 
mentioned above, the most elusive challenge in warning of violent threats has 
been to find a way to warn with some confidence when they are ideas before 
significant action begins.  The purpose is to gain time and save costs in 
managing the threat environment.  Once a malefactor starts to take action 
other than communications, the clock starts ticking and the costs of 
prevention start to rise.  
 
A good example of asyngnotic network emergence was the unexpected 
appearance of flash mobs in Cairo and Alexandria that preceded the ouster of 
President Mubarak.88  The flash mobs demonstrated a propensity for 
leaderless self-organization, at least in the initial stages.  Leaders do emerge 
or surface quickly, but not when the mob is gathering.   Somebody starts the 
phone-chain, but lots of other gang or cell leaders make the mob happen. 
They also illustrate in real life that asyngnotic networks are real and have 
impact, often significant.  
 
U.S. intelligence missed the overthrow of Mubarak, in part, because it failed 
to recognize the importance of the flash mobs.  Nevertheless, the flash mob 
were not the biggest impact of the memes that were floating around.  The 
Egyptian military leadership recognized in the memes and in the mobs that 
                                                     
88   Jurgenson, N., “When atoms meet bits: Social media, the mobile web and augmented 
revolution,” Future Internet (2012) 4, 83–91. 
Roche et al.: Cyber Intelligence Challenge
Produced by The Berkeley Electronic Press, 2015
#
# 
 
128 
 
they had an opportunity to push aside Mubarak because he wanted to install 
his son as his successor!  
 
Warning at the stage of emergence, based on chatter and ideas, has always 
depended on the experienced judgment of analysts.  That has seldom been a 
persuasive basis for action by decision makers.  Before asyngnosis, there was 
no systematic technique for capturing that professional judgment so it could 
be more persuasive in justifying and targeting early prophylactic measures. 
Asyngnosis also provides a scaffold that enables the wisdom of experienced 
officers to be preserved and taught to new analysts.  For example, when the 
probability of a cyber or terrorist threat is low, asyngnotic analysis can enable 
tracking the buildup or lack of buildup of mental intensity that will drive 
action.  This helps improve the accuracy and, more importantly, the 
confidence of warning.  As such, asyngnotic analysis is the only technique for 
warning based on emergence, vs overt action.  
 
Intelligence Antecedents to Analysis of Asyngnotic Networks  
This type of analysis rests on the shoulders of earlier intelligence work.  For 
example, there are some aspects of content analysis as applied to propaganda 
that offer an antecedent to the techniques needed to analyze asyngnotic 
networks.  Alexander George89 wrote the seminal book Propaganda Analysis 
which described the organization and techniques use by a specialized group of 
U.S. propaganda analysts during World War II.  This was the predecessor of 
the Foreign Broadcast Information Service (FBIS) and the Open Source 
Center.90   That group had remarkable success estimating Nazi offensives and 
weapons development.  They studied propaganda structures and 
organization, word counts, themes, the authority of the propaganda vehicle, 
and the frequency of repetition of the themes.  They were able to establish 
thresholds for distinguishing deception and misinformation from real 
developments.91  
 
                                                     
89 George, A.L., 1959. Propaganda Analysis. Row, Peterson, Evanston, Illinois.  
90 The work was initially set up inside the FBI.  
91 George, A.L., “Propaganda analysis: A study of inferences made from Nazi propaganda 
in World War II,” Copyright - Copyright UMI - Dissertations Publishing 1959; Last 
updated - 2014-01-21; First page - n/a.; George, A.L., 1954. The Scientific Status of 
Propaganda Analysis,” Technical Report P-616. “The Rand Corporation. Santa Monica”; 
George, A.L., 1955. “Prediction of political Action by Means of Propaganda Analysis,” 
Technical Report P-779. The Rand Corporation. Santa Monica.; George, A.L., “Prediction 
of political action by means of propaganda analysis,” The Public Opinion Quarterly 20, 
(1956) pp. 334–345.  
Journal of Strategic Security, Vol. 8  No. 3
http://scholarcommons.usf.edu/jss/vol8/iss3/7
DOI: http://dx.doi.org/10.5038/1944-0472.8.3.1448
#
# 
 
129 
 
In using George’s techniques to study North Korean propaganda, intelligence 
analysts were able to identify the emergence of a crisis based on key word 
counts, repetition, frequency, placement in the propaganda, and the authority 
of the propaganda vehicle.  For example, analysts learned that the threshold 
for a crisis was the appearance of a meme six times in different propaganda 
channels in a 24 hour period.92   This work was done manually without the 
benefit of a structured analytical method, computerized assistance, or a 
framework such as asyngnosis.  
 
Cyber Exploitation of Intelligence  
Uncovering an asyngnotic network may be as important as identification of a 
leader operating in a traditional network, but may be more difficult.93  Doing 
this poses a number of challenges for intelligence, particularly on the SIGINT 
and analytical side.94   Judgment is important and when it becomes compiled 
knowledge it is persuasive.  Nevertheless, the promise of computerized 
techniques is that they will enable the identification of thresholds for action 
by non-state and state actors that are not solely based on the analyst’s gut 
feeling from reading propaganda.  Finally, the use of a structured technique 
has special value in distinguishing genuine threats from deception, bluff, 
misdirection and misinformation.  Computerization will allow exploitation of 
the benefits of Big Data in support of crime prevention and early warning.95  
 
The asyngnosis approach will require the development of new language 
concepts and more R&D on how to analyze memes in open sources. It also 
will drive new ways of thinking about the linkage between memes and 
threatening action.  Those are healthy developments because the existing 
techniques are not working.  On the SIGINT side, there is a need for continual 
“flow through” monitoring or order to capture and model asyngnotic 
networks in real time.  But in order to set up real-time flow-through, much 
must be discovered and calibrated so that analysis is sensitive enough to bring 
this phenomenon to light.   
                                                     
92    George, A.L., “The Chinese Communist Army in Action: The Korean War and its 
aftermath,” Columbia University Press, 1967, New York.  
93   The classic example of the dismantling of a terrorist network using identification of its 
leadership is shown vividly in the Gillo Pontecorvo directed film La Battaglia di Algeri, 
1966.  
94   This discussion does not incorporate evaluation of the legal environment and any 
potential privacy issues.  
95   Big data is a general term for databases that are so large traditional processing 
applications are inadequate to handle them.  It usually refers to the use of predictive 
analytics to aid in decision-making. 
Roche et al.: Cyber Intelligence Challenge
Produced by The Berkeley Electronic Press, 2015
#
# 
 
130 
 
 
 
Figure 2: SIGINT monitoring of Asyngnotic networks examines 
memes flowing through both social and mass media (SMM) and 
Point to Point (P2).  
 
In each of these areas in Figure 2, the meme flow can be an undirected 
broadcast (BCST) or multi-cast (MCST) or it can be associated with a specific 
target (D-Target) or originate from a specific source (D-Source) or flow to a 
specific person (D-Name).  Traditional SIGINT monitoring is inside the red 
circle.  Using a meme framework allows integration of social media analysis 
with traditional SIGINT. 
 
Since asyngnotic networks may present rapid stand-up and dissolution 
capabilities, it will be necessary to develop capability to identify a network 
configuration as soon as it appears.  After a number of these are identified, it 
should be possible to generate a taxonomy of asyngnotic network types.  As 
latent asyngnotic networks may become visible then slide back into obscurity, 
multiple iterations of network instances might be summed to isolate latent 
network persistence.  Frequency might initially be assumed to be synonymous 
Journal of Strategic Security, Vol. 8  No. 3
http://scholarcommons.usf.edu/jss/vol8/iss3/7
DOI: http://dx.doi.org/10.5038/1944-0472.8.3.1448
#
# 
 
131 
 
with intensity and is an important variable because in some models it may 
drive the node to the trigger threshold.96   
 
The definition of memes may be problematical, but any result must include a 
classification schema.  Minimal factors for classification might be: 
a) incitement to specific acts; b) examples and “teaching” of specific acts, (but 
without direct incitement); c) logical arguments and Weltanschauung 
triggers; specific anti- or counter-regime assumption criticism and reframing; 
d) argument assist techniques such as “but what about”.97  Allowing networks 
such as ISIS to post repulsive videos on websites or through social media may 
aid in collection of that intelligence needed to identify and disable hidden 
networks.  A crucial factor in success will be choosing the best asyngnotic 
network to analyze initially.  Operationalization and calibration of 
multichannel indexes to substitute for the flow of “actions potentials” will be 
crucial.  There are at least three variable classes to consider: a) channel 
characteristics; b) meme classes; and c) meme frequency and intensity.98 
 
Conclusion 
In this paper, we have discussed the problems of asyngnotic networks and 
identified a shortcoming in literature and practice regarding their analysis.  
We then suggested that a neuroscience approach will be useful as an 
explanatory framework because it assumes that the same type of selforganization
and triggering functions can be models for asyngnotic networks. 
Finally, we discussed some of the challenges for intelligence, particularly for 
SIGINT, in using these models.   There are many more models that can be 
                                                     
96    It might be assumed that in the general media, this is easily defined by the common 
news cycle from output, to peak propagation, to decline and there are many off-the-shelf 
tools that already make these calculations. Nevertheless, it will be necessary to cluster 
related stories in order to estimate aggregate meme propagation trajectories. In the P2P 
directed channels, traditional SIGINT measures should suffice.  
97   “But what about” is a popular social media propaganda and trolling technique. 
Example: “Russia is complicating solution of the Syrian problem because of its continued 
support for the Assad government.” Refrain: “But what about US support for the nondemocratic
Saudi monarchy?” The point is to drive attention away from the original idea.  
98    Johnson (Johnson, R., “Developing a taxonomy of intelligence analysis variables,” 
Studies in Intelligence  (2003) 47) presents a taxonomy of variables used in intelligence 
analysis including a) systemic variables; b) systematic; c) idiosyncratic; and 
d) communicative, asyngnotic networks require careful attention to a different set of 
variable parameters. Because of the multi-channel nature of an asyngnotic network, the 
testing and variable calibration must be able to encompass several types of meme flows 
including a) close circuit broadcasts; b) point to point directed and names; c) social and 
mass media general; and d) social and mass media general directed targeted. See 
Figure 2. NB: the class of media general directed source is excluded because it is too 
obvious.  
Roche et al.: Cyber Intelligence Challenge
Produced by The Berkeley Electronic Press, 2015
#
# 
 
132 
 
harnessed to surface asyngnotic networks, but we hope this short discussion 
has stimulated discussion regarding the potential for this approach.  
 
  
Journal of Strategic Security, Vol. 8  No. 3
http://scholarcommons.usf.edu/jss/vol8/iss3/7
DOI: http://dx.doi.org/10.5038/1944-0472.8.3.1448
#
# 
 
133 
 
Appendix A. Full Statement of Anonymous99  
Greetings citizens of the world.  We are Anonymous.  Operation ISIS 
Continues.  First, we need to clarify few a things. We are: Muslims, Christians, 
Jews.  We Are hackers, crackers, hacktivists, phishers, agents, spies, or just 
the guy from next door.  We are students, administrators, workers, clerks, 
unemployed, rich, poor.  We are young, or old, gay or straight.  We wear 
smart clothes or rugs.  We are hedonists, ascetics, joy riders or activists.  We 
come from all races, countries, religions, and ethnicity.  United as one, 
divided by zero.  We are Anonymous.  
 
Remember: The terrorists that are calling themselves Islamic State, (ISIS), 
are not Muslims! ISIS.  We will hunt you, take down your sites, accounts, 
emails, and expose you.  From now on, no safe place for you online.  You will 
be treated like a virus, and we are the cure.  We own the Internet. 
  
We are Anonymous.  We are legion.  We do not forgive.  We do not forget. 
Expect us.  
  
                                                     
99   This is a transcript of the full statement from Anonymous released with its reports on 
#OpISIS. 
Roche et al.: Cyber Intelligence Challenge
Produced by The Berkeley Electronic Press, 2015
#
# 
 
134 
 
Appendix B. Symbolic Notation  
The notation for Hebbian Plasticity and McCulloch-Pitts can be adapted for 
quantitative analysis of social media and other content.100  
 
Note: The notation for standardized link analysis has been excluded since it is 
widely published and known.  
 
Hebbian Plasticity for Asyngnotic Terrorist Networks  
This refers to the assumption that the brain or asyngnotic organization learns 
through the repetitive stimulation and resulting strengthening of linkages 
between nodes.  It is given by ∆𝑊𝑖𝑗 ∝  𝑥𝑖 𝑥𝑗  where i is the previous network 
node and j is the downstream node.  As a result, 𝑊𝑖𝑗  becomes proportional to 
the statistical correlation between the activities of i and j. Co-variance is 
another way to state Hebbian Plasticity.  The covariance rule is ∆𝑊𝑖𝑗 ∝ (𝑥𝑖−<
𝑥𝑗 >)(𝑥𝑗−< 𝑥𝑖 >) with  < 𝑥𝑖 >  being the average activity for node i. In this 
approach, Wij becomes proportional to the covariance between the activities 
of nodes i and j.  
 
Adaption of McCulloch-Pitts  
McCulloch-Pitts uses θ as a surge parameter with inflowing memes being 
either stimulating or general and thus being coded in a binary manner as “0” 
or “1”.  The node takes action when the threshold is met.  By connecting one 
node to another, intermediated by meme flows, it is possible to estimate 
whether any node will take action.  The linear threshold (LT) model uses 
𝑥𝑖(𝑡 + 1) = 𝐻 (∑ 𝑊𝑖𝑗𝑥𝑗(𝑡) − 𝜃𝑗)
𝑁
𝑗=1  which allows for a variety of incoming 
memes at a specific time 𝑥1(𝑡), 𝑥2(𝑡) ⋯ 𝑥𝑁(𝑡) with each meme being valued at 
“0” or “1” representing regular information or other information that is 
stimulating to the node to take action (such as launch an attack at time  (t + 
1)).  Each arriving meme is multiplied by the Hebbian strength of the pathway 
it arrives on.  As long as the sum is less than the threshold, then there is no 
action taken.  But if the value either reaches or exceeds the threshold θ, then 
                                                     
100   See for example Stanley Wasserman & Katherine Faust, Social Network Analysis: 
Methods and Applications, Cambridge University Press, 1994, pps. 71-83 for (1) graph 
theoretic notation (single relation; multiple relations); (2) sociometric notation; and 
(3) algebraic notation.  For applications, see Xu, J.J. & Chen, H., CrimeNet Explorer: A 
Framework for Criminal Network Knowledge Discovery, ACM Transactions on 
Information Systems, 23(2), April 2005, pp. 201-226; Link Analysis Workbench, Air 
Force Research Laboratory Information Directorate, Rome Research Site, Rome, New 
York, September 2004.  
Journal of Strategic Security, Vol. 8  No. 3
http://scholarcommons.usf.edu/jss/vol8/iss3/7
DOI: http://dx.doi.org/10.5038/1944-0472.8.3.1448
#
# 
 
135 
 
it will trigger the node to take action.  The continuous model of the same is 
given by 𝜏
𝑑𝑟(𝑖)
𝑑𝑡
+ 𝑟𝑖 = 𝐹 (∑ 𝑊𝑖𝑗
𝑁
𝑗=1 𝑟𝑗 − 𝜃𝑗)  where 𝜏  is a constant and rj is a 
graded variable rather than a binary variable, so it responds to the frequency 
of communication or intensity of the incoming memes.  
 
Nodes  
Nodes can be thought of as parts of a networked organization.  Generally, they 
are either individuals or organizations themselves as the unit of analysis.  
 
Memes  
A term that refers to communication of ideas.  In a model of asyngnotic 
networks that has a neuroscience orientation, the flow of memes is equated 
with the flow of the action potential along the pathways linking one neuron to 
another.  
 
  
Roche et al.: Cyber Intelligence Challenge
Produced by The Berkeley Electronic Press, 2015
#
# 
 
136 
 
Appendix C. Uncovering Asyngnotic networks using 
Living Systems Theory (LST)  
Further evidence that complex human networks may undergo a type of selforganization
is found in a branch of science called general systems theory.101   
Much of this work focused on identifying universal principles of systems that 
are equally true whether applied to living organisms at the cellular level or at 
the level of human society.  In the Living Systems Theory (LST) model, 
networks operate as self-organization systems.102  The LST approach came out 
of general systems research that developed an extensive vocabulary to discuss 
organizations.103  In the LST model, there are nineteen critical subsystems 
divided into three classes: a) systems that process both matter-energy and 
information; b) systems that process matter-energy; and c) systems that 
process information.  By dispensing with the matter-energy subsystems, we 
are left with nine subsystems that pertain only to processing of information 
including: a) Input Transducer (IP); b) Internal Transducer (IT); c) Channel 
and Net (CN); d) Decoder (DE); e) Associator (AS); f) Memory (ME); g) 
Decider (DC); h) Encoder (EN); and i) Output Transducer (OT).104 Adding the 
timer completes the list of twenty subsystems.  
                                                     
101   The concept of a system involves control, if there is no control, then there is no system 
(Hammond, D., Wilby, J., “The life and work of James Grier Miller,” Systems Research 
and Behavioral Science (2006) 23, 429–435).  
102    James Grier Miller was the creator of LST. He served in the OSS. See Miller, J.G., 
“My role in the assessment program of the Office of Strategic Services,” Behavioral 
Science (1996) 41, 245–261 
103    Robbins, S.S., Oliva, T.A., “The empirical identification of fifty-one core general 
systems theory vocabulary components,” Behavioral Science (1982) 27, 377–386.  
104    See also Miller, J.G. Mater-energy processing subsystems. The extruder. Behavioral 
Science 38, 46-57 for the extruder matter-energy processing system, which may be 
thought of as waste by-products; the producer (Miller, Ibid, The Producer, pp. 46-57) who 
synthesizes materials internally for the network; the ingestor which would include 
recruitment and gathering of resources for the terrorist network (Miller, Ibid, pp. 10-18); 
the distributor which would include logistics (Miller, Ibid, pp. 19-32); the converter 
storage; motor; and the supporters which would include sympathetic persons in the 
community.   All of these appear in the 38th volume of Behavioral Science (1993). 
Journal of Strategic Security, Vol. 8  No. 3
http://scholarcommons.usf.edu/jss/vol8/iss3/7
DOI: http://dx.doi.org/10.5038/1944-0472.8.3.1448
Fake News Detection using Machine Learning
Prasad Kulkarni1,?, Suyash Karwande1,??, Rhucha Keskar1,???, PrashantKale1,????, and Sumitra Iyer1
1Electronics and Telecommunication Department, SIES, Graduate School of Technology, Navi Mumbai, India.
Abstract. Everyone depends upon various online resources for news in this modern age, where the internet
is pervasive. As the use of social media platforms such as Facebook, Twitter, and others has increased, news
spreads quickly among millions of users in a short time. The consequences of Fake news are far-reaching, from
swaying election outcomes in favor of certain candidates to creating biased opinions. WhatsApp, Instagram,
and many other social media platforms are the main source for spreading fake news. This work provides a
solution by introducing a fake news detection model using machine learning. This model requires prerequisite
data extracted from various news websites. Web scraping technique is used for data extraction which is further
used to create datasets. The data is classified into two major categories which are true dataset and false dataset.
Classifiers used for the classification of data are Random Forest, Logistic Regression, Decision Tree, KNN and
Gradient Booster. Based on the output received the data is classified either as true or false data. Based on that,
the user can find out whether the given news is fake or not on the webserver.
1 Introduction
The term ‘Fake news’ refers to the news content that
is false, misleading, or fabricated, in which the facts,
sources, or quoted statements of the news content are
unverified. Fake news has existed in the form of gossip,
rumor, and misinformation throughout human history
[1]. To increase its effectiveness this Fake news is spread
throughout social media. Along with the billions of people
using social media, there are also robots, or simply bots,
residing within. These Bots help to propagate fake news
faster and boost up its popularity on social media.
Fake news detection is used to avoid rumors from
spreading across various platforms, such as social media
and messaging platforms. The impetus for this work is
to avoid the spread of Fake-news which can even lead to
worse activities. There has been a rise in the news lately
about lynchings and riots that result in mass deaths; fake
news detection aims to detect these and stop similar activities,
thereby protecting society from these unwelcome
violent acts [3]. The proposed system helps to find the authenticity
of the news. The news given by the user is clas-
sified as true or false based on the data collected using Web
Scraping. This task uses five various classification models,
including Random Forest, Logistic Regression, Decision
Tree, KNN, and Gradient Booster. To improve prediction
accuracy, a mixture of these models is tested.
Further, the paper is structured as follows: section 2
takes a glance at previous work done in fake news detection.
In the next section, data extraction, pre-processing
?e-mail: prasadpkulkarni99@gmail.com
??e-mail: suyashkarwande20@gmail.com
???e-mail: rhucha.keskar17@siesgst.ac.in
????e-mail: prashant.kale17@siesgst.ac.in
and classifiers are discussed. Section 4 depicts the classifier
accuracies and related results. Finally, In section 5
concluding remarks are mentioned.
2 Related Work
There have been quite several initiatives taken to achieve
fake news detection. In [3], Mykhailo Granik et. al.
showed a simple approach for the fake news detection system
using a naive Bayes classifier model. This was im-
plemented as a software system and then tested against a
dataset of Facebook news or the posts on Facebook. The
news was gathered from three Facebook pages, as well
as three large mainstream political news pages (Politico,
ABC News, CNN). They were able to achieve an accuracy
of around 74 percent. Classification accuracy for false
news is a little worse. This could have been caused by the
skewness of the dataset, only 4.9 of it is fake news.
The author uses different ideas for processing the text
dataset such as TF-IDF, Count Vectors, and Word Embedding
[5]. Further, the author implements the comparison
on various classification models which includes SVM, Recurrent
Neural Network model, Logistic Regression (LR),
and Naïve Bayes Method. Based on the comparison the
author has examined the scores like recall and precision
etc of the various models.
An overview of qualitative data cleaning with error repairing
and error detection approaches is discussed in [6].
Cleaning of data techniques was focused on the errors like
duplication, inconsistency, and missing values were dealt
with. It also described a statistical perspective on qualitative
data cleaning with the help of Machine Learning
techniques.
ITM Web of Conferences 40, 03003 (2021)
ICACC-2021
https://doi.org/10.1051/itmconf/20214003003
   © The Authors,  published  by EDP Sciences.  This  is  an open  access  article distributed under the  terms of the Creative Commons Attribution License 4.0
 (http://creativecommons.org/licenses/by/4.0/). 
#
#In [4] Avinash Shakya et. al. used aggregators in
their study Smart System for Fake News Detection to see
news from various sources in a single convenient location.
Checking RSS Feeds regularly, extracting articles
from various news sites, and gathering information are all
part of the basic methodology. The proposed plan is a mixture
of Naive Bayes classifiers, SVM, and semantic inves-
tigation due to the multi-dimensional nature of fake news.
The proposed plan is entirely based on Artificial Intelligence
approaches, which are essential to precise order be-
tween the genuine and the fake. The three-section strategy
combines Machine Learning calculations, which are subdivided
into managed learning procedures, with traditional
language preparation techniques [4].
In [7], a variety of topics of web scraping, starting with
a simple introduction and a brief review of various web
scraping software and applications. The process of web
scraping, as well as the numerous sorts of web scraping
techniques, before closing with web scraping’s pros and
downsides, as well as a full discussion of the numerous
fields in which it can be employed have been discussed.
Open Data, Big Data, Business Intelligence, aggregators
and comparators, development of new applications and
mashups, and so on are just a few of the possibilities available
with this data.
The researchers in [8] proposed to focus on different
feature engineering methods for generating feature vectors,
such as count vector, TF-IDF, and word embedding.
Seven distinct ML classification algorithms are trained to
categorize news as false or real, and the top one is chosen
based on accuracy, F1 Score, recall, and precision.
3 Methodology
Any Machine Learning model primarily requires a set of
data to train or test model. To extract vast volumes of
data from websites and save it in table format to a local
file or a database, we used web data extraction popularly
known as the web scraping technique. The methodology
used is by collecting all of the data retrieved from multiple
sources using the vivid characteristics of the web crawler
’Scrapy’ and python scripts and then analyzing it according
to the requirements. The python-based web crawler
’Scrapy’ may also assist us in retrieving the desired result,
as we analyze the process with specific code and provide
the necessary URL for the iteration to scrape the data from
the source URL [7]. Figure 1 represents the workflow.
Further, the collected data is separated into two groups:
A training set and a testing set. Train/Test is a method to
measure the accuracy of the model. The general idea is to
train an algorithm on a huge number of manually examined
web pages [1].
Raw content needed certain data pre-processing before
it could be fed into the simulations. Data Preprocessing is
a technique for data exploration that converts original data
into a suitable form. Actual data (real-life data) is often
inaccurate and therefore could not be sent over the design
with that information. This may cause some mistakes. So
while we send over a system, we have to pre-process data.
After reading the dataset we use some preprocessing functions
like tokenizing, stemming, etc. The material and in-
formation were taken from websites that were thought to
be involved with fake news. Before using a machine learning
system, text must be translated into numbers. The pre-
dictive algorithm takes documents as input and generates
a class label as output for document classification. For the
algorithm to accept the texts as input, they must be transformed
into fixed-length vectors of numbers. After pars-
ing the text, a procedure known as tokenization is used, in
which particular terms are deleted. With the help of the
feature selection and extraction method, we are manually
selecting relevant features which will contribute most to
the prediction variable and to increase the accuracy of the
model. For feature selection, techniques like bag-of-words
and n-grams and then TF-IDF weighting from sci-kit learn
python libraries were used. The Bag of Words model is a
basic and effective machine learning model for parsing text
texts. The model ignores all word order information and
simply looks at how many times the words appear in the
document. This model can be implemented in two different
ways.
• TF-IDF Vectorizer.
• Countvectorizer.
Figure 1: Workflow Diagram.
The Count Vectorizer creates an encoded vector that
comprises the full vocabulary’s length as well as the freITM
Web of Conferences 40, 03003 (2021)
ICACC-2021
https://doi.org/10.1051/itmconf/20214003003
2
#
#quency with which each word appears in the document. A
popular approach is to use the TF-IDF algorithm to calculate
word frequencies (Term Frequency Inverse Docu-
ment). Each document is then assigned a TF - IDF score.
TF-IDF uses word frequency to identify words that are
more important (occur more frequently) in a document.
The TF-IDF Vectorizer converts documents into tokens,
learns vocabulary, inverses document frequency weightings,
and allows you to cipher new documents [2]. When
compared to a non-vectorized implementation, vectorization
in this technique can significantly speed up the calcu-
lation process.
We are using different classifiers for predicting fake
news. A classifier will help us in ordering data automatically
or categorizes data into one or more of a set of
’Classes’. Different classifiers will use the extracted features.
All of the extracted features will be used by the clas-
sifiers. The classifiers employed in the Machine Learning
model are as follows
3.1 Logistic Regression
The advantages of logistic regression include probability
modeling, the capacity to depend on features, and the flexibility
to update the model. However, for higher accuracy,
logistic regression requires a big data set, but Naive Bayes
may function with small datasets as well.
3.2 Decision Trees
A decision tree is made up of decision nodes that start at
the top and work their way down. Dependent characteristics,
no need for linear class separation, fast management
of outliers, and intuitive decision tree interpretation are all
advantages of employing a decision tree. When there are a
significant number of sparse features, however, a decision
tree will overfit and perform poorly on the testing data [2].
3.3 Random Forest Classifier
Many decision trees are built by the random forest algorithm.
Utilizing a subset of features, each decision tree is
created. Each decision tree produces one class and eventually
bootstraps the votes to obtain better accuracy from the
Random Forest technique. A tree-shaped pattern is used to
describe the plan of action in a decision tree. At any node,
a decision will be made.
3.4 KNN Classifier
The KNN method assumes that the new case/data and existing
cases are similar and places the new case in the cat-
egory that is most similar to the existing categories. This
means that new data can be quickly sorted into a suitable
category using the KNN algorithm. The KNN algorithm
can be used for both regression and classification, though
it is more commonly used for classification problems.
3.5 Gradient Boosting Classifier
Gradient boosting creates an ensemble of weak prediction
models, usually decision trees, as a prediction model. The
resulting technique is called gradient boosted trees when
a decision tree is a weak learner, and it usually outperforms
random forest. It constructs the model in the same
stage-by-stage manner as other boosting approaches, but it
broadens the scope by allowing optimization of any differentiable
loss function.
We will compare the score and examine the confusion
matrix after we have fitted the model. When all of the
classifiers have been fitted, we will dump those models and
vectorization models which will be used while connecting
the model with the webserver.
Next, to connect the webpage to the model we chose
flask as a web development framework, which is deployed
on Amazon Web Services (AWS) instance. The input received
from the website will be given to the AWS-EC2
instance. This instance has all required files of machine
learning models, flask files, and the front-end web server.
Once the required models are dumped from the machine
learning model we will further use only those dumped
models in the flask file. For the front-end of the website,
we used HTML, CSS, and Bootstrap which will help us to
provide easy access to the webserver. The output received
will be again displayed to the webserver. This deployment
will help us to provide access to the webserver on any
internet-enabled devices. Since it uses responsive code,
the fluidity of the frontend makes it platform-independent.
Whenever we run the instance we will be getting a unique
public DNS which will provide us direct access to the webserver.
This public DNS will keep changing whenever we
run the instance.
4 Implementation and Results
For implementation and better result purpose we created
a dataset a CSV file we made it by using web scraping
method we scraped the news articles for authentic news
web sites and created a dataset having more than 20,000
news here below is the screenshot of the result of making
the dataset using web scraping[9] [10].
Further, a Jupyter Notebook was created to implement
the ML program. We have used Logistic Regression, Gradient
Boosting, k-nearest neighbors, Decision Tree, and
Random Forest. After TF-IDF vectorization and cleaning
the data we train and test them according to these
classifiers we got the accuracy for Logistic Regression as
85.04%, Decision Tree as 78.11%, Gradient Boosting as
77.44%, Random Forest as 84.50%, KN Neighbors Classifier
as 80.20%
The columns of Table 1 are defined as follows
• Classifiers: Models that are used to train and test data
are known as classifiers.
• Accuracy: How often a data point is correctly classified
by the algorithm.
• Precision: The number of accurately predicted positive
observations divided by the total number of predicted
positive observations.
ITM Web of Conferences 40, 03003 (2021)
ICACC-2021
https://doi.org/10.1051/itmconf/20214003003
3
#
#Classifiers Accuracy Precision Recall F1-Score
Logistic Regression 85 84 87 85
Decision Tree 78 79 79 79
Gradient Boosting 77 70 84 76
Random Forest 84 83 86 84
KN Neighbors 80 75 85 79
Table 1: Tabular comparison of the classifiers (in %)
• Recall: The percentage of accurately anticipated positive
observations to the total number of observations in
a class is known as recall.
• F1 Score: The F1 Score is the weighted average of Precision
and Recall.
Confusion Matrix provides the values needed to calculate
the F1 score, Recall, and Precision. It is a table that
demonstrates how well a classification model (or a classifier)
performs on a set of test data with known True values
[8].
Figure 2: User-Interface.
These prediction values are calculated for all classifiers
and then the average of all these prediction values
will be taken as the final percentage. Based on these values
we are setting a range that will help to find the per-
centage of the truthness of the news. The webserver we
created is displaying the result followed by the news. To
catch the attention of users we used some emoticons as
a symbol to show the results more effectively. The created
web server is platform-independent. It means that all
of the arrangements of the webserver will be independent
of devices. Bootstrap helped us to make our web server
device-independent. The final output is in the form of a
message which is different according to different percentage
values. It is displayed as shown in figure 2.
5 Conclusion
In this paper, we looked at a computerized model for verifying
news extracted from social media, which provides
expository demonstrations for recognizing fake news. Following
the demonstration that even the most basic algo-
rithms in domains such as AI and Machine Learning can
produce a reasonable result on such a critical issue as the
spread of fake news around the world. As a result, the
findings of this investigation suggest that systems like this
could be very useful and effective in dealing with this critical
issue. Web scraping is also a key part of this paper as
the scraped data will be based on real-time news and will
be more reliable than the ready-made datasets available all
over the internet. It is an efficient and fast process and also
it is very easy to maintain. The dataset used in this study
is expected to be used in arrangements that use machine
learning-based statistical calculations, for example, Logistic
Regression (LR), Decision Tree, Gradient Booster,
Random Forest, and KN Neighbours. In the future, the
prototype’s efficiency and accuracy can be improved, as
well as the proposed model’s user interface.
References
[1] Burkhardt, Joanna M. Combating fake news in the
digital age. Vol. 53, no. 8. American Library Association,
2017.
[2] de Lima Salge, Carolina Alves, and Nicholas Berente.
"Is that social bot behaving unethically?." Communications
of the ACM 60, no. 9 (2017): 29-31.
[3] Granik, Mykhailo, and Volodymyr Mesyura. "Fake
news detection using naive Bayes classifier." In 2017
IEEE First Ukraine Conference on Electrical and
Computer Engineering (UKRCON), pp. 900-903.
IEEE, 2017.
[4] Jain, Anjali, Avinash Shakya, Harsh Khatter, and
Amit Kumar Gupta. "A smart System for Fake News
Detection Using Machine Learning." In 2019 International
Conference on Issues and Challenges in Intelli-
gent Computing Techniques (ICICT), vol. 1, pp. 1-4.
IEEE, 2019.
[5] Mahir, Ehesas Mia, Saima Akhter, and Mohammad
Rezwanul Huq. "Detecting fake news using machine
learning and deep learning algorithms." In 2019 7th
International Conference on Smart Computing Communications
(ICSCC), pp. 1-5. IEEE, 2019.
[6] Yalçın, Mehmet Adil, Niklas Elmqvist, and Benjamin
B. Bederson. "Keshif: Rapid and expressive tabular
data exploration for novices." IEEE transactions on visualization
and computer graphics 24, no. 8 (2017):
2339-2352.
[7] Singrodia, Vidhi, Anirban Mitra, and Subrata Paul.
"A Review on Web Scrapping and its Applications."
In 2019 International Conference on Computer Communication
and Informatics (ICCCI), pp. 1-6. IEEE,
2019.
[8] Smitha, N., and R. Bharath. "Performance Comparison
of Machine Learning Classifiers for Fake News
Detection." In 2020 Second International Conference
on Inventive Research in Computing Applications
(ICIRCA), pp. 696-700. IEEE, 2020.
[9] Thomas, David Mathew, and Sandeep Mathur. "Data
analysis by web scraping using python." In 2019 3rd
International conference on Electronics, CommunicaITM
Web of Conferences 40, 03003 (2021)
ICACC-2021
https://doi.org/10.1051/itmconf/20214003003
4
#
#tion and Aerospace Technology (ICECA), pp. 450454.
IEEE, 2019.
[10] Diouf, Rabiyatou, Edouard Ngor Sarr, Ousmane Sall,
Babiga Birregah, Mamadou Bousso, and Sény Ndiaye
Mbaye. "Web Scraping: State-of-the-Art and Areas of
Application." In 2019 IEEE International Conference
on Big Data (Big Data), pp. 6040-6042. IEEE, 2019.
ITM Web of Conferences 40, 03003 (2021)
ICACC-2021
https://doi.org/10.1051/itmconf/20214003003
5
ANA BEDUSCHI
MARIE MCAULIFFE
#
#The opinions expressed in the report are those of the authors and do not necessarily reflect the views of the International 
Organization for Migration (IOM). The designations employed and the presentation of material throughout the report do 
not imply expression of any opinion whatsoever on the part of IOM concerning the legal status of any country, territory, city 
or area, or of its authorities, or concerning its frontiers or boundaries.
IOM is committed to the principle that humane and orderly migration benefits migrants and society. As an intergovernmental 
organization, IOM acts with its partners in the international community to: assist in meeting the operational challenges of 
migration; advance understanding of migration issues; encourage social and economic development through migration; and 
uphold the human dignity and well-being of migrants.
_____________________________
Publisher:  International Organization for Migration
 17 route des Morillons
 P.O. Box 17
 1211 Geneva 19
 Switzerland
 Tel.: +41 22 717 9111
 Fax: +41 22 798 6150
 Email: hq@iom.int
 Website: www.iom.int
Cover photo:  Real feelings. © Coralie Vogelaar 2020
Required citation: Beduschi, A. and M. McAuliffe, 2021. Artificial Intelligence, migration  and mobility: implications for policy 
and practice. In: World Migration Report 2022 (M. McAuliffe and A. Triandafyllidou, eds.). International 
Organization for Migration (IOM), Geneva.
_____________________________
ISBN 978-92-9268-136-4 (PDF)
© IOM 2021
Some rights reserved. This work is made available under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 
IGO License (CC BY-NC-ND 3.0 IGO).*
For further specifications please see the Copyright and Terms of Use.
This publication should not be used, published or redistributed for purposes primarily intended for or directed towards 
commercial advantage or monetary compensation, with the exception of educational purposes e.g.  to  be included in 
textbooks.
Permissions: Requests for commercial use or further rights and licensing should be submitted to publications@iom.int.
* https://creativecommons.org/licenses/by-nc-nd/3.0/igo/legalcode
#
#1WORLD MIGRATION REPORT 2022
 11 ARTIFICIAL INTELLIGENCE, MIGRATION  
AND MOBILITY: IMPLICATIONS FOR POLICY  
AND PRACTICE1
Introduction
Artificial intelligence (AI) technologies underpin everyday activities in more ways than many people imagine. Just as 
personalized newsfeeds utilize AI to service many millions of people worldwide, every Google search relies on AI 
algorithms to produce search results in mere milliseconds. AI-driven “smart” phones, “smart” appliances, “smart” 
houses and “smart” digital voice assistants (e.g. Alexa and Siri) are becoming increasingly commonplace in societies, 
enabling people to better manage their time, information and energy consumption. However, the development of 
AI capabilities has also triggered dire warnings from some futurists, including Stephen Hawking, Steve Wosniak and 
Elon Musk, who express concern over the long-term AI implications for humanity.2 Perhaps the most significant 
aspect relates to AI weapons development, but the broader difficulties in aligning AI with human values underpin 
many concerns.3
Alongside growth in other sectors, AI has been increasingly used in the context of migration and mobility. The 
deployment of such technologies is not new, although there has been a recent surge in interest in AI utilization 
in migration as part of the broader raised profile of AI and related concerns about its development.4 For many 
years, migration-related State authorities have used a variety of technologies, including AI systems, to support 
administrative processing and decision-making in matters related to migration. AI is increasingly used throughout 
the migration cycle, for example to facilitate pre-departure identity checks, support visa application lodgement and 
processing, enhance border procedures, produce data analytics on lodgement, applications and compliance with 
visa conditions (amongst other aspects), as well as to forecast migration trends. There can be benefits in applying 
AI technologies that are able to increase efficiency of systems and reduce processing times for clients, as well as 
better manage the increasing demand for migration-related services. 
However, AI poses a variety of issues for policymakers, practitioners and migrants, including concerns about 
technology-enabled surveillance of individuals, the potential for systemic bias in AI decision-making in the areas of 
migration and mobility, the increased interactions between public and private sectors and their competing interests, 
and the negative impact of AI technologies on the protection of migrants’ human rights. 
Data-driven AI technologies also occupy a central role in the fight against COVID-19. Many governments around 
the world have implemented measures to monitor public health, such as mobile phone applications for contact 
tracing and digital health passports.5 Such measures may disproportionately affect vulnerable groups, including 
migrant communities, thus impacting the protection of their human rights. 
1 Ana Beduschi, Associate Professor of Law, University of Exeter; Marie McAuliffe, Head, Migration Research and Publications Division, 
IOM.
2 Mack, 2015. 
3 Wolchover, 2015. 
4 Tegmark, 2016. 
5 McAuliffe and Blower, 2021.
#
#2 Artificial intelligence, migration and mobility: Implications for policy and practice
This chapter examines implications of AI for policy and practice in the context of migration and mobility through 
the prism of the existing international human rights framework of rules, standards and principles.6 This is important 
because of the potential for human rights to be eroded – or bolstered – as a result of the design, development, 
implementation and expansion of AI technologies around the world.7 The next section outlines key concepts and 
definitions, which is then followed by a brief precis of the current AI context. The use of AI across the migration 
cycle is then examined, with reflections on key strategic challenges and opportunities in this important area of new 
technology, including as it relates to the “future of work” and long-term migration trends. 
Key concepts and definitions
There is no single, universally agreed definition of AI, although in a broad sense it can be thought of as “the 
programming of computers to do tasks that would normally require human intelligence”.8
With its roots in computer science of the 1950s, AI was originally conceived to convey the aspirational development 
of a computer that would deliver the high-level or cognitive capability of humans to reason and to think – otherwise 
referred to as “general AI”.9 More than six decades later, however, high-level reasoning and thought remain elusive, 
and most of what is referred to as AI in non-technical discourse is a significant step down from this and is often 
more akin to a particular branch of AI called “machine learning”.10 This lower-order AI is referred to as “narrow 
AI”, as it relates to the performance of narrow tasks, such as matching facial features in images or calculating the 
relevance of written material to specific search terms, rather than broader, more general “thinking”.11 
From its historical emergence in computer science, AI has developed over time to encapsulate different streams 
that utilize machine capabilities for such work as natural language processing, speech processing, machine learning, 
visual recognition, neural networks and robotics.12 In reality, AI is not a single thing, but is a group of related 
technologies designed to match or replace human intelligence.13 An overview of different definitions of AI offered 
by organizations and leading scholars can be found in Appendix A.
AI-based systems can be purely software based, acting in the virtual world (e.g. voice assistants, image analysis 
software, search engines, and speech and face recognition systems) or AI can be embedded in hardware devices 
(e.g.  advanced robots, autonomous cars, drones and Internet of Things applications).14 It is also useful in the 
context of this chapter on migration and mobility to offer definitions of commonly used terms that relate to AI 
technologies, such as:
• Algorithms: These are sets of machine instructions used to process and solve problems. AI algorithms can 
analyse data, find patterns, make inferences and predict behaviour at a level and speed greatly surpassing human 
capabilities.15 
6 E.g. UN, 1966; UNHRC, 2011; UNHRC, 2008.
7 UN SG, 2020; Pizzi et al., 2020.
8 Mehr, 2017. 
9 Jordan, 2019; Tegmark, 2016. 
10 Jordan, 2019. 
11 Tegmark, 2016. 
12 McLaughlin and Quan, 2019.
13 Duan et al., 2019; Walsh et al., 2019. 
14 European Commission, 2018: para. 1; Accenture, 2018.
15 LeCun et al., 2015.  
#
#3WORLD MIGRATION REPORT 2022
• Machine learning: Machine learning is one of the techniques by which machines are trained to perform tasks 
that are generally associated with human intelligence, such as natural language processing.16 Machines learn from 
vast amounts of data, including big data sets, using algorithms.
• Deep learning: A subset of machine learning, deep learning imitates the functioning of the human brain and is 
increasingly being relied upon for image and face recognition.17 Deep learning applications structure algorithms 
into layers to create an artificial neural network, enabling machines to learn and make decisions on their own.18 
This makes it difficult or even impossible to explain how the machines reach specific decisions.19
• Big data: Big data can be defined as the “large volumes of high velocity, complex and variable data that require 
advanced techniques and technologies to enable the capture, storage, distribution, management, and analysis 
of the information.”20
• Digital identity: A digital identity refers to a set of attributes available in digital format and relating to a person 
or entity.21 These attributes include biometric data (e.g. fingerprint, eye scan, 3D face map), and demographic data 
(e.g. date and place of birth). They can also be combined with evidence of government-issued ID (e.g. passport, 
driver’s licence) and digital activities on social media, including search history online and geotagging data. Existing 
digital identity platforms use AI as well as blockchain-related technologies to verify the identity of individuals by 
enabling “digital identity wallets” to run via online platforms and mobile phone devices.22
• Chatbot: A computer programme designed to converse with humans, especially over the Internet.23
How are digitalization and AI related?
AI technologies rely on underlying data capture and digital capabilities in order to be applied. “Digitalization” 
of aspects of migration systems is, therefore, a necessary condition for the application of AI technologies. 
However, digitalization does not necessarily result in AI technologies being developed and implemented. 
Compared with digitalization, AI in migration and mobility is currently much more limited.
AI is routinely used in a variety of sectors, including agriculture, finance and banking, education and health care, as 
summarized in Appendix B.
16 Flach, 2012; Nilsson, 1982; Ertel, 2017. 
17 LeCun et al., 2015.
18 Ibid.
19 Rudin, 2019; Angelov and Soares, 2020; Watson and Floridi, 2020.
20 TechAmerica Foundation, 2012.
21 ISO, 2019. 
22 E.g. Sovrin, available at https://sovrin.org and Digital Identity Alliance, available at https://id2020.org.  
23 Cambridge Dictionary definition available at https://dictionary.cambridge.org/dictionary/english/chatbot (accessed 14 January 2021).    
#
#4 Artificial intelligence, migration and mobility: Implications for policy and practice
AI technologies throughout the migration cycle
Notwithstanding a recent surge in interest, AI technologies have actually been used in the fields of migration and 
mobility for many years.24 For instance, AI and related technologies have been used in Australia, the United States 
of America, Japan, many European countries and the European Union to manage increasing numbers of crossborder
movements.25 
This section situates the analysis within the historical context of uses of AI technologies in migration and mobility. 
It describes the key uses of AI throughout the migration cycle,26 providing examples of AI capability and deployment 
at the different stages: pre-departure, entry, stay, and return, noting that more examples can be found in relation 
to entry and stay. This section also describes the application of AI technologies in migration forecasting, through 
for example the use of predictive analytics. A summary of AI technologies throughout the migration cycle is 
provided in Figure 1. The next section then provides an analysis of the key issues and challenges, as well as the 
main benefits, that result from the increasing use of AI technologies in migration and mobility processes, especially 
in the context of human rights.
Figure 1. Artificial Intelligence and the migration cycle
  Stay
• Immigration information  
chatbots; 
• Visa application lodgement  
e-platforms;
• Migration application decision- 
making, including asylum claims;
• Chatbot legal advice for refugees;
• Refugee settlement placements;
• Job-matching;
• Facial recognition technology in mass 
surveillance to identify undocumented migrants;
• Chatbot psychological support;
• Distribution of humanitarian aid  
supported by digital identity systems.
  Return
• Returning migrant worker “smart card” in origin countries;
• Community detection for forced return;
• Return decision-making utilizing  
machine learning.
  Pre-departure
• Government visa information chatbots; 
• Visa application lodgement e-platforms;
• Automated profile and security checks; 
• Private sector visa service chatbots;
• Visa application decision-making.
Entry
• Chatbot information at arrival in 
humanitarian contexts;
• Automated identity verification at 
borders utilizing bio data;
• Automated security checks at borders 
utilizing biodata; 
• AI drone monitoring of borders;
• Behavioural analysis identifying hostile 
intent;
• Risk profiling.
24 ANAO, 2008. 
25 ANAO, 2012.
26 Gmlech, 1983; McAuliffe and Koser, 2017.
Source: McAuliffe et al., 2021.
#
#5WORLD MIGRATION REPORT 2022
AI technologies have been developed and deployed over many years to support pre-departure aspects of migration 
and mobility management. Several government authorities have for example harnessed emerging technologies in the 
areas of visa application lodgement e-platforms and pre-departure checking, including the use of biometric data.27 
In anticipation of the 2000 Olympics in Sydney, the Australian Government introduced a system of pre-departure 
checking of bio-data linked to passenger boarding, so that airlines could not board passengers and crew onto an 
aircraft unless they had been cleared to do so.28 Referred to as advanced passenger processing, this system was 
designed to draw on new technology and enhanced connectivity supporting real-time checking through border 
security systems. The more generic versions of these types of systems, known as advanced passenger information 
(API) systems, have subsequently been regulated by international guidelines covering their development and use 
globally.29 API systems are seen as a way to overcome a range of problems in managing the movements of people 
internationally, most especially related to the significant growth in global travel and projected further growth, 
but also in relation to security threats, including terrorism and drug smuggling; significant carrier penalty regimes; 
and efficiency gains for border agencies through greater automation.30 API requires automation of cross-checking 
processes involving multiple systems by utilizing AI capability to conduct searches and match biodata and other 
variables stored in different domains.
Alongside the growing use of AI technologies in border systems, online visa application platforms and the 
development of “e-visas”, processing systems have also allowed automated systems utilizing the analytical capability 
of machine learning to process routine visa applications and refer more complex applications to case officers.31 
Again, one of the first automated systems was developed by Australia in 1996 in anticipation of the surge of visitors 
attending the Olympics, which then led to further developments over subsequent years and reductions in staffing 
levels overseas previously needed to process routine visa applications.32 One of the early online visa application 
systems resulted in between 15 and 20 basic application checks being automated, thereby significantly reducing 
processing time and staffing costs.33 In the first online system, human visa officers were still required to make the 
final decision on the application. However, these online platforms have been further developed over time so that 
they can provide automated decisions for low-risk applications, including the use of profiling techniques that do not 
require a human visa officer to be involved.34 More complex cases or applications that do not “fit” the processing 
algorithms are then referred to visa officers for assessment and final determination.
More recently, there have been increased efforts to develop chatbots for information service functions provided by 
government authorities, as well as for private sector service providers such as commercial migration agents or visa 
application centres, to assist potential clients exploring opportunities to migrate for work or family reasons, study 
overseas or work temporarily in other countries.35 Chatbots have also been developed by migrants who themselves 
had struggled to navigate the vast amount of information (and misinformation) on visa and immigration regulations.36
27 ANAO, 2008; DIAC, 2008; Shelfer and Verner, 2003.
28 DIAC, 2008; Franzi, n.d.; WCO, IATA and ICAO, 2010.
29 WCO, IATA and ICAO, 2010. 
30 Ibid.
31 Aggarwal, 2018; PwC, 2011; Molnar, 2018. 
32 PwC, 2011.
33 Rizvi, 2004.
34 Ibid.
35 E.g. https://hellotars.com/chatbot-templates/travel/H1mUrB/immigration-services-chatbot.
36 Hemmadi, 2017. 
#
#6 Artificial intelligence, migration and mobility: Implications for policy and practice
Entry
In a similar vein to changes in pre-departure processes, the management of entry-related processes, especially 
those directly focused on borders, has seen automation and enhanced analytical capability being increasingly utilized 
to improve efficiency and manage increasing passenger numbers. Automated border gates using biometric and 
biographical data for identity and security-related checking require substantial investments in data collection, IT 
systems and AI capabilities such as machine learning.37 As a result, many countries are unable to roll out such 
sophisticated systems, leaving them to rely on manual systems and traditional border guard assessment protocols 
to detect potential integrity issues.38 In addition, there can be challenges in initial implementation and questions 
regarding sustainability. In South Africa, for example, the introduction of biometric technology (fingerprint and facial 
recognition technology) as part of the government’s modernization programme, initially at O.R. Tambo International 
Airport, caused delays due to the time required to collect passenger biometrics.39 As a result, only non-nationals’ 
details were collected, which goes to the broader issue of digital capabilities underlying the implementation of AI 
functionality. The broader issue of asymmetrical power between States in relation to AI migration technologies is 
discussed in the text box below. 
Other areas that have seen a rapid rise in AI technologies have been in border detection systems, such as AIbacked
drone technology,40 as well as behavioural analysis in public locations, including airports and other mass 
transit facilities.41 This AI-driven behavioural analysis utilizes machine learning to read biometric data such as facial 
(micro)expressions, gait and other physical movements to identify those intent on causing potential harm to others, 
although such approaches have been highly contested for reasons related to (in)accuracy, intrusiveness and privacy.42 
Other highly contested initiatives include the so-called “virtual border wall” between the United States and Mexico, 
currently being developed by United States Customs and Border Protection (CBP) in partnership with leading 
tech firms as part of CBP’s innovation programme.43 If this goes ahead, the “virtual border wall” will involve mass 
surveillance via drones and towers deploying capabilities similar to Google’s Vision AI product, which can rapidly 
detect and categorize people and objects in an image or video file.44
Deepening asymmetries between States
The deployment of AI technologies can deepen such asymmetries in two main ways. First, it can amplify 
the so-called digital divide between States with more advanced technological capabilities and those lacking 
such technologies.a AI enthusiasts’ main claim is that it can be used to cut costs and increase efficiency.b AI 
technologies would, therefore, be advantageous for migration and asylum procedures, which are normally 
lengthy, primarily manual, and largely based on migrants’ and asylum-seekers’ claims. 
Accordingly, AI technologies could cement the leading position of those AI-capable States, which would 
be placed at the forefront of the global efforts to manage migration in the years to come. Such a situation 
would create an AI divide. In this new paradigm, States with less advanced technological means could be 
37 Thales Group, n.d.; WCO, 2019.
38 Heath, 2019; IOM, 2016.
39 Darch et al., 2020.
40 Campbell, 2019; Koslowski, 2005.  
41 Al Hamar et al., 2018; Rawlings, 2019.
42 Al Hamar et al., 2018; Huszti-Orbán and Ní Aoláin, 2020; Jupe and Keatley, 2019.
43 Fang and Biddle, 2020. 
44 Ibid.; Google Cloud, 2020.
#
#7WORLD MIGRATION REPORT 2022
further isolated. … Besides, the AI divide could either reinforce or, conversely, represent a shift from the 
North–South paradigm.c If those AI capabilities concentrate in the global North, the AI divide would rather 
reinforce the existing North–South paradigm. However, if States in the global South take the opportunity to 
develop their AI capabilities, this could give them an additional means to exert influence in matters related to 
migration management as fully fledged AI-capable States. … Accordingly, the AI divide could simultaneously 
contribute to deepening the already asymmetrical relationships between North–South States, while shifting 
the focus slightly towards what could come to be the “AI-capable States and the others” split in international 
migration management.
Source: Abridged extract from Beduschi, 2020a.
a  Norris, 2001.
b  Chui et al., 2018.
c  Chetail, 2008.
Stay
Chatbots are increasingly featuring in the provision of information and advisory services to migrants in destination 
countries, and have been developed by government authorities, such as Finland’s immigration robot assistant, called 
Kamu,45 as well as by civil society organizations supporting migrants. For example, a chatbot called Mona, designed 
to provide refugees with basic legal advice, has been developed by the United States-based startup Marhub in 
an effort to provide accurate information in real time.46 We have also seen the development of chatbots that 
provide psychological support to refugees and internally displaced persons, who are often extremely vulnerable and 
unable to access mental health services. One such initiative by United States-based tech company X2AI involved 
partnering with a non-governmental organization in Lebanon to provide Arabic-language support via a chatbot 
called Karim, which delivers personalized text messages to Syrian refugees, using natural language processing and 
cognitive behavioural therapy capabilities.47 Karim is an offshoot of an initial mental health chatbot called Tess, which 
delivers services to more than 19 million people worldwide.48
Application e-platforms and visa-related decision-making are similar to the pre-departure processes discussed above; 
however, after people have entered a country they may have ongoing interactions with immigration authorities, 
such as to renew visas, apply for a new visa type, or demonstrate compliance with visa conditions through staged 
processing. The use of AI has been found to reduce the need for manual processing and in-person appointments, 
such as has been achieved in Hong Kong SAR, China, where the immigration department’s eBrain system reduced 
processing times and community costs, as described in the text box below.49 
45 Miessner, 2019. 
46 Peters, 2019. 
47 Solon, 2016; Sengupta, 2019. 
48 See www.x2ai.com. 
49 Wong and Chun, 2006. 
#
#8 Artificial intelligence, migration and mobility: Implications for policy and practice
Hong Kong SAR, China immigration department’s eBrain system
In early 2006, the Hong Kong SAR, China immigration department introduced a new eBrain system using AI 
capability to improve case processing. A visual representation of eBrain’s overall AI architecture demonstrates 
the complexity of the overall scheme and the way in which machine-learning aspects feature within its 
architecture.
Closed
application
approve/refuse
Assessment
Rule-base
Schemas
Case-base
Dtree
Procedures
Workow
Case learning
Dtree Learning
Rule
Admin
Schema
Admin
Dtree
Admin
Case
Admin
legal references
self-learned result
recommendations
follow-up actions
workow routing
related cases
eBrain Rule Engines
New self-learned rules
Knowledge Base
eBrain Self-learning
Engine
eBrain Case-based
Reasoning Engine
New
application
Rule
Core
CBR
Core
Learning
Core
In addition to e-lodgement of applications, the eBrain system is used for case management and decisionmaking.
After lodgement, the eBrain schema-based reasoning engine generates a set of suggested actions, 
such as requesting additional documentation, to get the application to a state that can be assessed. Machine 
learning is used to build procedural knowledge of typical steps taken for different kinds of cases.
Source: Wong and Chun, 2006.
Machine learning incorporating algorithmic data analysis has also been tested and used to support better refugee 
resettlement placement in Switzerland, which had previously been performed by human case officers, resulting in 
improved outcomes for refugee integration.50 Other similar initiatives – such as the global Matching and Outcome 
Optimization for Refugee Empowerment (MOORE) initiative – utilize machine learning, integer optimization and 
50 Bansak et al., 2018. 
#
#9WORLD MIGRATION REPORT 2022
matching theory to determine the best matches between refugees and local communities.51 Refugee resettlement 
optimization applications using AI technologies are fairly limited and specialized, although it is not clear whether 
similar systems are already in place for other migration programmes in destination countries, such as those for 
skilled workers. 
One deployment of AI technologies that existed well before COVID-19, but which came under the spotlight 
during the pandemic, was related to the use of AI surveillance technology to track and monitor groups of interest, 
including migrants.52 In the United States, for example, AI facial recognition technology has been used by immigration 
authorities to conduct mass monitoring of people in traffic flows to detect undocumented migrants.53 Similar 
capabilities relying on facial recognition software have been implemented in many other countries, as outlined in 
the text box below. 
Global expansion of AI surveillance
AI surveillance technology is spreading at a faster rate to a wider range of countries than experts have 
commonly understood. At least 75 out of 176 countries globally are actively using AI technologies for 
surveillance purposes. This includes: smart city/safe city platforms (56 countries), facial recognition systems 
(64 countries), and smart policing (52 countries).
Liberal democracies are major users of AI surveillance. The index shows that 51 percent of advanced 
democracies deploy AI surveillance systems. In contrast, 37 percent of closed autocratic states, 41 percent 
of electoral autocratic/competitive autocratic states, and 41 percent of electoral democracies/illiberal 
democracies deploy AI surveillance technology. Governments in full democracies are deploying a range of 
surveillance technology, from safe city platforms to facial recognition cameras. 
Governments in autocratic and semi-autocratic countries are more prone to abuse AI surveillance than 
governments in liberal democracies. Some autocratic governments are exploiting AI technology for mass 
surveillance purposes. Other governments with dismal human rights records are exploiting AI surveillance in 
more limited ways to reinforce repression. Yet all political contexts run the risk of unlawfully exploiting AI 
surveillance technology to obtain certain political objectives.
Source: Abridged extract from Feldstein, 2019. 
Return
In the context of return migration, there appears to be much less utilization of AI technologies. Insofar as return 
relates to programme integrity, such as the return of failed asylum seekers, visa overstayers or unlawful non-citizens, 
AI technology does appear to be utilized to facilitate return processes; however, its implementation appears to 
be more related to mass population surveillance initiatives to detect potential undocumented migrants for forced 
return.54 There are also indications in court documents from the United States showing that private sector data 
51 Trapp et al., 2018. 
52 IOM, 2020. 
53 Matyus, 2020. 
54 Majidi et al., 2021.
#
#10 Artificial intelligence, migration and mobility: Implications for policy and practice
brokers, which use AI algorithms to pull together a wide range of data on individuals to create “data dossiers”, 
are being utilized to identify potential deportees.55 In this sense, both physical surveillance and virtual surveillance 
techniques that utilize AI technologies are being drawn upon to identify potential returnees. 
The consequences of relying on AI technologies in forced return can be very significant, especially if there is 
overreliance on such systems without other forms of human assessment or verification. In 2016, the United 
Kingdom revoked the visas of around 34,000 international students on the basis of a contracted language services 
company’s AI human voice recognition analysis indicating that the students had used proxies in English language 
tests needed to secure visas. However, subsequent human analysis found that around 7,000 (or 20%) of these 
students had been falsely accused of cheating, with the United Kingdom immigration appeals tribunal finding that 
the evidence used by the Home Office to deport the students had “multiple frailties and shortcomings”.56 
The use of AI technologies in the return of migrant workers back to their origin countries has had some traction, 
but appears to suffer sustainability and implementation obstacles. In Bangladesh, for example, the introduction of 
“smart cards” in 2010 to support smoother departure processes of migrant workers has been useful for border 
processing and data collection purposes, but calls to utilize these cards also for returning migration workers 
have not eventuated.57 Multipurpose smart cards would assist in further movement away from a paper-based 
migration management system to a digital system, enabling better data collection and related analysis for policy 
and programming purposes.58
AI as a tool to predict movements, while also shaping long-term trends
Forecasting migration and mobility has been undertaken for many years and has typically relied on statistical 
modelling, as well as expert insights. With the expansion of data sets, especially in humanitarian displacement 
contexts (such as IOM’s Displacement Tracking Matrix), there is an increasing focus on the use of AI technologies 
to leverage these data.59 Unsurprisingly, predictive analytics is increasingly being used in the context of humanitarian 
settings; recent research has highlighted the growth in AI technologies to predict humanitarian crises, including 
displacement impacts due to conflict and violence, food insecurity, disease outbreaks and disaster.60 Of 49 initiatives 
utilizing AI technologies, such as machine learning, big data and statistical modelling, the researchers found that 
the focus was on where humanitarian crises will occur (71% of initiatives) and who will be affected (40%).61 Less 
emphasis was placed on what such predicted crises will involve and when crises are likely to occur. 
While we see a trend relating to the scaling-up of predictive analytics technologies and forecasting applications, the 
analysis of how AI technologies are likely to impact international migration patterns over time is of more significance 
in strategic terms. This is because more and more occupations are being automated or otherwise replaced by 
computers, which in turn has the potential to affect migration dynamics worldwide. AI and the future of work is a 
strategic topic of keen interest to many, with some arguing that it is the traditional white-collar jobs that are most 
at risk.62 This highly topical issue is summarized in the text box below.
55 Currier, 2019; Molnar 2019; Rivlin-Nadler, 2019. 
56 Baynes, 2019. 
57 Bhuyan, 2018; Rashid and Ashraf, 2018.
58 Rashid and Ashraf, 2018.
59 Bither and Ziebarth, 2020.
60 Hernandez and Roberts, 2020. 
61 Ibid.
62 Hanke, 2017.
#
#11WORLD MIGRATION REPORT 2022
How will AI impact long-term migration patterns?
While it is unclear exactly how migration patterns will be affected, analysis points to significant shifts over 
time as countries seek to invest in AI in an increasingly diverse range of sectors, from health and social 
care, to agriculture, education and finance (see Appendix B). As part of the broader considerations of the 
future of work, automatization utilizing AI is expected to affect the economics that underpin migration, 
reducing the demand for migrant workers over the long term. These changes are likely to affect many labour 
markets globally, with significant destination regions for migrants workers not immune. In a report focusing 
on Bahrain, Egypt, Kuwait, Oman, Saudi Arabia and the United Arab Emirates, for example, researchers find 
that the automation of routine work is particularly relevant to migrant workers in these countries, as the 
majority are low skilled. In the United Arab Emirates, for example, more than 93 per cent of automation 
potential affects jobs held by migrant workers.
Sources: aus dem Moore et al., 2018; Ernst et al., 2018; Hanke, 2017; Hertog, 2019.
AI technologies in migration and mobility: key issues, challenges and 
opportunities
As AI systems become increasingly common throughout the migration cycle, they give rise to a variety of issues 
and pose significant challenges for the protection of migrants’ human rights. 
This section analyses these issues through the prism of the human rights law framework, identifying challenges, 
but also possible available opportunities. It elaborates on the descriptive analysis presented in the previous section 
by focusing on critical areas in which AI has a significant impact: visa and asylum processing and decision-making; 
border security and migration management; support to migration management; and migration and mobility in an 
interconnected world. 
Visa and asylum processing and decision-making
AI technologies are frequently used for visa and asylum processing and decision-making. A key advantage of using 
AI systems is that they can speed up visa and asylum application processing while screening for security threats 
and reducing irregular migration. However, AI technologies make it possible to automate, often in untransparent 
ways, large-volume processing involving risk profiling, with limited transparency and often without the possibility 
of recourse.63 
The lack of transparency and the presence of biases in AI algorithms is a widespread concern, extending well 
beyond migration. While humans also display biases in their decision-making independently of the use of AI, AI 
systems can amplify existing human biases, not just encode them. AI thus has the potential to institutionalize and 
systematize human bias. This can ultimately lead to discrimination and exclusion of people based on protected 
63 McCarroll, 2020; Molnar and Gill, 2018. 
#
#12 Artificial intelligence, migration and mobility: Implications for policy and practice
characteristics, including race and ethnicity.64 Bias is a common issue that permeates AI systems in a variety of 
sectors.65 
Typology of biases in algorithms 
1.  Historical bias arises when there is a misalignment between the world as it is and the values or 
objectives to be encoded and propagated in a model. It is a normative concern with the state of the 
world, and exists even given perfect sampling and feature selection.
2.  Representation bias arises while defining and sampling a development population. It occurs when the 
development population under-represents, and subsequently fails to generalise well, for some part of the 
use population.
3.  Measurement bias arises when choosing and measuring features and labels to use; these are often 
proxies for the desired quantities. The chosen set of features and labels may leave out important factors 
or introduce group or input-dependent noise that leads to differential performance.
4.  Aggregation bias arises during model construction, when distinct populations are inappropriately 
combined. In many applications, the population of interest is heterogeneous and a single model is unlikely 
to suit all subgroups.
5.  Evaluation bias occurs during model iteration and evaluation. It can arise when the testing or external 
benchmark populations do not equally represent the various parts of the use population. Evaluation bias 
can also arise from the use of performance metrics that are not appropriate for the way in which the 
model will be used.
6.  Deployment bias occurs after model deployment, when a system is used or interpreted in inappropriate 
ways.
Source: Abridged extract from Suresh and Guttag, 2020. 
In the context of migration and mobility, the consequences of biased AI algorithms can be life-changing. For 
example, there is potential for visa applications to be rejected because the AI algorithms used for the initial triage 
do not correctly recognize darker skin complexions and misidentify applicants. Such a scenario is not far from 
reality. Facial recognition technologies are considerably less accurate when used to recognize darker-skinned female 
faces when compared with white male faces.66 Commercially available facial recognition AI systems were also proven 
to be more prone to misidentifying black people’s faces and matching them with faces of people who had previously 
been arrested by the police, in an investigation in the United States.67 
64 Eubanks, 2018; Ferguson, 2017; Noble, 2018; Zuboff, 2019.
65 Creemers et al., 2015; Zou and Schiebinger, 2018. 
66 Buolamwini and Gebru, 2018. 
67 Snow, 2018. 
#
#13WORLD MIGRATION REPORT 2022
These inaccuracies in identifying darker-skinned people’s faces may be caused by a representation bias, due, for 
example, to a lack of diversity in the data sets used to train the AI algorithms. This effect may also be the result 
of a historical bias, reflecting decades of preconceptions and stereotypes in society. Technology is indeed shaped by 
long-standing cultural and context-based perceptions about race, ethnicity, gender and other inequalities prevalent 
in society.68 
These illustrations are an important reminder that technology is not a neutral tool and that it can also make 
mistakes. Decision makers should be aware of this. They should also take into consideration the propensity of 
human beings to favour the suggestions presented by AI systems, even if there are indications that these are 
mistaken, a phenomenon known as automation bias.69 Further, biometric matching involving algorithmic capabilities 
can be very difficult for humans to refute, especially where technology is able to extract information not able to be 
detected by the human eye (e.g. gait patterns, facial recognition and iris matching).70 Therefore, procedures should 
be in place to allow individuals to obtain redress in case mistakes in the AI systems lead to erroneous decisions 
or even to violations of their human rights. Individuals should be able to challenge decisions made by or with the 
assistance of AI systems before an independent and impartial tribunal or authority, including through administrative 
procedures.71 To enable procedural fairness, State authorities need to be transparent about how they use AI in visa 
and asylum processing and decision-making. 
Despite recent progress, there are still many technical hurdles preventing AI systems from being fully explainable 
by humans, notably the more complex AI models, such as deep neural networks.72 These are systems designed to 
learn by themselves through “thought processes” that are not fully explicable. Increasing reliance on AI technologies 
for visa and asylum processing, given this inherent unpredictability and opacity, risks compromising fairness and 
equity of processes. 
Border security and migration management
As discussed above, AI systems are also present in the field of border security and migration management. They 
are used, for example, to automate identity verification at borders, to automate security checks and monitor hostile 
intention at borders, or even to monitor borders remotely using sensors and AI-powered drones. 
There are advantages in using AI in the context of border security and migration management. AI systems can 
analyse vast amounts of data, including big data, to identify patterns and predict behaviour.73 They do so at 
speeds vastly surpassing human capability. AI algorithms can thus increase efficiency by streamlining repetitive 
tasks that depend on the review of large amounts of data.74 Depending on how they are designed, developed, 
and deployed, AI systems can fast-track identity verification at border crossing points. They can also contribute 
to better identification of individuals posing potential threats to national security, public safety and immigration 
programme integrity. 
68 See UNGA, 2011; UNHRC, 2020a. 
69 Huszti-Orbán and Ní Aoláin, 2020; Wickens et al., 2015. 
70 Israel, 2020.
71 This is according to the International Covenant on Civil and Political Rights (ICCPR) (UN, 1966: art. 14) and UN HRCttee (2007). 
72 Graves and Clancy, 2019; Pasquale, 2015; Watson and Floridi, 2020; Vilone and Longo, 2020. 
73 Burrell, 2016. 
74 Chui et al., 2018. 
#
#14 Artificial intelligence, migration and mobility: Implications for policy and practice
However, AI systems also bring many risks for the protection and respect of migrants’ human rights in the context 
of border security and migration management. First, there are significant concerns about the respect for individuals’ 
right to privacy. Under international human rights law, everyone has the right to respect for their private life 
and correspondence, which includes personal information in digital formats.75 Measures restricting the right to 
privacy must only be taken to safeguard a legitimate interest, which includes national security and public safety. 
They must also satisfy the cumulative tests of legality, necessity and proportionality.76 The legality test requires 
that measures adopted by States have a legal basis in domestic law and are compatible with the rule of law. They 
must be accessible and foreseeable and afford adequate legal protection against arbitrariness.77 The necessity test 
demands that the measures adopted address a pressing social need.78 The proportionality test requires that the 
measures taken by public authorities are proportionate to their legitimate aims and represent the least-restrictive 
viable solution.79
Second, there are concerns about the collection and use of sensitive personal information, such as biometric 
data. In particular, a central problem relates to the modalities of storage, processing and access to the data by 
different public authorities and services. These concerns are heightened by the establishment of interoperable 
information technology (IT) systems. Interoperability allows data to be available and easily shared between different 
IT systems, including those used for the management of border security and migration. In the European Union, 
a dedicated agency called eu-LISA oversees the implementation of interoperable IT systems concerning asylum, 
border management and migration.80
There is no doubt that, on the one hand, interoperability can improve AI systems, enhance security and provide 
for better identity management.81 For example, if data sets lack interoperability because they are only available 
in incompatible or different proprietary software, AI systems that are heavily data-driven will not reach their full 
potential. However, on the other hand, interoperability may also allow for mistakes in one data base to be cascaded 
forward if they are not quickly identified and corrected. For example, a person could be erroneously identified as 
a security threat with this information being recorded in one data base. If this mistake is not promptly corrected, 
border authorities that might access information in that data base could refuse entry and unfairly place this person 
in detention. The person in this scenario may not even be aware of the mistake in the data base. The lack of 
transparency could make it more difficult to rectify the error and allow the individual to obtain redress for the 
violation of his or her right to liberty.82 Accordingly, interoperable IT systems should be developed in line with 
the basic principles of data protection. These involve lawfulness, fairness and transparency, purpose limitation, data 
minimization, accuracy, storage limitation, integrity, confidentiality and accountability.83 
Third, there is a growing risk that the technologization of borders, using AI technologies such as AI-powered drones 
and/or AI-driven behavioural analysis of biometric data, may lead to excessive technology-enabled surveillance of 
75 Universal Declaration of Human Rights (UNGA, 1948: art. 12). See also ICCPR (UN, 1966: art. 17); European Convention on Human 
Rights (CoE, 1950: art. 8); American Convention on Human Rights (OAS, 1969: art. 11); UNGA, 2014; UN HRCttee, 1988.
76 UNGA, 2014: para. 23.
77 UNGA, 2014: para. 23; UN HRCttee, 1988; ECtHR, 2008: para. 95.
78 UNGA, 2014: para. 24; ECtHR, 2008: para. 101.
79 Ibid.
80 Available at www.eulisa.europa.eu.  
81 European Commission, 2017. 
82 ICCPR (UN, 1966: art. 9). 
83 EU, 2016: art. 5.  
#
#15WORLD MIGRATION REPORT 2022
individuals.84 While surveillance may be necessary to meet national security and public safety interests, measures 
that disproportionately interfere with people’s privacy are not tolerated in a democratic society.85 
AI supporting migration management
AI systems are also used to facilitate access to services and the integration of migrants and refugees throughout 
the migration cycle. For example, chatbots can now provide legal advice and psychological support to migrants 
and refugees, AI-powered applications can assist with refugee settlement placements, and digital applications can 
be used to support migrant integration in the host country. Chatbots have also been developed by migrants as a 
result of their own experiences in navigating vast amount of (mis)information on visa regulations and processes, as 
highlighted in the text box below.
Botler: the A.I. chatbot developed by a migrant
Amir Moravej’s body may have been in Tehran, but his mind was in Montreal. The engineer had spent a half 
decade in Canada, but an expiring work permit forced him to leave the country and return to his native Iran. 
Back home, Moravej scoured immigration forums and joined group chats where applicants shared advice and 
information about their cases. “It was impossible for me to read all of it,” he recalls. “So I wrote a bot to go 
and read all the forum posts, and find the ones that were most relevant to my own case.”
A little over a year ago, Moravej landed in Montreal once more, this time as a permanent resident. His 
creation has become Botler, an immigration tool powered by artificial intelligence…The first scheme Botler 
is being applied to: the Programme de l’expérience québécoise (PEQ), for foreign workers and students 
residing in Quebec. “Imagine you don’t have any information about the [program] but you want to apply,” 
instructs Moravej. “You can use this bot for the whole process, from the very beginning to the very end.”
Users start by answering questions about their qualifications and circumstances, which allows Botler to 
determine if they’re eligible for the program. Would-be applicants who meet the criteria then upload their 
documents, which the tool reviews. “If everything is fine, the bot will create an application package” that can 
be submitted to the immigration ministry, explains Moravej. Users who miss the mark get to see what gaps 
remain in their application, and what conditions they must meet to become eligible.
Botler’s machine learning engine uses the guidelines published by Quebec’s immigration department, and was 
trained on anonymized data from real cases. Lawyers at Montreal firm Campbell Cohen, which is partnering 
with the startup, conducted the product’s quality assurance testing. 
Source: Abridged extract from Hemmadi, 2017. 
84 E.g. UNHRC, 2020b.
85 UNGA, 2014: para. 24; Molnar, 2020.
#
#16 Artificial intelligence, migration and mobility: Implications for policy and practice
At the same time, this trend has also raised concerns about the limits of “techno-solutionism”, or the attempt to 
use technology to fix all sorts of problems.86 Migration is indeed a complex phenomenon that cannot be easily 
managed.87 In this sense, it cannot be easily “fixed” by technology either.
Technology may indeed enable capabilities and function as an equalizer of societal disparity.88 For example, digital 
identity initiatives may provide excluded individuals, such as migrants and refugees who cannot prove legal identity, 
with the means to open a bank account and access a variety of services in a host country.89 AI technologies can 
also reduce processing times and eliminate or reduce the inconvenience and time required for in-person interviews 
with immigration case officers, which often need to be booked weeks in advance (see text box below). However, 
their implementation is not without risks, as discussed in the next section. 
AI visa e-platforms and client service processing improvements
Paper-based manual visa application lodgement and processing systems require hard copies of application 
forms to be submitted by the applicant, together with copies of relevant documents. Case officers 
knowledgeable about the relevant laws, regulations and guidelines then review applications and often request 
additional supplementary documents from the applicant. When all the supporting documents have been 
submitted, and an interview (if needed) conducted, the case officer will then make an assessment, which 
may require endorsement by a more senior colleague. The entire process may require several visits by the 
applicant; total processing times can be from several days to several months, depending on the visa type and 
complexity of the case.
Through the use of AI technologies, visa application lodgement processes can become much faster and 
eliminate the need for visits to immigration offices. Straightforward, low-risk applications can be submitted 
online, paid for and processed within minutes, reducing the inconvenience of in-person visits and allowing 
much faster decisions. These capabilities also allow for the more complex and/or higher-risk cases to be 
handled by human immigration officers. Such systems, as highlighted above, have been in place in some 
countries for more than two decades; however, they require significant information and communication 
technology (ICT) investment and they can only work effectively where the ICT accessibility for clients is 
high. In some regions and countries, the lack of ICT risks undermining service delivery and can result in 
inability to access online e-platforms. For example, there exists a digital divide between developed and 
developing countries, with 81 per cent of individuals using the Internet in developed countries, compared 
with 40 per cent in developing countries and only 15.6 per cent in the least developed countries. Further, 
there is also a digital gender gap, with higher Internet access rates for men than for women in all regions of 
the world; globally, men’s access rate is 51 per cent and women’s is 44.9 per cent.
Sources: Aggarwal, 2018; IOM, 2016; ITU, 2019; Rizvi, 2004; Wong and Chun, 2006.
86 Latonero and Kift, 2018; Morozov, 2014.  
87 Castles, 2004.  
88 Haenssgen and Ariana, 2017; Beduschi, 2019. 
89 See, for example, Digital Identity Alliance, available at https://id2020.org and The Rohingya Project, available at https://rohingyaproject.
com. 
#
#17WORLD MIGRATION REPORT 2022
Migration and mobility in an interconnected world
As the world becomes increasingly interconnected, technology gives people the means to access more and 
more sources of information. This profoundly influences and impacts people’s strategies and decisions to move.90 
For example, mobile phone technology enables migrants to stay in touch with family, friends and humanitarian 
organizations, but it is also exploited by smugglers and a variety of criminal networks, including via social media 
applications.91 GPS and geolocation technologies embedded in mobile phones allow people to find and compare 
routes while on the move. Search engines are frequently used to gather information about transit and destination 
countries. While using these technologies, people leave behind significant digital footprints that can be exploited 
and analysed. Data-driven AI systems build on such varieties of available data. 
Two main sets of challenges are particularly relevant for data-driven AI systems used in migration and mobility.
First, the growing “datafication” of migration management, whereby different types of data including biometric, 
satellite and big data are increasingly collected, stored and used for migration management, can lead to critical 
issues.92 Poor practices in the collection, including storage and analysis of data from vulnerable groups such as some 
migrants and refugees, can have significant consequences. Cybersecurity flaws and poor storage practices could 
expose sensitive information about migrants and refugees.93 This can have dangerous consequences for their safety 
if the data fall into the hands of malicious actors or persecuting agents. 
Further, mistakes in data sets used to train AI algorithms can be cascaded forward if they go undetected. If these 
are proprietary algorithms or “black boxes”, they can be even more difficult to audit, making it harder to identify any 
errors.94 Such mistakes can have devastating effects. For instance, people may be denied access to essential services 
if they are misidentified due to an error in the software used to collect biometric data or to recognize people’s 
faces. If these are humanitarian services provided within the context of a situation of conflict, the consequences 
for the affected people can be even more ravaging.95
Second, such concerns about the “datafication” of migration and mobility are further exacerbated by the increasingly 
common interactions between the public and private sectors. The private sector plays a central role in designing 
and developing the technologies that will later be deployed by States and international organizations at all stages 
of the migration cycle. 
Technology companies have been positioning themselves in the humanitarian and migration arena for many years.96 
Such public–private interactions raise concerns for data protection. For instance, data-sharing practices and access 
to sensitive data by private corporations should only occur when there are sufficient measures in place to safeguard 
the basic principles of data protection.97 
90 McAuliffe and Goossens, 2018. 
91 McAuliffe, 2016. 
92 Broeders and Dijstelbloem, 2016:242–260; Beduschi, 2019.
93 Parker, 2020a; Parker, 2020b.
94 Pasquale, 2015.
95 See International Red Cross and Red Crescent Movement, Humanitarian Crises Digital Dilemmas, available at https://digital-dilemmas.
com.
96 Molnar, 2019; Parker, 2019; Kinstler, 2019.
97 Kuner and Marelli, 2020.
#
#18 Artificial intelligence, migration and mobility: Implications for policy and practice
More broadly, States, international organizations and the private sector are often motivated by potentially conflicting 
interests. For example, private companies may logically follow their commitment towards profit-making and the 
safeguarding of the interests of their shareholders; whereas States defend the public interest, while international 
organizations need to act within the limits of their mandate to protect the interests of their beneficiaries. These 
different motivations are subsequently reflected in the design and development of AI systems. Some argue that 
profit-making interests frequently prevail.98
Conversely, migrants’ interests and the protection of their rights are often unaccounted for in the design, 
development and deployment of these technologies. For example, activists and experts have raised the alarm 
about private sector access and control over migrants’ data, often without any meaningful consent.99 There are 
also reservations about international organizations’ practices in the field, which create additional bureaucracies and 
may hinder the protection of migrants and refugees.100 Scholars have also pointed out the challenges concerning 
the monetization of the insights gathered from migrants’ data and the incentives in maintaining a crisis narrative.101 
A human rights-based approach is, therefore, needed to address these issues and rebalance the power structures at 
play. International human rights treaties and the United Nations Guiding Principles on Business and Human Rights 
offer a comprehensive framework for algorithmic accountability.102 Similarly, States, international organizations and 
technology companies should adhere to the humanitarian “do no harm” imperative when designing, developing and 
deploying AI systems throughout the migration cycle.103 This ensures that once deployed, such AI systems do not 
damage the populations they are intended to serve.  
Moreover, States and international organizations can require that providers of AI technologies, including private 
sector suppliers, abide by human rights standards and basic principles of data protection. They can use public 
procurement processes for this purpose.104 These can include specific clauses in public procurement notices 
requiring that suppliers implement technical and organizational measures to integrate data protection principles 
into AI systems by design and by default.105 They can also request that AI technology providers assess the impact 
of their products against human rights standards before the deployment of these AI systems.106 Such practices can 
increase algorithmic fairness and accountability and prevent situations in which these technologies are tested on 
vulnerable populations, such as some migrant groups, without the prior assessment of risks. 
98 Madianou, 2019; Zuboff, 2019.
99 See, for example, Molnar, 2019 and Madianou, 2019. See also UNGA, 2019; UNHRC, 2020a, 2020b.
100 Duffield, 2016; Read et al., 2016; Latonero, 2019.
101 Taylor and Meissner, 2019.
102 McGregor et al., 2019.
103 Sandvik et al., 2017.
104 Martin-Ortega and O’Brien, 2019; Beduschi, 2020b.
105 Kuner and Marelli, 2020.
106 Danish Institute for Human Rights, 2020.
#
#19WORLD MIGRATION REPORT 2022
Conclusion
AI in migration and mobility is not a new phenomenon. However, the increase in computational power, advances 
in technologies and the availability of large amounts of data have provided fertile ground for the contemporary 
development and expansion of AI in this area.
This chapter framed the analysis of the uses of AI within each stage of the migration cycle, demonstrating that 
such technologies have already influenced pre-departure, entry, stay and return policies and practices. Additionally, 
it acknowledged how the expansion of AI in labour markets is expected to impact long-term migration patterns, 
as technologies and automation increasingly affect the future of work around the world.
AI certainly brings about a series of advantages for policy and practice. For example, AI systems can increase the 
efficiency of migration management by streamlining repetitive tasks that depend on the review of large amounts of 
data. Depending on how they are designed, developed and deployed, AI systems can fast-track identity verification 
at border crossing points. They can also contribute to better identification of individuals posing potential threats 
to security. 
In this regard, good practices include machine-learning tools incorporating algorithmic data analysis to support 
refugee resettlement placement through chatbots providing information and advisory services to migrants in 
destination countries. There is also a growing focus on the use of AI to predict the likelihood of displacement 
events and populations at risk of displacement, ostensibly in order to support and avoid such events.
Such predictions could help authorities to prepare more efficiently for large influxes of people. This could contribute 
towards fulfilling their human rights obligations.107 For example, State authorities could act swiftly and better 
prepare their reception facilities, based on migration and movement forecasts. However, these predictions could 
also be used to reinforce non-entrée policies, understood as measures aimed at obviating access by migrants and 
asylum seekers to a State’s territory.108 These include unlawful non-refoulement practices, as forbidden by Article 33 
of the Refugee Convention and human rights treaties.109 Therefore, it is important to acknowledge that AI systems 
also present many risks for the protection and respect of migrants’ human rights in the context of migration and 
mobility. 
Three main implications for policy and practice can be drawn from this chapter:
• AI systems can amplify existing human biases, not just encode them. This can ultimately lead to discrimination 
and exclusion of people based on protected characteristics, including race and ethnicity. Bias is a common issue 
that permeates AI systems in a variety of sectors. Therefore, AI systems need to be developed in a way that 
deliberately and systematically seeks to remove or reduce bias throughout the process, from data collection 
and analysis to the reporting and assessment stages. Further, there is much greater awareness that overreliance 
on AI systems can result in incorrect and biased decisions, requiring policymakers and systems architects to 
ensure that regular monitoring and recalibration of systems, as well as human verification protocols, are in place. 
• The increasing datafication of migration and mobility can create and magnify vulnerabilities. Datafication refers 
to the different types of data, including biometric, satellite and big data, which are increasingly collected, stored 
and used for migration management. Poor data storage practices and cybersecurity flaws can expose migrants’ 
107 UN HRCttee, 2004.  
108 Hathaway, 2005; Gammeltoft-Hansen and Hathaway, 2014.
109 UN, 1951. See also ICCPR (UN, 1966: art. 7), as interpreted in UN HRCttee, 1992.
#
#20 Artificial intelligence, migration and mobility: Implications for policy and practice
sensitive information. This can have dramatic consequences for migrants if the information falls into the hands 
of malicious actors. Such concerns are further exacerbated by the interactions between the public and private 
sectors. Technology companies have been positioning themselves in the humanitarian and migration arena for 
many years, raising concerns about data protection. Therefore, data-sharing practices and access to sensitive 
data by private corporations should only occur when there are sufficient measures in place to safeguard the 
basic principles of data protection. 
• One of the key aspects currently underpinning analysis in this salient and strategic area of migration policy and 
practice is the extent to which a lack of transparency dominates. To some extent, this is likely to be fuelled by 
the risk of malicious acts of cybersecurity to undermine or control AI systems.110 However, this in itself creates 
different risks, especially as they relate to the erosion of human rights.
Accordingly, a human rights-based approach is needed to address these issues and rebalance the power structures 
at play. For instance, human rights impact assessment tools could be used before the deployment of AI systems. 
This would increase algorithmic fairness and accountability and prevent situations in which AI technologies are 
tested on vulnerable populations, such as migrants and refugees, without prior assessment of the risks. Adherence 
to the “do no harm” imperative during the design, development and deployment of AI systems could help to 
mitigate some of the risks brought about by these technologies throughout the migration cycle. 
110 Lohn, 2020.
#
#21WORLD MIGRATION REPORT 2022
Appendix A.  Definitions of AI
There is no universal definition of artificial intelligence (AI), which is a generic term with wide applicability to many 
contexts. Some useful definitions that assist in explaining the term include:
Source Definition 
The Canadian Information 
and Communications 
Technology Council111
A multidisciplinary subject, involving methodologies and techniques from various 
fundamental disciplines such as mathematics, engineering, natural science, 
computer science and linguistics, to name a few. Over the last few decades, 
AI has evolved into a number of technological areas such as planning, natural 
language processing, speech processing, machine learning, vision recognition, 
neural networks and robotics, among others.
International 
Telecommunication Union 
(ITU) AI for Good Global 
Summit 2017112
A set of associated technologies and techniques that can be used to complement 
traditional approaches, human intelligence and analytics and/or other techniques.
High-Level Expert Group 
on Artificial Intelligence 
set up by the European 
Commission113 
AI refers to systems that display intelligent behaviour by analysing their 
environment and taking actions – with some degree of autonomy – to achieve 
specific goals. 
AI-based systems can be purely software-based, acting in the virtual world 
(e.g. voice assistants, image analysis software, search engines, speech and face 
recognition systems) or AI can be embedded in hardware devices (e.g. advanced 
robots, autonomous cars, drones or Internet of Things applications).
World Intellectual Property 
Organization (WIPO)114
AI is generally considered to be a discipline of computer science that is aimed at 
developing machines and systems that can carry out tasks considered to require 
human intelligence. Machine learning and deep learning are two subsets of AI. 
In recent years, with the development of new neural network techniques and 
hardware, AI is usually perceived as a synonym for “deep supervised machine 
learning”.
Organisation for Economic 
Co-operation and 
Development (OECD)115
An AI system is a machine-based system that can, for a given set of humandefined
objectives, make predictions, recommendations, or decisions influencing 
real or virtual environments. AI systems are designed to operate with varying 
levels of autonomy.
111 McLaughlin and Quan, 2019.
112 ITU and XPrize, 2017.
113 European Commission, 2019.
114 WIPO, n.d.
115 OECD, 2019.
#
#22 Artificial intelligence, migration and mobility: Implications for policy and practice
United Kingdom 
Government Digital Service 
and Office for Artificial 
Intelligence116
At its core, AI is a research field spanning philosophy, logic, statistics, computer 
science, mathematics, neuroscience, linguistics, cognitive psychology and 
economics.
AI can be defined as the use of digital technology to create systems capable of 
performing tasks commonly thought to require intelligence.
AI is constantly evolving, but generally it:
• involves machines using statistics to find patterns in large amounts 
of data;
• is the ability to perform repetitive tasks with data without the need 
for constant human guidance.
Nils J. Nilsson117 AI is that activity devoted to making machines intelligent, and intelligence is that 
quality that enables an entity to function appropriately and with foresight in its 
environment.
Hila Mehr118 AI is the programming of computers to do tasks that would normally require 
human intelligence. This includes the ability to understand and monitor visual/
spatial and auditory information, reason and make predictions, interact with 
humans and machines, and continuously learn and improve.
John McCarthy119 It is the science and engineering of making intelligent machines, especially 
intelligent computer programmes. It is related to the similar task of using 
computers to understand human intelligence, but AI does not have to confine 
itself to methods that are biologically observable.
Dario Gil et al.120 AI is a field of computer science that studies how machines can be made to 
act intelligently. AI has many functions, including, but not limited to: learning, 
understanding, reasoning and interacting. 
Ronald Ashri121 AI refers to the effort to create machines that are able to tackle any problem 
by applying their skills. Just like humans, they can examine a situation and make 
best use of the resources at hand to achieve their objectives.
Jerry Kaplan122 The essence of AI is the ability to make appropriate generalizations in a timely 
fashion based on limited data. The broader the domain of application, the 
quicker conclusions are drawn with minimal information, the more intelligent 
the behaviour.
116 United Kingdom Government Digital Service and Office for Artificial Intelligence, 2019.
117 Nilsson, 2010.
118 Mehr, 2017.
119 McCarthy, 2007.
120 Gil et al., 2020.
121 Ashri, 2020.
122 Kaplan, 2016.
#
#23WORLD MIGRATION REPORT 2022
Appendix B.  AI usage in different sectors
Agriculture: AI is largely present in the farming and agriculture industry, especially with the increase in the use of 
intelligent tractors and plucking machines during harvest days. In addition, the agricultural sector relies on harvesting 
robots handling essential agricultural tasks such as planting seeds and monitoring crop and soil health. Flying and 
floating drones with AI capabilities are also being used to detect the quality of soil and water in order to improve 
the quality and quantity of crop yield.123
Business and finance: AI applications and usage have become essential for companies to save costs while improving 
outreach and quality of services. Computer algorithms and data-mining interfaces are allowing companies to improve 
the quality of their services by ensuring these better match customers’ expectations and needs. For instance, Netflix 
and Amazon recommendation lists provide a more personalized experience by capturing their engagement patterns 
through data mining. Human agents are also being replaced by intelligent software robots such as chatbots that 
can provide customers with instant answers to their queries,124 while reducing the cost of hiring human assistants. 
Education: AI applications in education include adaptive learning technology,125 which tailors content to students 
based on their abilities. AI is also used for plagiarism checking (e.g. Turnitin) and automated grading, as well as 
autocorrect and grammar checking (e.g. Grammarly).
Environment: AI has been integrated in ecological policy plans and has played a vital role in search and rescue 
missions in the responses to natural and human-made disasters. Examples include robots with AI capabilities that 
can sort recyclable material from waste, as well as using AI on satellite data to map and predict the progression 
of wildfires and find missing persons.126
Governance and security: Governments are using AI to improve security apparatuses. AI systems and autonomous 
flying machines such as drones are being used for surveillance to help automate the detection of, and response to, 
threats and patterns of criminal behaviour.127 
Science and health care: Investment in the field of AI in science and health care has witnessed a significant uptake, 
especially after the emergence of COVID-19.128 Using AI in science has proved to be indispensable, as it allows 
for cheaper experimentation, enables faster scientific discoveries and improves the effectiveness and efficiency of 
the health-care system. AI technologies are now able to monitor patients’ health, provide automated diagnostic 
support systems in hospitals and complement the work of physicians in the operation room. They are also being 
widely used in scientific research and experimentation, especially in magnetic resonance imaging (MRI) segmentation 
and statistics. 
Transportation: The transportation industry is one of the sectors benefiting most from the surge of AI, through 
research and investment in autonomous vehicles with virtual driver systems by car companies such as Tesla.129 The 
sector has also been leveraging AI algorithms to optimize public transport for scheduling, routing and traffic light 
management.130
123 Walch, 2019.
124 Nguyen, 2020
125 Haoyang Li, 2020.
126 Chui et al., 2018.
127 OECD, 2019.
128 Sivasubramanian, 2020. 
129 Niestadt et al., 2019.
130 Takyar, 2020.
#
#24 Artificial intelligence, migration and mobility: Implications for policy and practice
References*
Accenture
2018 Artificial Intelligence, Genuine Impact. Available at www.accenture.com/us-en/insights/public-service/
artificial-intelligence-genuine-impact.
Aggarwal, S.
2018 Dreading your visa application process? Here’s how AI is reducing the drudgery. Qrius, 14 
November. Available at https://qrius.com/dreading-your-visa-application-process-heres-how-aihas-made-it-easier/.
Al Hamar, J., J. Chamieh, H. Al-Mohannadi, M. Al Hamar, A. Al-Mutlaq and A.S. Musa
2018 Biometric of intent: A new approach identifying potential threat in highly secured facilities. 
Conference paper. Institute of Electrical and Electronics Engineers. Available at https://doi.
org/10.1109/W-FiCloud.2018.00037.
Angelov, P. and E. Soares
2020 Towards explainable deep neural networks (xDNN). Neural Network, 130:185–194.
Ashri, R.
2020 The AI-Powered Workplace: How Artificial Intelligence, Data, and Messaging Platforms Are Defining 
the Future of Work. Apress, New York City.
aus dem Moore, J.P., V. Chandran and J. Schubert
2018 The Future of Jobs in the Middle East. The World Government Summit and McKinsey and 
Company. Available at www.mckinsey.com/~/media/mckinsey/featured%20insights/middle%20
east%20and%20africa/are%20middle%20east%20workers%20ready%20for%20the%20impact%20
of%20automation/the-future-of-jobs-in-the-middle-east.ashx.
Australian National Audit Office (ANAO)
2008 DIAC’s Management of the Introduction of Biometric Technologies. Department of Immigration and 
Citizenship, Australia.
2012 Processing and Risk Assessing Incoming International Air Passengers. Australian Customs and 
Protection Service, Australia.
Bansak, K., J. Ferwerda, J. Hainmueller, A. Dillon, D. Hangartner, D. Lawrence and J. Weinstein
2018 Improving refugee integration through data-driven algorithmic assignment. Science, 359(6373):325–
329.
Baynes, C.
2019 Government “deported 7,000 foreign students after falsely accusing them of cheating in English 
language tests”. The Independent, 14 June.
Beduschi, A.
2019 The big data of international migration: Opportunities and challenges for states under international 
human rights law. Georgetown Journal of International Law, 49(4).
* All hyperlinks were working at the time of writing this report.
#
#25WORLD MIGRATION REPORT 2022
2020a International migration management in the age of artificial intelligence. Migration Studies. Available 
at https://doi.org/10.1093/migration/mnaa003. 
2020b Research Brief: Human Rights and the Governance of Artificial Intelligence. Academy of International 
Humanitarian Law and Human Rights, Geneva. Available at www.geneva-academy.ch/joomlatoolsfiles/docman-files/Human%20Rights%20and%20the%20Governance%20of%20Artificial%20
Intelligence.pdf.
Bhuyan, O.U.
2018 Smartcards issued to migrant workers remain unused. New Age, 10 August.
Bither, J. and A. Ziebarth
2020 AI, Digital Identities, Biometrics, Blockchain: A Primer on the Use of Technology in Migration Management. 
The German Marshall Fund of the United States. Available at www.gmfus.org/publications/aidigital-identities-biometrics-blockchain-primer-use-technology-migration-management.
Broeders, D. and H. Dijstelbloem 
2016 The Datafication of Mobility and Migration Management. The Mediating State and its 
Consequences, in Irma Van der Ploeg and Jason Pridmore, J. (eds) Digitizing Identity: Doing Identity 
in a Networked World, Routledge: Oxon.
Buolamwini, J. and T. Gebru
2018 Gender shades: Intersectional accuracy disparities in commercial gender classification. Proceedings 
of Machine Learning Research, 81:1–15.
Burrell, J.
2016 How the machine ‘thinks’: Understanding opacity in machine learning algorithms. Big Data and 
Society. Available at https://doi.org/10.1177/2053951715622512.
Calo, R.
2017 Artificial intelligence policy: A primer and roadmap. University of California, 51(399). Available at 
https://lawreview.law.ucdavis.edu/issues/51/2/Symposium/51-2_Calo.pdf.
Campbell, Z.
2019 Swarms of drones, piloted by artificial intelligence, may soon patrol Europe’s borders. The 
Intercept, 11 May. Available at https://theintercept.com/2019/05/11/drones-artificial-intelligenceeurope-roborder/.
Castles, S.
 2004 Why migration policies fail. Ethics and Racial Studies, 27(2):205–227. 
Chetail, V.
2008 Paradigm and paradox of the migration–development nexus: The new border for the North–
South dialogue. German Yearbook of International Law, 52:183–215.
Chui, M., M. Harrysson, J. Manyika, R. Roberts, R. Chung, A. van Heteren and P. Nel
2018 Applying Artificial Intelligence for Social Good. McKinsey Global Institute, New York, December. 
Available at www.mckinsey.com/~/media/McKinsey/Featured%20Insights/Artificial%20Intelligence/
Applying%20artificial%20intelligence%20for%20social%20good/MGI-Applying-AI-for-social-goodDiscussion-paper-Dec-2018.pdf.

#
#26 Artificial intelligence, migration and mobility: Implications for policy and practice
Council of Europe (CoE) 
1950  Convention for the Protection of Human Rights and Fundamental Freedoms. 4 November. ETS 
No. 005 (entry into force: 3 September 1953). 
Crawford, K., R. Dobbe, T. Dryer, G. Fried, B. Green, E. Kaziunas, A. Kak, V. Mathur, E. McElroy, A. Nill Sánchez, 
D. Raji, J.L. Rankin, R. Richardson, J. Schultz, S. Myers West and M. Whittaker
2019 AI Now 2019 Report. AI Now Institute, New York. Available at https://ainowinstitute.org/AI_
Now_2019_Report.pdf. 
Creemers, N., D. Guagnin and B.-J. Koops
2015 Profiling Technologies in Practice Applications and Impact on Fundamental Rights and Values. Wolf 
Legal Publishers, Nijmegen, The Netherlands.
Currier, C. 
2019 Lawyers and scholars to Lexisnexis, Thomson Reuters: Stop helping ICE deport people. The 
Intercept, 14 November. Available at https://theintercept.com/2019/11/14/ice-lexisnexis-thomsonreuters-database/.
Danish Institute for Human Rights (DIHR)
2020 Human rights impact assessment guidance and toolbox, DIHR: Copenhagen. Available at: www.
humanrights.dk/tools/human-rights-impact-assessment-guidance-toolbox.
Darch, C., Y. Majikijela, R. Adams and S. Rule
2020 AI, Biometrics and Securitisation in Migration Management: Policy Options for South Africa. Policy 
Action Network, South Africa. Available at https://policyaction.org.za/sites/default/files/PAN_
TopicalGuide_AIData5_Migration_Elec.pdf. 
Department of Immigration and Citizenship (DIAC) [Australia]
2008 Australia’s App Advance Passenger Processing System. National Communications Branch. Available 
at https://easyeta.com/pdf/APP-Guide.pdf.
Duan, Y., J.S. Edwards and Y.K. Dwivedi 
2019  Artificial intelligence for decision making in the era of Big Data – evolution, challenges and 
research agenda. International Journal of Information Management, 48:63–71. Available at https://
doi.org/10.1016/j.ijinfomgt.2019.01.021.
Duffield, M.
2016 ‘The resilience of the ruins: towards a critique of digital humanitarianism’, Resilience: International 
Policies, Practices and Discourses, 4(3):147–165.
Ernst, E., R. Merola and D. Samaan
2018 The Economics of Artificial Intelligence: Implications for the Future of Work. International Labour 
Organization, Geneva. Available at www.ilo.org/wcmsp5/groups/public/---dgreports/---cabinet/
documents/publication/wcms_647306.pdf.
Ertel, W. 
2017 Introduction to Artificial Intelligence. Springer International Publishing, Cham, Switzerland.
#
#27WORLD MIGRATION REPORT 2022
Eubanks, V.
2018 Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor. St. Martin’s Press, 
New York City.
European Commission
2017 Communication from the Commission to the European Parliament, the European Council and the Council. 
Seventh Progress Report towards an Effective and Genuine Security Union. 16 May. COM(2017) 261 
final. Available at https://eur-lex.europa.eu/legal-content/en/TXT/?uri=CELEX:52017DC0261.
2018 Communication from the Commission to the European Parliament, the European Council, the Council, 
the European Economic and Social Committee and the Committee of the Regions. Artificial Intelligence 
for Europe, Brussels. Available at www.forbes.com/sites/ericmack/2015/07/27/hawking-muskwozniak-freaked-about-artificial-intelligence-getting-a-trigger-finger/?sh=746de77f7416.
2019 A Definition of AI: Main Capabilities and Scientific Disciplines. High-Level Expert Group on Artificial 
Intelligence. Available at https://ec.europa.eu/digital-single-market/en/news/definition-artificialintelligence-main-capabilities-and-scientific-disciplines.
European Court of Human Rights (ECtHR)
2008 S. and Marper v. The United Kingdom. Judgment, Grand Chamber. Applications nos. 30562/04 and 
30566/04. 4 December. Available at http://hudoc.echr.coe.int/eng?i=001-90051. 
European Union (EU)
2016 Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 
on the Protection of Natural Persons with Regard to the Processing of Personal Data and 
on the Free Movement of Such Data, and repealing Directive 95/46/EC (General Data 
Protection Regulation). 4 May. OJ L 119. Available at https://eur-lex.europa.eu/legal-content/EN/
TXT/?uri=CELEX%3A02016R0679-20160504&qid=1532348683434. 
Fang, L. and S. Biddle
2020 Google AI tech will be used for virtual border wall, Cbp contract shows. The Intercept, October 
21.
Feldstein, S.
2019 The Global Expansion of AI Surveillance. Carnegie Endowment for International Peace, 17 
September. Available at https://carnegieendowment.org/2019/09/17/global-expansion-of-aisurveillance-pub-79847.
Ferguson, A.G.
2017 The Rise of Big Data Policing: Surveillance, Race, and the Future of Law Enforcement. New York 
University Press, New York City.
Flach, P.
2012 Machine Learning: The Art and Science of Algorithms that Make Sense of Data. Cambridge University 
Press, Cambridge and New York City.
Franzi, J.
n.d. Australian advanced passenger processing (APP). Presentation for the Australian Department of 
Immigration and Border Protection.
#
#28 Artificial intelligence, migration and mobility: Implications for policy and practice
Gammeltoft-Hansen, T. and J.C. Hathaway
2014 Non-refoulement in a world of cooperative deterrence. Law and Economics Working Papers, 
University of Michigan Law School. Available at https://repository.law.umich.edu/cgi/viewcontent.
cgi?article=1216&context=law_econ_current. 
Gil, D., S. Hobson, A. Mojsilović, R. Puri and J.R. Smith
2020 AI management: An overview. In: The Future of Management in an AI World: Redefining Purpose 
and Strategy in the Fourth Industrial Revolution (J. Canals and F. Heukamp, eds.). IESE Business 
Collection, pp. 3–19.
Gmelch, G.
1983 Who Returns and Why: Return Migration Behavior in Two North Atlantic Societies. Human 
Organization, 42(1):46–54.
Google Cloud
2020  Vision AI. Available at https://cloud.google.com/vision.
Graves, A. and K. Clancy 
2019 Unsupervised learning: The curious pupil. DeepMind, 25 June.
Haenssgen, M.J. and P. Ariana
2017 The place of technology in the Capability Approach. Oxford Development Studies, 44(1):98–112.
Hanke, P. 
2017 Artificial intelligence and big data – an uncharted territory for migration studies? National Center 
of Competence in Research.
Haoyang Li, D.
2020 How AI can realize the promise of adaptive education. Forbes, 26 March. Available at www.
forbes.com/sites/insights-ibmai/2020/03/26/how-ai-can-realize-the-promise-of-adaptiveeducation/#711dfa0912b3.
Hathaway, J.
2005 The Rights of Refugees under International Law. Cambridge University Press, Cambridge.
Heath, G.
2019 AI and intuitive decision-making in irregular people movement. Border Management, March. 
Available at http://bordermanagement.net/?p=1641. 
Hemmadi, M.
2017 Meet Botler, an A.I. chatbot that helps people immigrate to Canada. Canadian Business, 8 
February.
Hernandez, K. and T. Roberts
2020 Predictive analytics in humanitarian action: A preliminary mapping and analysis. Institute for 
Development Studies, United Kingdom. 
#
#29WORLD MIGRATION REPORT 2022
Hertog, S.
2019 The future of migrant work in the GCC: Literature review and a research and policy agenda. 
Fifth Abu Dhabi Dialogue Ministerial Consultation, 16–17 October. Available at http://eprints.lse.
ac.uk/102382/1/Hertog_future_of_migrant_work_in_GCC_published.pdf. 
Huszti-Orbán, K. and F. Ní Aoláin 
2020 Use of biometric data to identify terrorists: Best practice or risky business? Human Rights Center, 
University of Minnesota, Minneapolis. Available at www.ohchr.org/Documents/Issues/Terrorism/
biometricsreport.pdf.
International Organization for Migration (IOM)
2016 The Assessment of Border Crossing Points in the Volta and Western Regions and Training Capacity 
of Ghana Immigration Service. Available at https://publications.iom.int/books/assessment-bordercrossing-points-volta-and-western-regions-and-training-capacity-ghana.
2020 Technology to Support Analysis and Responses – IOM Covid-19 Analytical Snapshot #24. Migration 
Research Division. Available at www.iom.int/sites/default/files/documents/covid-19_analytical_
snapshot_24_-_technology_to_support_analysis_and_response.pdf. 
International Organization for Standardization (ISO)
2019  IT Security and Privacy — A framework for identity management — Part 1: Terminology and concepts. 
Available at www.iso.org/standard/77582.html.
International Telecommunication Union (ITU)
2019 Digital Inclusion of All. November. Available at www.itu.int/en/mediacentre/backgrounders/Pages/
digital-inclusion-of-all.aspx. 
International Telecommunication Union (ITU) and XPrize
2017  AI for Good Global Summit Report. Available at www.itu.int/en/ITU-T/AI/Documents/Report/AI_
for_Good_Global_Summit_Report_2017.pdf. 
Israel, T.
2020 Facial recognition is transforming our borders and we are not prepared. Policy Options, November. 
Available at https://policyoptions.irpp.org/magazines/november-2020/facial-recognition-istransforming-our-borders-and-we-are-not-prepared/.
Jordan, M.I. 
2019 Artificial Intelligence – the revolution hasn’t happened yet. Harvard Data Science Review, 2 July. 
Available at https://hdsr.mitpress.mit.edu/pub/wot7mkc1/release/8.
Jupe, L.M. and D.A. Keatley
2019 Airport artificial intelligence can detect deception: or am I lying? Security Journal, 33:622–635.
Kaplan, J.
2016 Artificial Intelligence: What Everyone Needs to Know. Oxford University Press, Oxford and New 
York City.
#
#30 Artificial intelligence, migration and mobility: Implications for policy and practice
Kinstler, L.
2019 ‘Big tech firms are racing to track climate refugees’, MIT Technology Review, 17 May. Available 
at www.technologyreview.com/2019/05/17/103059/big-tech-firms-are-racing-to-track-climaterefugees/.
Koslowski, R.
2005 Smart borders, virtual borders or no borders: Homeland security choices for the United States 
and Canada. Law and Business Review of the Americas, 11(3). Available at https://core.ac.uk/
download/pdf/147641794.pdf.
Kuner, C. and M. Marelli (eds)
2020 Handbook on Data Protection in Humanitarian Action, ICRC: Geneva.  Available at https://shop.icrc.
org/download/ebook?sku=4305.01/002-ebook.
Latonero, M.
2019 ‘Stop Surveillance Humanitarianism’, 11 July, The New York Times: New York.  
Latonero, M. and P. Kift
2018 On digital passages and borders: Refugees and the new infrastructure for movement and control. 
Social Media + Society, March. Available at https://doi.org/10.1177/2056305118764432.
LeCun, Y., Y. Bengio and G. Hinton
2015 Deep learning. Nature, 521(7553):436–44.
Lohn, A. 
2020 Hacking AI: A Primer for Policymakers on Machine Learning Cybersecurity. Center for Security 
and Emerging Technology, Georgetown University, December. Available at https://doi.
org/10.51593/2020CA006. 
Ma, W., O.O. Adesope, J.C. Nesbit and Q. Liu
2014 Intelligent tutoring systems and learning outcomes: A meta-analysis. Journal of Educational 
Psychology, 106(4):901–918.
Mack, E.
2015 Hawking, Musk, Wozniak warn about artificial intelligence’s trigger finger. Forbes, 27 July. Available 
at www.forbes.com/sites/ericmack/2015/07/27/hawking-musk-wozniak-freaked-about-artificialintelligence-getting-a-trigger-finger/.
Madianou, M.
2019 ‘Technocolonialism: Digital Innovation and Data Practices in the Humanitarian Response to Refugee 
Crises’, Social Media + Society, 5(3):1–13. Available at https://doi.org/10.1177/2056305119863146.
Majidi, N., C. Kasavan and G.H. Harindranath
2021 In support of return and reintegration? A roadmap for a responsible use of technology, in 
McAuliffe, M. (ed) Research Handbook of International Migration and Digital Technology, Edward 
Elgar: Oxford.
Martin-Ortega, O. and C. Methven O’Brien (eds)
2019 Public Procurement and Human Rights: Opportunities, Risks and Dilemmas for the State as Buyer, 
Edward Elgar: Oxford.
#
#31WORLD MIGRATION REPORT 2022
Matyus, A.
2020 ICE weaponizes state-issued licenses against Maryland’s undocumented immigrants. Digitaltrends, 
27 February. Available at www.digitaltrends.com/news/ice-weaponizes-state-licences-againstundocumented-immigrants/.
McAuliffe, M.
2016 How transnational connectivity is shaping irregular migration: Insights for migration policy and 
practice from the 2015 irregular migration flows to Europe. Migration Policy Practice, 6(1):4–10.
McAuliffe, M. and A.M. Goossens
2018 Regulating international migration in an era of increasing interconnectedness. In Handbook of 
Migration and Globalisation (A. Triandafyllidou, ed.). Edward Elgar Publishing, Northampton, 
Massachusetts.
McAuliffe, M. and J. Blower
2021 Migration, mobility and digital technology in a post-COVID-19 world: initial reflections on 
transformations underway. In McAuliffe, M. (ed.) Research Handbook on International Migration 
and Digital Technology. Edward Elgar, Cheltenham.
McAuliffe, M., J. Blower and A. Beduschi
2021  Digitalization and artificial intelligence in migration and mobility: Transnational implications of the 
COVID-19 pandemic. Societies 2021, 11. Available at www.mdpi.com/2075-4698/11/4.
McAuliffe, M. and K. Koser
2017  A Long Way to Go: Irregular Migration Patterns, Processes, Drivers and Decision-making. ANU Press, 
Canberra.
McCarroll, E.
2020 Weapons of Mass Deportation: Big Data and Automated Decision-making Systems in Immigration 
Law. Georgetown University Law Center. Available at www.law.georgetown.edu/immigrationlaw-journal/wp-content/uploads/sites/19/2020/08/Weapons-of-Mass-Deportation-Big-Data-and-
Automated-Decision-Making-Systems-in-Immigration-Law.pdf.
McCarthy, J.
2007 What is Artificial Intelligence? Stanford University, Stanford. Available at http://jmc.stanford.edu/
articles/whatisai/whatisai.pdf.
McGregor, L., D. Murray and V. Ng 
2019 ‘International Human Rights Law as a Framework for Algorithmic Accountability’ International & 
Comparative Law Quarterly, 68:309–343.
McLaughlin, R. and T. Quan
2019  On the Edge of Tomorrow – Canada’s AI Augmented Workforce. The Information and Communications 
Technology Council. Available at www.ictc-ctic.ca/wp-content/uploads/2020/02/canadas-aiworkforce-FINAL-ENG-2.24.20.pdf.
Mehr, H.
2017 Artificial Intelligence for Citizen Services and Government. ASH Center for Democratic Governance 
and Innovation. Available at https://ash.harvard.edu/files/ash/files/artificial_intelligence_for_citizen_
services.pdf.
#
#32 Artificial intelligence, migration and mobility: Implications for policy and practice
Miessner, S.
2019 Meet Kamu: Co-designing a chatbot for immigrants. The Service Gazette, August 15. Available at 
https://medium.com/the-service-gazette/co-designing-a-chatbot-for-immigrants-81bc3b3b7937.
Molnar, P.
2018 Using AI in immigration decisions could jeopardize human rights. Centre for International 
Governance Innovation. Available at www.cigionline.org/articles/using-ai-immigration-decisionscould-jeopardize-human-rights.
2019 New technologies in migration: Human rights impacts. Forced Migration Review, 61:7–9.
2020 Technological Testing Grounds: Migration Management Experiments from the Ground Up. EDRi and 
the Refugee Law Lab. Available at https://edri.org/wp-content/uploads/2020/11/TechnologicalTesting-Grounds.pdf.
Molnar, P. and L. Gill
2018 Bots at the Gate: A Human Rights Analysis of Automated Decision-Making in Canada’s Immigration 
and Refugee System. Citizen Lab and International Human Rights Program, University of Toronto. 
Available at https://tspace.library.utoronto.ca/handle/1807/94802.
Morozov, E.
2014 To Save Everything, Click Here: The Folly of Technological Solutionism. Public Affairs, New York City.
Nguyen, M.-H.
2020 How artificial intelligence and machine learning produced robots we can talk to. Business Insider, 
27 January. Available at www.businessinsider.com/chatbots-talking-ai-robot-chat-machine. 
Niestadt, M., A. Debyser, D. Scordamaglia and M. Pape
2019 Artificial Intelligence in Transport Current and Future Developments, Opportunities and Challenges. 
European Parliamentary Research Service, March. Available at www.europarl.europa.eu/RegData/
etudes/BRIE/2019/635609/EPRS_BRI(2019)635609_EN.pdf. 
Nilsson, N.J.
1982 Principles of Artificial Intelligence. Springer-Verlag, Berlin and Heidelberg.
2010 The Quest for Artificial Intelligence: A History of Ideas and Achievements. Cambridge University 
Press, Cambridge.
Noble, S.U.
2018 Algorithms of Oppression: How Search Engines Reinforce Racism. New York University Press, New 
York City.
Norris, P. 
2001 Digital Divide: Civic Engagement, Information Poverty and the Internet Worldwide. Cambridge 
University Press, Cambridge. Available at https://academic.oup.com/migration/advance-article/
doi/10.1093/migration/mnaa003/5732839?login=true.
#
#33WORLD MIGRATION REPORT 2022
Organisation for Economic Co-operation and Development (OECD)
2019 Artificial Intelligence in Society. Paris. Available at www.oecd.org/publications/artificial-intelligencein-society-eedfee77-en.htm.
Organization of American States (OAS) 
1969  American Convention on Human Rights (Pact of San Jose). 22 November. OAS Treaty Series 
No. 36 (entry into force: 18 July 1978).
Parker, B.
2019 ‘New UN deal with data mining firm Palantir raises protection concerns’, The New Humanitarian, 
5 February 2019.
2020a ‘The cyber attack the UN tried to keep under wraps’, The New Humanitarian, 29 January 2020.
2020b ‘Dozens of NGOs hit by hack on US fundraising database’, The New Humanitarian, 4 August 
2020.
Pasquale, F.
2015 The Black Box Society: The Secret Algorithms that Control Money and Information. Harvard University 
Press, Cambridge, Massachusetts.
Peters, A.
2019 There’s now a chatbot to give refugees instant legal aid. Future of Philanthropy, 12 July. 
Pizzi, M., M. Romanoff and T. Engelhardt
2020 AI for humanitarian action: Human rights and ethics. International Review of the Red Cross, 
102(913):145–180. Available at https://international-review.icrc.org/sites/default/files/reviewspdf/2021-03/ai-humanitarian-action-human-rights-ethics-913.pdf.
Pricewaterhouse Coopers (PwC) 
2011 Policy study on an EU electronic system for travel authorization (EU ESTA). The Australian 
mission, Australia. Available at https://ec.europa.eu/home-affairs/sites/homeaffairs/files/e-library/
docs/pdf/esta_annexes_en.pdf. 
Rashid, S.R. and A.A. Ashraf
2018 The Mapping and Scoping of Services for the Migrant Workers of Bangladesh at Various Stages of 
Labour Migration Cycle. International Organization for Migration, Bangladesh. 
Rawlings, L. 
2019  Identifying hostile intent: Behavioural analysis. Border Management Magazine, 26 April. Available 
at www.bordermanagement.net/?p=1660.
Read, R., B. Taithe and R. MacGinty
2016 ‘Data hubris? Humanitarian information systems and the mirage of technology’, Third World 
Quarterly,  37(8), pp. 1314-1331. https://doi.org/10.1080/01436597.2015.1136208.
Rivlin-Nadler, M.
2019 How ICE uses social media to surveil and arrest immigrants. The Intercept, 22 December.
#
#34 Artificial intelligence, migration and mobility: Implications for policy and practice
Rizvi, A.
2004 Designing and delivering visas. People and Place, 12(2).
Rudin, C.
2019 Stop explaining black box machine learning models for high stakes decisions and use interpretable 
models instead. Nature Machine Intelligence, 1:206–215.
Sandvik, K.B., K. Lindskov Jacobsen and S.M. McDonald
2017 ‘Do no harm: A taxonomy of the challenges of humanitarian experimentation’, International 
Review of the Red Cross, No. 904. Available at https://international-review.icrc.org/articles/do-noharm-taxonomy-challenges-humanitarian-experimentation.
Sengupta, Y.
2019 The role of chatbots in mental healthcare. DZone, 26 March. Available at https://dzone.com/
articles/the-role-of-chatbots-in-mental-healthcare.
Shelfer, K.M. and J.M. Verner
2003 Using competitive intelligence to develop an automated visa approval system. In: Towards 
the Knowledge Society ( J.L. Monteiro, P.M.C. Swatman and L.V. Tavares, eds.). Vol. 105. The 
International Federation for Information Processing. Springer, Boston.
Sivasubramanian, S.
2020 How AI and machine learning are helping to fight COVID-19. World Economic Forum, 28 
May. Available at www.weforum.org/agenda/2020/05/how-ai-and-machine-learning-are-helpingto-fight-covid-19/.
Snow, J.
2018 Amazon’s face recognition falsely matched 28 members of Congress with mugshots. American 
Civil Liberties Union, 26 July. Available at www.aclu.org/blog/privacy-technology/surveillancetechnologies/amazons-face-recognition-falsely-matched-28.
Solon, O.
2016 Karim the AI delivers psychological support to Syrian refugees. The Guardian, 22 March.
Suresh, H. and J.V. Guttag
2020 A framework for understanding unintended consequences of machine learning. arXiv, 28 January. 
Available at https://arxiv.org/pdf/1901.10002v3.pdf.
Takyar, A.
2020 AI applications across major industries. Leeway Hertz, 12 August. Available at www.leewayhertz.
com/ai-applications-across-major-industries/. 
Taylor, L. and F. Meissner
2019 ‘A Crisis of Opportunity: Market-Making, Big Data, and the Consolidation of Migration as Risk’, 
Antipode. Available at https://doi.org/10.1111/anti.12583.
TechAmerica Foundation 
2012 Demystifying Big Data: A Practical Guide to Transforming the Business of Government. Federal Big 
Data Commission. Available at https://breakinggov.com/documents/demystifying-big-data-a-
practical-guide-to-transforming-the-bus/.
#
#35WORLD MIGRATION REPORT 2022
Tegmark, M.
2016 Benefits and Risks of Artificial Intelligence. Future of Life Institute. Available at https://futureoflife.
org/background/benefits-risks-of-artificial-intelligence/.
Thales Group
n.d. Automated border control – Slash queues with faster eGates. Available at www.thalesgroup.
com/en/markets/digital-identity-and-security/government/eborder/eborder-abc. 
Trapp, A., A. Teytelboym, A. Martinello, T. Andersson and N. Ahani 
2018 Placement optimization in refugee resettlement. Working paper 23, Lund University.  
Available at https://project.nek.lu.se/publications/workpap/papers/wp18_23.pdf. 
United Kingdom Government Digital Service and Office for Artificial Intelligence
2019 A Guide to Using Artificial Intelligence in the Public Sector. Available at www.gov.uk/government/
publications/understanding-artificial-intelligence/a-guide-to-using-artificial-intelligence-in-thepublic-sector.
United Nations (UN)
1951 Convention Relating to the Status of Refugees. United Nations Treaty Series, 189:137, 28 July 
(entry into force: 22 April 1954). 
1966  International Covenant on Civil and Political Rights. United Nations Treaty Series, 999:171, 16 
December (entry into force: 23 March 1976). 
United Nations General Assembly (UNGA)
1948  Universal Declaration of Human Rights. 10 December. Resolution 217 A (III). Available at www.
un.org/en/ga/search/view_doc.asp?symbol=A/RES/217(III). 
2011 Report of the Special Rapporteur on Contemporary Forms of Racism, Racial Discrimination, Xenophobia 
and Related Intolerance, Githu Muigai. 24 May. A/HRC/17/40. Available at https://undocs.org/A/
HRC/17/40.
2014 The Right to Privacy in the Digital Age. 21 January. A/RES/68/167. Available at https://undocs.org/A/
RES/68/167. 
2019 Report of the Special Rapporteur on Extreme Poverty and Human Rights. 11 October. A/74/493. 
Available at https://undocs.org/A/74/493. 
United Nations Human Rights Committee (HRCttee)
1988 General Comment No. 16: Article 17. The Right to Respect of Privacy, Family, Home and 
Correspondence, and Protection of Honour and Reputation. 8 April. In: HRI/GEN/1/Rev.1, 1994, 
p. 21. Available at https://tbinternet.ohchr.org/_layouts/15/treatybodyexternal/Download.
aspx?symbolno=INT%2fCCPR%2fGEC%2f6624&Lang=en. 
1992 General Comment No. 20: Article 7 (Prohibition of Torture, or Other Cruel, Inhuman or Degrading Treatment 
or Punishment). HRI/GEN/1/Rev.1, 1994, p. 30. Available at https://tbinternet.ohchr.org/_layouts/15/
treatybodyexternal/Download.aspx?symbolno=INT%2fCCPR%2fGEC%2f6621&Lang=en. 
#
#36 Artificial intelligence, migration and mobility: Implications for policy and practice
2004 General Comment No. 31: The Nature of the General Legal Obligation Imposed on States Parties to the 
Covenant. 26 May. CCPR/C/21/Rev.1/Add.13. Available at https://tbinternet.ohchr.org/_layouts/15/
treatybodyexternal/Download.aspx?symbolno=CCPR%2fC%2f21%2fRev.1%2fAdd.13&Lang=en. 
2007  General Comment No. 32. Article 14: Right to Equality Before Courts and Tribunals and to a Fair Trial. 23 
August. CCPR/C/GC/32. Available at https://tbinternet.ohchr.org/_layouts/15/treatybodyexternal/
Download.aspx?symbolno=CCPR%2FC%2FGC%2F32&Lang=en. 
United Nations Human Rights Council (UNHRC)
2008 Protect, Respect and Remedy: A Framework for Business and Human Rights, Report of the Special 
Representative of the Secretary-General on the Issue of Human Rights and Transnational Corporations 
and Other Business Enterprises, John Ruggie. 7 April. A/HRC/8/5. Available at www2.ohchr.org/
english/bodies/hrcouncil/docs/8session/A-HRC-8-5.doc.  
2011 Report of the Special Representative of the Secretary-General on the Issue of Human Rights and 
Transnational Corporations and Other Business Enterprises, John Ruggie. Guiding Principles on Business 
and Human Rights: Implementing the United Nations “Protect, Respect and Remedy” Framework. 21 
March. A/HRC/17/31. Available at https://undocs.org/en/A/HRC/17/31. 
2020a Racial Discrimination and Emerging Digital Technologies: A Human Rights Analysis. Report of the 
Special Rapporteur on Contemporary Forms of Racism, Racial Discrimination, Xenophobia and Related 
Intolerance. 18 June. A/HRC/44/57. Available at https://undocs.org/en/A/HRC/44/57. 
2020b Impact of the Use of Private Military and Security Services in Immigration and Border Management on 
the Protection of the Rights of All Migrants. Report of the Working Group on the Use of Mercenaries 
as a Means of Violating Human Rights and Impeding the Exercise of the Right of Peoples to SelfDetermination.
9 July. A/HRC/45/9. Available at https://undocs.org/A/HRC/45/9. 
United Nations Secretary-General (UN SG)
2020 Report of the Secretary-General Roadmap for Digital Cooperation. New York, June. Available at 
www.un.org/en/content/digital-cooperation-roadmap/.
United States Department of Homeland Security 
2005 A Review of U.S. Citizenship and Immigration Services’ Alien Security Checks. Office of Inspector 
General. Available at www.hsdl.org/?view&did=464051. 
Vilone, G. and L. Longo
2020 Explainable artificial intelligence: A systematic review. arXiv, 29 May. Available at https://arxiv.org/
abs/2006.00093.
Walch, K.
2019 How AI is transforming agriculture. Forbes, 5 July. Available at www.forbes.com/sites/
cognitiveworld/2019/07/05/how-ai-is-transforming-agriculture/#3c79edb94ad1.  
Walsh, T., K. Miller, J. Goldenfein, F. Chen, J. Zhou, R. Nock, B. Rubinstein and M. Jackson
2019 Closer to the Machine: Technical, Social, and Legal Aspects of AI. Office of the Victorian Information 
Commissioner. Available at https://ovic.vic.gov.au/wp-content/uploads/2019/08/closer-to-the-
machine-web.pdf.
#
#37WORLD MIGRATION REPORT 2022
Watson, D.S. and L. Floridi
2020 The explanation game: A formal framework for interpretable machine learning. Synthese. Available 
at https://link.springer.com/article/10.1007/s11229-020-02629-9.
Wickens, C.D., B.A. Clegg, A.Z. Vieane and A.L. Sebok 
2015 Complacency and automation bias in the use of imperfect automation. Human Factors, 57(5):728–
739. Available at www.researchgate.net/publication/275101423_Complacency_and_Automation_
Bias_in_the_Use_of_Imperfect_Automation/link/555b890608aec5ac2232401e/download.
Wolchover, N.
2015 Concerns of an artificial intelligence pioneer. Quantamagazine, 21 April. Available at 
www.quantamagazine.org/artificial-intelligence-aligned-with-human-values-qa-with-stuartrussell-20150421.
Wong, R.W.-M. and A.H.W. Chun
2006 eBrain – Using AI for automatic assessment at the Hong Kong immigration department. American 
Association for Artificial Intelligence. Available at www.cs.cityu.edu.hk/~hwchun/research/PDF/
eBrain_IAAI.pdf. 
World Customs Organization (WCO)
2019 Study Report on Disruptive Technologies. WCO Permanent Technical Committee.
World Customs Organization (WCO), International Air Transport Association (IATA) and International Civil 
Aviation Organization (ICAO)
2010 Guidelines on Advance Passenger Information. Available at www.icao.int/Security/FAL/
Documents/2010%20API%20Guidelines%20Final%20Version.ICAO.2011%20full%20x2.pdf.
World Health Organization (WHO)
2020 Estonia and WHO to jointly develop digital vaccine certificate to strengthen COVAX. 7 October. 
Available at www.who.int/news-room/feature-stories/detail/estonia-and-who-to-jointly-developdigital-vaccine-certificate-to-strengthen-covax.
World Intellectual Property Organization (WIPO)
n.d. Artificial Intelligence and Intellectual Property. Available at www.wipo.int/about-ip/en/artificial_
intelligence/. 
Zou, J. and Schiebinger, L.
2018 AI can be sexist and racist – it’s time to make it fair. Nature, 559(7714):324–326. Available at 
www.nature.com/articles/d41586-018-05707-8.
Zuboff, S.
2019 The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of Power. Public 
Affairs, New York City.
RESEARCH ARTICLE
Using Twitter to track immigration sentiment during early
stages of the COVID-19 pandemic
Francisco Rowe1,* , Michael Mahony1, Eduardo Graells-Garrido2, Marzia Rango3 and
Niklas Sievers3
1Geographic Data Science Lab, Department of Geography and Planning, School of Environmental Sciences, University of
Liverpool, Liverpool, United Kingdom
2Data Science Institute, Universidad del Desarrollo, Santiago, Chile
3Global Migration Data Analysis Centre, International Organization for Migration, Berlin, Germany
*Corresponding author. E-mail: F.Rowe-Gonzalez@liverpool.ac.uk
Received: 13 March 2021; Revised: 20 November 2021; Accepted: 05 December 2021
Key words: immigration sentiment; migration; pandemic; sentiment analysis; topic modeling; Twitter
Abbreviations: IOM, International Organization for Migration; LDA, latent Dirichlet allocation; ODI, Overseas Development
Institute; UN, United Nations; VADER, Valence Aware Dictionary and sEntiment Reasoner
Abstract
Large-scale coordinated efforts have been dedicated to understanding the global health and economic implications of the
COVID-19 pandemic. Yet, the rapid spread of discrimination and xenophobia against specific populations has largely
been neglected. Understanding public attitudes toward migration is essential to counter discrimination against immigrants
and promote social cohesion. Traditional data sources to monitor public opinion are often limited, notably due to
slow collection and release activities. New forms of data, particularly from social media, can help overcome these
limitations.While somebias exists, socialmediadata are produced at an unprecedented temporal frequency, geographical
granularity, are collected globally and accessible in real-time. Drawing on a data set of 30.39 million tweets and natural
language processing, this article aims tomeasure shifts in public sentiment opinion aboutmigration during early stages of
the COVID-19 pandemic inGermany, Italy, Spain, theUnitedKingdom, and the United States. Results show an increase
of migration-related Tweets along with COVID-19 cases during national lockdowns in all five countries. Yet, we found
no evidence of a significant increase in anti-immigration sentiment, as rises in the volume of negativemessages are offset
by comparable increases in positive messages. Additionally, we presented evidence of growing social polarization
concerning migration, showing high concentrations of strongly positive and strongly negative sentiments.
Policy Significance Statement
This article demonstrates that Twitter data can be a useful resource to monitor immigration-related sentiment in
real or near real-time to complement traditional data sources. Although Twitter users are generally not
representative of the population at large, Twitter data can contribute to developing effective strategies to counter
the spread of discrimination against migrants in emergency scenarios and more broadly, in line with Objective
17 of the Global Compact for Safe, Orderly and Regular Migration, which is to “Eliminate all forms of
©TheAuthor(s), 2021. Published byCambridgeUniversity Press. This is anOpenAccess article, distributed under the terms of the Creative Commons
Attribution licence (http://creativecommons.org/licenses/by/4.0), which permits unrestricted re-use, distribution and reproduction, provided the
original article is properly cited.
This research article was awarded Open Data and Open Materials badges for transparent practices. See the Data
Availability Statement for details.
*The online version of this article has been updated since original publication. A notice detailing the change has also been published.
Data & Policy (2021), 3: e36
doi:10.1017/dap.2021.38
#
#discrimination and promote evidence-based public discourse to shape perceptions ofmigration.”As social media
is likely to reinforce and geographically extend the trend of social polarization on topics such as migration
through “echo chambers,” it is critical to investigate how misinformation and negative sentiments toward
migrants spread across virtual networks to design programmes aimed at countering discrimination and fostering
social cohesion in communities worldwide.
1. Introduction
Currently, the world faces an unprecedented challenge to tackle and understand the spread and impacts of
COVID-19.While large-scale coordinated efforts have been dedicated to understand the global health and
economic implications of the pandemic, the rapid spread of prejudice and xenophobia has largely been
neglected. Incidents of rising anti-immigration sentiment have been reported since the start of the
pandemic (Nature, 2020). Wikipedia (2020) has a dedicated site documenting key racist and xenophobic
incidents around the world. Acts and displays of intolerance, discrimination, racism, xenophobia and
violent extremism have emerged linking individuals of Asian descendent and appearance to COVID-19
(Coates, 2020; Nature, 2020). Fear mongering and racial stereotyping spread on social media and rapidly
spilled onto the streets (Cowper, 2020).
Understanding damaging xenophobic narratives is key to prevent the spread of misinformation fuelling
misperception, negative attitudes, and discrimination against immigrants. Xenophobic narratives and racist
attacks erode social cohesion and are likely to have repercussions which will likely persist beyond the
pandemic. Anti-immigration sentiment is often rooted in misperceptions (European Commission, 2019),
and experimental evidence has indicated that providing information to address these misconceptions can
shift attitudes toward a more supportive view of immigration (Grigorieff et al., 2020). The availability and
accuracy of data on public opinion of migration are thus critical for tackling misperceptions and understanding
the extent of local openness to immigration and ethnic diversity (Dennison and Dražanová, 2018).
Traditionally, data on attitudes toward immigrants are collected through qualitative sources, namely
ethnographies and interviews, or surveys (Rowe et al., 2021a). Yet, qualitativemethods rely on small samples
and normally suffer fromsample bias (Ochieng, 2009). Similarly,while surveys can provide a reliable national
representation, they are expensive, infrequent, lack statistical validity at fine geographical scales, and may
suffer from data latency with a considerable gap between the date of data collection and release (Ochieng,
2009;Green et al., 2021).Moreover, survey data do not typically provide insights intowhypeople hold certain
views onmigration, and respondentsmay interpret the same survey question in differentways (Goyder, 1986).
New forms of data can help overcome these deficiencies. Social media, particularly microblogging,
offers an open and dynamic space which provides a unique window to better understand public opinion
about immigration.Microblogging is a new form of communication inwhich users can publish short posts
to express live opinions on digital devices. In October 2020, 53% of the world’s population (over 4.1
billion people) were estimated to be active social media users (Hoosuite andWeAre Social, 2020). Social
media data are produced at an unprecedented temporal frequency, geographical granularity and are
accessible in real time (McCormick et al., 2017). Coupled with cost-efficient computing and machine
learning algorithms, these data enable real-time processing of information to measure and monitor antiimmigration
sentiment at frequent temporal intervals over extended timeframes and across the globe
(Bartlett and Norrie, 2015; Freire-Vidal and Graells-Garrido, 2019).
This article aims tomeasure andmonitor changes in attitudes toward immigrants during early stages of
the current COVID-19 outbreak in five countries: Germany, Italy, Spain, the United Kingdom, and the
United States using Twitter data and natural language processing. Specifically, we seek to:
• determine the extent of intensification in anti-immigration sentiment as the geographical spread and
fatality rate of COVID-19 increases;
• identify key discrimination topics associated with anti-immigration sentiment;
• assess how these topics and immigration sentiment change over time and vary by country.
e36-2 Francisco Rowe et al.
#
#Drawing on a data set of 30.39million tweets, we examine immigration sentiment fromDecember 1, 2019
(pre-pandemic) to April 30, 2020 comprising the early months of the COVID-19 pandemic. We
acknowledge the potential limitations of Twitter to capture immigration sentiment. Twitter may only
capture public opinions made by a selected segment of the population whose size and attributes vary by
country according to access to digital technology, offering a partial portrayal of immigration sentiment
(Rowe, 2021). Yet, Twitter constitutes a novel source of data which offers information on public opinion
that did not exist 14 years ago. They have global coverage and offer the potential to enable real- or near
real-time monitoring of changing attitudes toward immigrants during dynamic and fast-evolving events,
such as pandemics. Thus, Twitter data can be used to complement traditional sources and develop our
understanding of public perceptions toward immigration where appropriate data are not available.
Our selection of countries was based on the cooccurrence of having an extensive Twitter user base,
immigrant population, and high incidence of confirmed COVID-19 cases and fatality for countries
outside Asia. The key rationale was: countries with large immigrant populations that were first and more
severely affected by COVID-19 were more likely to experience an escalating number of incidents of
racism and xenophobia early in the pandemic (Nature, 2020; United Nations, 2020). Further, a key
objective was to identify migration sentiment toward Asian populations so that we selected countries
outside Asia. We thus identified countries with some of the largest Twitter user penetration rates globally,
in order to maximize the representation of a diverse group of individuals and minimize potential selection
biases (Sehl, 2020; Statista, 2020). We also selected countries which first reported COVID-19 cases
outside Asia and recorded the world’s largest number of cumulative COVID-19 deaths at the start of the
pandemic.
The rest of the article is structured into five main sections. The next section discusses emerging
evidence relating to racism during the COVID-19 pandemic and how feelings of anxiety and fear may
have influenced perceptions toward immigrants. This section also discusses prior empirical work on
attitudes toward immigration based on traditional sources and identify their limitations before discussing
the key challenges and opportunities of using Twitter data to measure immigration sentiment. We then
describe our strategy of data collection and analysis before presenting and discussing the results. We
conclude by identifying implications for future research and immigration policy.
2. Background
2.1. Emerging evidence on anti-immigration sentiment related to COVID-19
AsCOVID-19 expanded throughout the world in early February 2020, mounting fear andmisinformation
led to rapid spread of xenophobic and racist rhetoric against individuals of Asian descendent and
appearance (OECD, 2020). This rhetoric rapidly expanded through news media coverage and social
media platforms across the world via the publication of numerous articles explicitly blaming or
associating China with the COVID-19 outbreak (Coates, 2020; Cowper, 2020). Narratives quickly
translated into actions, with a rising number of violent physical aggression incidents (Bhattacharya
et al., 2020; Wikipedia, 2020). In the United Kingdom, the government reported a 21% increase in hate
crime incidents against Asian communities between January andMarch, and Chinese businesses reported
a notorious reduction in footfall during Chinese celebrations (Home Affairs Committee, 2020). The
editorial team of Nature published an apology for erroneously associating COVID-19 with China, and
noted the influential role of key political figures in invigorating that narrative, particularly in the United
States and Brazil: where President Donald Trump repeatedly used the terms “Chinese Virus,” “China
Virus,” and “Fung Flu” in reference to COVID-19, and Eduardo Bolsonaro, a son of the Brazilian
President, called it “China’s fault” (Nature, 2020).
Prejudice, discrimination and stigmatization during pandemics are not new. They have emerged
throughout history as fear and panic mount. The 1300s saw Jewish persecution during the Black Death
(Link and Phelan, 2006). The 1980s witnessed the stigmatization of lesbian, gay, bisexual, transgender,
queer, and questioning communities during the HIV outbreak (Berger et al., 2001). More recently, the
Data & Policy e36-3
#
#Ebola outbreak was labeled as an “African disease” (Davtyan et al., 2014). Stigmatization may influence
individual livelihoods and impact broader socioeconomic outcomes. At the individual level, stigma has
been linked to mental stress, depression and low self-esteem—all of which can reduce quality of life,
generate unemployment and income loss (Link and Phelan, 2006). At the societal level, stigma can impact
social inclusion, businesses and the economy (Nature, 2020). In the context of COVID-19, the international
education market and universities are predicted to endure large economic losses as international
Asian students, who have been primary subjects of verbal and physical abuse, may decide not to resume
their studies in fear of racism (Nature, 2020).
The rise in stigmatization has encountered a wave of positive immigration sentiment events recognizing
the key role of immigrant workers in frontline occupations during COVID-19, such as social care,
health, essential retail and food production (Fasani and Mazza, 2020; Gelatt, 2020). An event widely
shared by the international media press was Prime Minister Boris Johnson’s praise to two immigrant
nurses for helping save his life during his time in intensive care due to COVID-19 (BBC, 2020).
Communication campaigns have also been put in place to tackle emerging COVID-19 anti-immigration
narratives. The United Nations, for instance, issued a set of recommendations to address and counter hate
speech related to COVID-19. In Germany, an antidiscrimination agency launched a dedicated campaign
to raise awareness about the rise of anti-Semitic discrimination and racism (OECD, 2020). In Finland, the
government started a nation-wide campaign to tackle misconceptions about COVID-19 on social media
(OECD, 2020). In France, actions were put in place to accelerate naturalization processes for immigrants
working in the frontline during COVID-19 (OECD, 2020), and in the United Kingdom, frontline
healthcare workers have been exempted from paying an immigration health surcharge required for
temporary visa applications (Gower, 2020).
Various theories of attitudinal formation could help explain shifts in immigration sentiment during
COVID-19. Individual-level theories of self-interest (Olzak, 1994), social identity (Tajfel, 1982), and
moral foundation theories (Haidt, 2007) link feelings of fear, anxiety, and personal identification with a
foreign group to the formation of negative sentiment toward immigrants. As diseases are linked to specific
countries and populations are stigmatized, natives’ feelings of negative impacts of their wellbeing are
likely to contribute to the formation or intensification of negative attitudes toward certain populations
during pandemics, as has been documented over periods of economic recession (Burns and Gimpel,
2000). As suggested by group threat theories (Blumer, 1958), contextual-level factors may have also
contributed to intensifying or shifting attitudes toward immigrants as a result of perceived or actual
competition which could have been accentuated during lockdowns, closure of retail services, and
increased waiting times. Similarly, contact theories suggest that greater interaction promotes more
favorable attitudes toward immigrants (Allport et al., 1954). During COVID-19, interactions between
immigrants in key sectors and native-born individuals may have increased, and hence contributed to more
favorable attitudes toward immigrants in these sectors.
2.2. Contemporary trends in immigration sentiment
Large-scale surveys have been used to provide a global-scale perspective on public attitudes toward
immigration.1 The Gallup World Poll in 2012–2014 probably remains the most comprehensive source of
data on attitudes toward immigration with a coverage of 140 countries. Based on these data, the
International Organization for Migration (IOM, 2015) published the first global-scale study on public
attitudes toward immigration examining variations in respondents’ views on whether or not immigration
levels should be increased, decreased or stay at present levels. A key finding is that globally a greater
percentage of the population would prefer national levels of immigration to reduce (34%), rather than to
either stay at their present levels (22%) or to increase (21%) (IOM, 2015).
1Various global surveys exist, including The GallupWorld Poll, Pew Global Attitudes Survey, International Social Survey Program
(ISSP), the World Values Survey, and Ipsos Global Trends Survey (Dempster et al., 2020).
e36-4 Francisco Rowe et al.
#
#Yet, wide variability exists across world regions and countries. World regional averages from Gallup
World Poll show that people in every major region have a preference for either maintaining or increasing
current immigration levels, except in Europe (IOM, 2015). In Europe, residents appear to have the least
positive attitudes toward immigration globally, with 52% of the surveyed population, indicating a need to
reduce current immigration levels (IOM, 2015). Yet, a sharp divergence exists between Northern and
Southern Europe (IOM, 2015). Southern Europeans, including respondents in Spain and Italy, tend to
display more negative attitudes toward immigration preferring lower national immigration levels, while
northern Europeans favor maintenance or an increase in current immigration levels (IOM, 2015;
Dennison and Geddes, 2019. In Northern Europe, the United Kingdom stands out as an exception with
a larger percentage (69%) of the population in favor of lower immigration levels (IOM, 2015). By
contrast, attitudes toward immigration levels are more positive in the United States, with 63% of the
surveyed population indicating immigration levels should increase (IOM, 2015). Germany sits on a
middle ground with a larger percentage (49%) of people preferring immigration to remain at present
levels.
Immigration is however a complex multidimensional issue. Distinctive dimensions of immigration
sentiment exist which can even provide varying views of immigration. For instance, public attitudes
toward the perceived level of immigration have remained relatively constant over recent decades, while
the salience “perceived importance” of immigration has varied wildly in Europe (Dempster et al., 2020).
Analyzing the salience of immigration, recent longitudinal studies have revealed a softening in antiimmigration
sentiment across Europe. Based on 2014–2018 Eurobarometer data on feelings toward
immigrants from different origin countries, comparative studies have reported evidence of a decreasing
trend in anti-immigration sentiment across all 28 European Union countries, including the United
Kingdom (Dennison and Dražanová, 2018; Dennison and Geddes, 2019). This trend is observed for
feelings toward both EU and non-EU immigrants; yet, stronger negative feelings exist toward non-EU
immigrants (Dennison and Geddes, 2019). In the United Kingdom, this paradoxical trend of softening in
anti-immigration sentiment, against little change in perceived levels of immigration, started in 2014
before the Brexit Referendum and has continued since (Blinder and Richards, 2020; Schwartz et al.,
2020).
Analyzing different dimensions may even reveal the coexistence of contrasting views on immigration
sentiment. Existing evidence suggests that people are generally more positive about the impacts of
immigration on the economy, labormarket and culture than on immigration levels (Dempster et al., 2020).
For instance, Ueffing et al. (2015) showed that German residents feel strongly about maintaining or
reducing current levels of immigration, with an estimated 95%of respondents wanting to keep or decrease
existing levels. Yet, they have a relatively more positive view on the impacts of immigration on the
economy, labor market and culture, with 26, 29, and 54% (respectively) indicating that: immigrants are
perceived as generally good for the country’s economy, do not take jobs away from German-born
residents, and improve society by bringing new ideas and culture (Ueffing et al., 2015). Similarly, recent
British data indicate a prevailing and stable positive perception about the impact immigration had on the
country over 2015–2018, but a predominant desire to reduce existing immigration levels, with 58% of
respondents wanting to see a reduction in immigration (Kaur-Ballagan et al., 2017).
Thus, while existing data sources on public opinion about immigration provide a valuable understanding
of long-term changes in attitudes toward immigration and cross-national differences, key
challenges remain. Survey data represent the main source of immigration sentiment studies; however,
they are typically spatially coarse, costly, and infrequent (Rowe et al., 2021a). Existing data can be limited
through slow data releases and statistical representation, especially at small geographical units. Real-time,
frequent, exhaustive and internationally spanning information is crucial to monitor changing attitudes
toward immigrants during dynamic and fast-evolving events, such as pandemics. Twitter offers data with
these attributes to supplement and complement traditional data systems, to cover their gaps and feed into
real-time monitoring of immigration sentiment at an unprecedented temporal granularity and geographic
coverage.
Data & Policy e36-5
#
#2.3. Use of Twitter to capture immigration sentiment
Twitter data have increasingly been used to estimate, understand, and predict the spread of diseases
(Achrekar et al., 2011), misinformation (Vosoughi et al., 2018; Green et al., 2020), political polarization
(Conover et al., 2011), natural disasters (Bruns and Liang, 2012), and population movements (Zagheni
et al., 2014). Yet, a very small set of studies has employed Twitter data to measure immigration sentiment
(Righi, 2019). Flores (2017) probably represents the most substantive study. He conducted a quasiexperimental
design to assess the impact of a more restrictive immigration policy on rises in anti-
immigration attitudes in Arizona. Other research consists of focused case studies to explore the potential
use of Twitter data. Bartlett and Norrie (2015) explored online conversations relating to immigration,
analyzing the frequency of keywords and their temporal changes. They pointed to key considerations for
data collection, such as potential demographic biases and importance of a clearly defined set of search
words. Previous work also focused on exploring how Twitter could be used to understand the refugees’
experiences in Europe during the 2015–2017 refugee crisis (e.g., Gualda and Rebollo, 2016; UN Global
Pulse, 2017). More recent work (Freire-Vidal and Graells-Garrido, 2019; Freire-Vidal et al., 2021) has
explored how changes in migration sentiment are linked to specific emotions, and how positive and
negative immigration sentiment communities can be identified by analyzing their retweet network.
A related and growing area of research, in which Twitter has displayed great potential, is in the
detection and characterization of hate speech (Fortuna andNunes, 2018; Ribeiro et al., 2018). Hate speech
detection seeks to identify and classify threatening, harassing or seriously offensive language. Several
studies have employed Twitter data to study hate speech against immigrants (Bosco et al., 2017;
Sanguinetti et al., 2018; Basile et al., 2019; Comandini and Patti, 2019; Calderón et al., 2020). Hate
speech is seen as a specific expression of sentiment or attitudes and thus related studies have focused on
characterizing how the social network expresses hate. Our work takes a broader perspective. We develop
an analytical framework to measure and characterize the pulse of both positive and negative sentiment
toward migration.
Several methodological challenges have been identified in the use of Twitter data to measure
immigration sentiment (Flores, 2017). First, Twitter users make heavy use of slang, emoticons, emojis,
initialisms, acronyms, and punctuation to express sentiments. However, the word sets of lexicons
normally used to assign the sentiment scores do not contain these expressions, as these are attuned to
well-structured text and domain-specific. Second, punctuation, emoticons, emojis, capital shape, and
degree intensifier words are often used to signal increased sentiment intensity. Yet these elements are
generally dismissed, and positive and negative sentiment are only based on the text component of tweets.
Third, tweets contain contrasting statements (e.g., “Immigration is great, though it can have a negative
impact on rising housing prices”). By not capturing differences in sentiment intensity, tweets involving
contrasting statements are categorized as neutral sentiment (i.e., a score of zero). Fourth, sarcastic
statements are often misclassified by sentiment algorithms (UN Global Pulse, 2017). However, sarcastic
tweets generally account for a small fraction of tweets and have little to no impact on aggregate daily and
monthly metrics of sentiment (Flores, 2017; Freire-Vidal and Graells-Garrido, 2019).
3. Data and Methods
3.1. Data
We draw on a country-stratified random sample of 30.39 million tweets across Germany, Italy, Spain, the
United Kingdom, and the United States. Our sample covers the start of the COVID-19 pandemic between
December 1, 2019 to April 30, 2020. Differences in the data collection observed across countries reflect
variations in local tweet activity relating to migration-related topics. Data were collected via an application
programming interface (API) (Campan et al., 2019). We used Twitter’s Premium API, which
enables access to historical data with a monthly cap of 1.25million tweets. The API allows 500 tweets per
request at a rate of 60 requests per minute; access to tweets, retweets, URLs, hashtags and profile
geographic information; and, a total number of 2.5k requests per month.
e36-6 Francisco Rowe et al.
#
#The data were collected based on a random sampling strategy. To maximize our monthly API data
allowance, a sampling strategy was developed to collect a sample of 1.5k tweets for each day of the study
period. Geographic and language parameters were applied to each country’s search criteria, to filter for
tweets occurring within a country’s geographical boundaries and native language. Data were collected
around the peak hour of daily tweet activity. The Twitter API collects data in a reverse manner in relation
to time. We set the starting time of data collection 1 hr after the peak in daily activity. In that way, we
ensured that themost popular tweetswere included in our data set, and as a result, that our sentiment scores
were representative of the daily tweet sentiment activity relating to migration. This decision was based on
the observation that migration sentiment scores based on Twitter data around the peak time of daily tweet
activity are generally good predictors of sentiment scores based on a full-day data set. We conducted this
analysis for a sample of data for the United Kingdom and the United States during March, 2020. This
approach follows Rowe et al. (2021a) approach. They compared migration sentiment scores from: (a) a
sample containing tweets around the daily peak hour; and (b) a sample containing tweets all daily tweets
for seven full days for the United Kingdom. The resulting sentiment scores from both data sets
consistently identified similar daily patterns of immigration sentiment across four lexicons.
To collect tweets focusing onmigration, wewere guided by the principles of the Campbell policies and
guidelines standards for the conduct of systematic reviews (Campbell Collaboration, 2020). A key
component of conducting a systematic review is planning a search strategy to capture relevant content.
In consultation withmigration experts at IOM, a set of key search termswere developed, including words,
Twitter accounts and hashtags. No accounts of media outlets were specifically targeted to avoid biases in
our analysis. However, we captured relevant migration discussions posted bymedia outlets via our search
terms and other users. Media outlet accounts were excluded because they are known to have a political
leaning and thus a certain stand onmigration issues; and, they all publish large volumes of information on
a range of topics which would have limited our capacity to capture a wider variety of views on migration
given the Twitter cap restrictions. The list of words and hashtags used in our search terms is reported in
Supplementary Table S1. Twitter accounts are not displayed for ethical considerations. Table 1 reports the
data used in the analysis.
We translated tweets collected for Spain, Germany, and Italy samples into English using the Google
Cloud Translation API (Google, 2020). This was necessary as the lexicon used to measure tweet-level
sentiment only contains English words. While we did not directly assess how tweet-based sentiment
scores were impacted by the translation, existing research on machine translation and sentiment analysis
has shown that English lexicons applied to translated texts produce comparable results to lexicons applied
within a text in the original language as the resulting sentiment scores are word-based, rather than on the
semantic structure of text (Peisenieks and Skadins, 2014; Mohammad et al., 2016; Shalunts et al., 2016).
The translation was also necessary for our topic modeling approach, in order to make comparisons across
countries. Topic modeling relies on term frequency to identify themes and estimate similarity between
texts (Grün and Hornik, 2011). If applied to nontranslated tweets, it is likely that similarities between
conversations in different languages would remain unidentified, due to the differences in vocabulary,
making cross-country comparison less effective (Grün and Hornik, 2011). For topic modeling, we also
used the Natural Language Toolkit (NLTK) English stop words corpus to remove stop words (Loper and
Bird, 2002). This corpus is widely used in natural language processing research applications and can be
readily implemented in Python (Perkins, 2010).
Table 1. Summary of tweets collected between December 1, 2019 and April 30, 2020
Germany Italy Spain United Kingdom United States
Total tweets 914,731 3,286,956 1,726,168 11,931,467 12,528,015
Average daily tweets 6,018 21,625 11,356 78,496 82,421
Unique users 24,227 25,414 53,818 56,182 103,779
Data & Policy e36-7
#
#3.2. Sentiment analysis
To capture immigration sentiment, we used sentiment analysis, also known as opinion mining or emotion
artificial intelligence. Sentiment analysis refers to the use of natural language processing to systematically
identify, measure and analyse emotional states and subjective information. It computationally identifies
the polarity of text, that is, whether the underpinning semantics of an opinion is positive, negative or
neutral. Furthermore, it allows deriving quantitative scores to identify the attitude or position on the
distribution of negative or positive terms in a given piece of text.
We specifically employed VADER (Valence Aware Dictionary and sEntiment Reasoner) (Hutto and
Gilbert, 2014). VADER is a lexicon and rule-based sentiment analysis tool which is tailored to the analysis
of sentiments expressed in social media. VADER has been shown to perform better than 11 typical stateof-practice
sentiment algorithms at identifying the polarity expressed in tweets (Hutto and Gilbert, 2014),
and has remained one of the most widely used sentiment analysis methods for social media data (e.g.,
Elbagir andYang, 2020)—seeGhani et al. (2019) andRosa et al. (2019) for recent comprehensive reviews
of social media analytics.
VADER overcomes limitations of existing approaches by more appropriately handling informal text,
including the use of negations, contractions, slang, emoticons, emojis, initialisms, acronyms, punctuation,
and word-shape (e.g., capitalization) as a signal of sentiment polarity and intensity. Most commonly,
lexicon-based approaches only capture differences in sentiment polarity (i.e., positive or negative) but do
not identify differences in sentiment intensity (strongly positive vs. moderately positive) or contrasting
statements. They have also been designed to capture sentiment in well-structured sentences, generally
meaning their lexicons do not include slang, emoticons, emojis, acronyms, and capitalized word
differentiation. We note that accurate identification and scoring of sarcastic statements remain a key
challenge in natural language processing, but these statements tend to represent a small fraction of daily
tweets.
VADER provides a normalized, weighted composite score which captures the polarity and intensity of
individual tweets. The score ranges from 1 to þ1, representing the most negative to most positive
sentiment, respectively. Intuitively, to derive the score, VADER assigns a score to each word in a tweet,
ranging from 4 (extremely negative) through 0 (neutral) to þ4 (extremely positive) based on positive
and negative text features identified in the text. These scores are then aggregated and normalized to range
between 1 and þ1. We used the daily average of the composite score to track the daily evolution of
immigration sentiment on Twitter. We then identified positive sentiment tweets (i.e., composite
score > 0.05) and negative sentiment tweets (i.e., composite score < 0.05).
3.3. Topic modeling
We used topic modeling to identify themes or topics of discussion in our data. Topic modeling uses term
frequency measures to identify patterns in word-usage across a corpus of documents (Grün and Hornik,
2011). We employed latent Dirichlet allocation (LDA), which is an unsupervised topic modeling
approach (Blei et al., 2003; Negara et al., 2019). LDA is a hierarchical Bayesian model which assigns
each document within a corpus a list of weights, based on the prevalence of different topics within the text.
We chose this approach because it is a well-established methodology within topic modeling (Blei et al.,
2003; Grün and Hornik, 2011) and has been proven to perform well when classifying tweets (e.g.,
Ostrowski, 2015; Del Gobbo et al., 2021), as well as in other online contexts, such as product reviews
(Guo et al., 2017;Wang et al., 2018). As a first step, the user defines a desired number of topics. Themodel
then uses the frequency of terms across all documents in a corpus to probabilistically identify commonly
occurring collections of terms. Topic probabilities are then used to optimize the partitioning of the data
into the desired number of topics, with the aim of maximizing the probability of words occurring within a
topic, whilst minimizing the probability of them occurring within other topics. For each document, these
probabilities are then aggregated to create a document-level estimate as to the prevalence of each topic
within the text. A LDA model produces two sets of outputs: probabilities of words occurring within a
topic; and, estimates of topic prevalence within a document.
e36-8 Francisco Rowe et al.
#
#To decide the number of topics, we used “ldatuning” (Moor, 2020), an R packagewhich calculates how
well LDA group the data into distinct topics. Four sets of metrics—developed by Griffiths and Steyvers
(2004), Cao et al. (2009), Arun et al. (2010), and Deveaud et al. (2014)—were used. Supplementary
Figure S1 shows the results of these metrics for 2–20 topic solutions. We selected a data partition of
15 topics as it provided a good balance between level of detail, ease of interpretation and communication.
Each tweet was associated with one of the most prevalent topics occurring in the text. The tweet text was
processed before being used formodeling, removing punctuation, emojis, line breaks, URL links, account
names and stop words (Bird et al., 2009). The remaining words were then converted to their root form
using Lemmatization (NLTK, 2020).
Supplementary Figure S2 shows the 25 terms most likely to appear in each topic and the 15 topics
generated by our LDA model. In addition to examining individual words, we assessed the tweets
containing the most frequently occurring terms and identified those that most strongly associated with
each topic. Based on this information, we refined the resulting LDA typology to achieve a more optimal
partition of the data. To this end, we collapsed all groups of tweets relating to migration that could not be
clearly defined into a single miscellaneous topic, whilst groupings considered not relevant to migration
were assigned to a noise topic. The final solution comprises 12 topics which are described in
Subsection 4.4.
4. Results
4.1. Describing the extent of immigration-related tweet activity
Figure 1a reports the total number of tweets containing at least one of our search terms betweenDecember
1, 2019 and April 30, 2020. They capture all the tweet activity relating to migration. Figure 1b presents a
Figure 1.Migration-related tweet activity, COVID-19 cases and level of stringency measures, December
1, 2019 to April 30, 2020. (a) Number of daily immigration-related tweets and (b) immigration-COVIDrelated
tweets. Details about the selection of tweets reported are provided in Section 3.1. (c) Number of
cases (purple line) refers to the number of new COVID-19 cases per million. The stringency index (yellow
line) measures the level of nonpharmaceutical interventions to COVID-19, such as social distancing and
lockdown measures. Hundred indicates the strictest.
Data & Policy e36-9
#
#selection of these tweets; that is, tweets relating to migration and COVID-19. To identify these tweets, we
filtered our data based on a list of termswhich have been commonly used on Twitter to refer to COVID-19
(Twitter, 2020). Figure 1c displays the number of new COVID-19 cases per million and a stringency
index, which is part of theOxford COVID-19Government Response Tracker (OxCGRT) and a composite
indicator to measure the extent and variation of the nonpharmaceutical interventions globally, such as
school closures, workplace closures, travel restrictions and public gathering bans (Hale et al., 2020). The
indicator ranges from 0 (no measures) to 100 (the strictest possible scenario).
Figure 1 reveals a consistent level of immigration-related tweet activity with sudden spikes during key
events, except for Spain, which displays high variability. Comparatively higher volume of immigrationrelated
tweet activity is observed in Italy, the United Kingdom and particularly the United States, relative
to Germany and Spain. Figure 1 also reveals an escalation in immigration-COVID-19-related tweet
activity as the number of new COVID-19 cases started to rise in individual countries. Increases in
immigration-COVID-19-related tweet activity began first in Italy and Spain during late January and early
February, and in early-mid March in Germany, the United Kingdom, and the United States as the number
of new COVID-19 cases accelerated and strict lockdown measures were implemented. High levels of
immigration-COVID-19-related tweet activity seem to have remained relatively stable in Italy, the United
Kingdom and the United States during April, while a declining trend is observed in Germany and Spain.
4.2. Assessing the overall distribution of sentiment
Figure 2 displays the overall distribution of tweet sentiment scores betweenDecember 1 toApril 30, 2020.
As expected, it shows a high frequency of neutral polarity tweets across all five countries in our sample,
but also reveals a relatively high share of negative and positive polarity scores in the [0.5–1] range,
indicating the existence of a polarized discussion on issues relating to immigration. Negative sentiment
scores in this range account for an average of over 30% of all tweets across all five countries, which
exceeds the respective share of positive sentiment tweets (18.6%). The share of neutral sentiment tweets
accounts for a small fraction (13.1%), and scores ofmoderate negative and positive sentiment in the |0.05–
0.5| range record 18.1 and 16.4% of all tweets, respectively. These results may reflect a wider societal
trend of social polarization on controversial issues, in which immigration has featured as a key divisive
topic in all the countries in our sample. Social divisions have become increasingly aligned with partisan
identities in recent years (Dennison and Geddes, 2019) and attributed to echo chambers—patterns of
information—sharing that reinforce pre-existing beliefs by restricting exposure to opposing political
views (Bail et al., 2018). Social media is often believed to represent a main channel leading to selective
exposure to information and political polarization (Conover et al., 2011; Hong and Kim, 2016).
Across our sample, public opinion around immigration in Spain seems to be the most polarized.
Strongly negative sentiment scores account for 37.6%, while the share of strongly positive sentiment
scores is 20%. Germany, Italy, the United Kingdom and the United States display a similar spread of
sentiment scores, with a larger share of strongly negative sentiment scores. Italy displays a more balanced
distribution.
4.3. Determining the extent of short-term fluctuations in sentiment
Figure 3a shows the daily average tweet sentiment score. Figure 3b reveals the sentiment intensity
composition of tweets. Four key observations emerge from these figures. First, they reveal a consistently
predominant pattern of overall negative sentiment, except for Italy, where the daily average sentiment
score tends to fluctuate between0.05 and 0.All countries also display a trending daily average sentiment
score approaching 0 in late April, indicating neutral sentiment. Second, they reveal that the observed
polarization in the Twitter discussion around immigration in Spain reported above based on the overall
distribution of sentiment scores is fairly persistent over time, showing consistently large daily average
sentiment scores approximating 0.5. Third, a cyclical pattern of short-term fluctuations of increased
negative immigration sentiment in reaction to key events (see Figures 1 and 3). For instance, Italy displays
e36-10 Francisco Rowe et al.
#
#a greater share of negative sentiment tweets in December 18, 2019 and February 12, 2020 reflecting
tweets on a criminal case against Matteo Salvini, a far-right leader, accused of preventing 131 migrants
from disembarking from a coast guard ship. Similarly, the United Kingdom records a greater share of
negative sentiment tweets in January 31, 2020 and February 18, 2020 relating to the day the country exited
the European Union and following the announcement of a new points-based immigration system.
Fourth, the results show little fluctuation in the average daily sentiment score suggesting no significant
intensification of negative sentiment tweets during early stages of the COVID19 pandemic. This is an
Figure 2. Density (a) and cumulative (b) distribution of sentiment scores.
Data & Policy e36-11
#
#unexpected finding, given the significant rise in immigration-COVID-related tweets observed following
increases in the number of new COVID-19 cases, roll-out of stringent nonpharmaceutical interventions
and reports of rising xenophobic and racist incidents against people of Asian descent and appearance.
However, coincidingwith these events, slightly larger average daily negative scores during late February–
mid April are observed in Germany, the United Kingdom, and the United States, suggesting a moderate
rise in negative immigration-related sentiment. Additionally, these trends also indicate that the increase in
immigration-COVID-related tweet activity observed in Figure 1 does not only reflect a rise in tweets
carrying negative immigration sentiment narratives, but also represents a comparable increase in tweets
containing a positive immigration sentiment (see Figure 3b).
4.4. Identifying emerging sentiment narratives
To identify the narratives underpinning changes in immigration-related sentiment, we applied topic
modeling as described in Section 3, and identified 12 distinctive topics:
1. Human Rights Abuses (12.38%): This cluster is largely composed of tweets reporting abuses of
human rights, including harassment, sexual abuse, homicide, torture, oppression of civil liberties,
and genocide. A particularly prominent theme is the detention and deportation of migrants and
refugees on the US Southern Border. Tweets expressing concern about COVID-19 and conditions
of detention centers, as well as migrants’ access to medical facilities were also present.
Figure 3.Daily evolution of tweet sentiment: (a) Average overall sentiment score. Smoothed conditional
means are reported and were estimated via locally weighted scatterplot smoothing (loess) using a span of
0.3. (b) Percentage of sentiment scores classified into strongly negative (<0.5), negative (0.5 to
0.05), neutral (0.05 to 0.05), positive (0.05 to 0.5), and strongly positive (>0.5).
e36-12 Francisco Rowe et al.
#
#2. Activism (1.68%): This cluster includes tweets relating to activism around a variety of social
issues, involving racism, sexism, ableism, and homophobia linked to migration. The prominent
term is a hashtag “#LeaveNoOneBehind.”
3. Trump (11.15%): This grouping encapsulates discussions about the Trump administration and its
stance on immigration. Tweets tend to express neutral or negative views toward Trump’s
immigration stance, including changes in US immigration policy, voicing anger at the President
and advisors, or defending the rights and contribution of immigrants to the US society. A smaller
minority of tweets in defense of the Trump administration is also included.
4. Vulnerable EU Migrants (7.16%): This cluster primarily involves views about immigrants and
refugees in detention and holding camps in Southern Europe, particularly Greece and Turkey. It
largely revolves around Turkey’s decision to open their border in early March, allowing 4 million
migrants and refugees to enter Europe (Bhatti and Apostolou, 2020; Rankin, 2020). It contains
pro-migrant tweets expressing empathy about the poor health and safety conditions of migrant
camps, including COVID-19 related risks, and demands on camp closure or opening of EU
borders. Neutral tweets tend to report related news. Negative tweets tend to express fear and anger
toward migrants and refugees in camps.
5. Legal Assistance (1.58%): This group largely includes tweets discussing migration-related legal
issues, such as communicating changes in immigration laws, and providing legal travel and
migration support services (e.g., visa applications).
6. COVID-19 East Asian Prejudice (15.22%): Tweets in this category generally refer to COVID-19related
prejudice behavior against individuals of Asian background, particularly China. The most
prominent sentiment is anger toward the Chinese government and people, and tweets accusing the
Chinese of causing the pandemic. This cluster also includes expressions of sympathy and
solidarity with Chinese and Asian regions.
7. Racism and Xenophobia (4.74%): This topic includes tweets on racism and xenophobia relating to
right-wing extremist politics, mostly expressing anger toward social intolerance, accusing people
or institutions of discriminating against immigrants, as well as religious, ethnic or other minority
groups.
8. Undocumented Immigration (10.85%): Tweets on this topic largely involve expressions of
animosity toward undocumented immigrants. Prevalent concerns include migrants’ illegal status,
their burden on taxpayers through benefits and relief funds, and potential to obstruct democratic
processes through voter fraud. These tweets tend to use detrimental language, referring to
migrants as “illegals,” “aliens,” or “criminals.”
9. Brexit (4.41%): This topic includes tweets relating to Brexit and impact on migrants. Prominent
issues include proposed changes to the UK immigration policy and implications for EUmigrants.
10. Migrant Boat Crossings (6.95%): This cluster primarily involves tweets about migrant boat
crossing, relating to the UK, Spain and particularly Italy. Tweets mostly refer to Africans crossing
the Mediterranean to the coast of Italy.
11. Miscellaneous (8.77%): This category includes a variety of tweets related to migration that could
not be easily assigned to other categories. A prominent theme is advertisements for events relating
to migration, including lectures, debates, academic publications, and opportunities at research
institutions. It also contains immigration related tweets from topics which could not be clearly
defined (see plots 12 and 13, Supplementary Figure S2).
12. Noise (15.09%): This topic incorporates all tweets considered not relevant to migration filtered
from topics 5, 9, and 11. It includes themes such as commercial law, UK politics and adverts for
nonmigrant events which are neutral in sentiment. It also includes an additional cluster of
miscellaneous tweets generated by the LDA (see plot 1, Supplementary Figure S2).
Figure 4 shows the topic composition of tweets for individual countries. Supplementary Figure S3 reports
a breakdown of this composition by positive and negative sentiment scores. Whilst a range of views on
immigration exists in each country, Figure 4 reveals a consistent prevalence of certain topics. In Germany,
Data & Policy e36-13
#
#vulnerable EU migrants represents the dominant topic, comprising 39% of all German tweets and
reflecting concerns around the health and safety conditions of migrants in refugee camps, and opening
of Turkey’s EU borders to immigrants. In Italy, migrant boat crossings is the most prevalent topic,
accounting for 52% of all tweets in Italy. This topic reflects an ongoing migrant debate largely defined by
Mediterranean boat crossings originating from Africa. In Spain, racism and xenophobia is the most
prominent topic, involving 33% of all tweets. The predominance of this topic reflects the high degree of
negative sentiment observed in Spain (see Figure 2) as related tweets make use of highly detrimental
language against migrants. In the United Kingdom and the United States, COVID-19 East Asian
prejudice is the most prevalent topic followed by Trump, human rights abuses, and undocumented
Figure 4. Per cent of tweets by topic. See text for a description of each topic.
e36-14 Francisco Rowe et al.
#
#immigration—all accounting for over 10% of the respective country’s total tweet count. Brexit also
features as a key topic in the United Kingdom with 7.7% of all tweets.
Figure 4 reveals the topics underpinning the temporal changes in daily average sentiment observed in
the previous section. It shows that the strengthening in negative sentiment observed in the United
Kingdom and the United States was underpinned by a noticeable rise in tweets linked to the COVID19
East Asian prejudice topic, particularly during mid-March to mid-April. This rise coincides with the
remarks made by US President Trump referring to COVID-19 as the “Chinese Virus” on March 17,
triggering an increase in the use of negative radicalized terms (see Supplementary Figure S3). Prior to this
period (i.e., late February–early March), increased negative sentiment seems to have been driven by
tweets linked to three topics: Trump, vulnerable EU migrants, and undocumented immigration. A key
event linked to the vulnerable EUmigrants topic is Turkey’s decision to let 4millionmigrants into Europe
in early March (Bhatti and Apostolou, 2020; Rankin, 2020), which seems to have resulted in increased
negative sentiment in all five counties.
In Germany, the topicCOVID-19 East Asian prejudice had very little prominence.What seems to have
driven an increase in negative sentiment during late February and early April is the vulnerable EU
migrants topic. During this period, this topic comprised between 35 and 52% of all tweets in Germany.
While positive and negative messages were expressed, tweets containing negative language were
expressed more strongly, resulting in higher average sentiment scores. For example, for the first 3 weeks
of March, it was 0.52 exceeding average positive sentiment score of 0.44.
The share of the category noise varies across countries. It ranges from only 5% of all tweets in Italy to
11% in Spain. This category tends to comprise tweets with neutral sentiment content, and thus it exerts a
very marginal impact on the daily average sentiment score reported in Figure 3. In fact, if anything, it
would offset some of the negative sentiment, particularly in Spain where the noise category is more
prevalent.
Figure 4 also reveals key topics underpinning the softening of negative sentiment observed toward the
end of April in Spain, the United Kingdom, and the United States. In Spain, an increase in positive
miscellaneous tweets relating to academic work seems to be a key cause (see Supplementary Figure S3).
In the United Kingdom and the United States, three key changes appear to explain this pattern. First, the
share of tweets related to the COVID-19 East Asian prejudice topic declined in late April, resulting in a
decline in associated expressions of negative sentiment. Second, both countries experience an increase in
tweets relating to the Trump topic from April 21, lasting for approximately a week. This shifted the focus
of the discussion in both countries to the US President’s announcement of plans to suspend all
immigration to the United States (Robinson, 2020). Associated tweets were characterized by a high
proportion of neutral to moderately positive tweets (see Figure 3b).
5. Discussion
5.1. Key results
Incidents of rising anti-immigration sentiment have been reported since the start of the COVID-19
pandemic. Yet, lack of data has prevented large-scale analysis of shifts in immigration sentiment. Drawing
on Twitter data, we developed a novel framework to measure and monitor changes in immigration
sentiment during the COVID-19 pandemic. While we found evidence of an increase in the number of
tweets relating to immigration and COVID-19 as the number of new cases rise and lockdown measures
were implemented, we did not find consistent evidence of a significant increase in anti-immigration
sentiment in any of the five countries in our sample. Yet, we found evidence of a moderate rise in
immigration-related tweets conveying negative sentiment during the period of rapidly rising COVID-19
cases in late February to mid-April in Germany, the United Kingdom, and the United States.
We also presented a novel classification of immigration-related tweets, identifying 11 distinct topics
(excluding a noise category—see Section 3.3). This classification revealed the prevalence of particular
immigration-related discussions in individual countries over time: vulnerable EU migrants in Germany,
Data & Policy e36-15
#
#migration boat crossings in Italy, racism and xenophobia in Spain, East Asian prejudice, Trump, human
rights abuses, and undocumented immigration in the United Kingdom and the United States. It also
revealed that an increase in negative sentiment tweets linkingCOVID-19 and immigrants underpinned the
moderate rise in negative sentiment in theUnitedKingdom and theUnited States, particularly duringmidMarch
to mid-April. Prior to this period (i.e., mid-February/early March), topics revolving around US
President Trump’s stance on immigration, vulnerable EU migrants and undocumented immigration
largely underpinned the rise in negative sentiment in both countries. In Germany, tweets around racism
and xenophobia, and vulnerable EU migrants were associated with the moderate rise in negative
sentiment observed during late February–mid April.
We also provided evidence of a consistent pattern of moderate negative sentiment across all countries
in our sample. Shifts in tweet sentiment occurred but the observed changes were temporary. Spain stood
out as the country with the largest daily negative sentiment scores, which were associated with a
discussion around racism and xenophobia that tended to include strong negative language. We also
offered evidence of social polarization in relation to immigration, showing a relatively high concentration
of strongly positive and negative sentiment tweets. Neutral, moderate positive and negative sentiment
tweets were less prominent.
5.2. Interpretation
Our findings suggest a mild intensification in negative sentiment only in Germany, the United Kingdom,
and the United States, despite a significant increase in tweet activity as the pandemic unfolded in late
February–early March in the five countries in our sample. This is because, coupled with a rise in negative
sentiment tweets on immigration, there was also an increase of comparable magnitude in positive
sentiment tweets, highlighting the contribution of migrants to the COVID-19 response working in
essential occupations, such as the healthcare, food and agriculture, and hospitality sector, particularly
in the United Kingdom and United States (ODI, 2020). In addition, positively praised, empathetic tweets
expressing concerns about the ways COVID-19 was impacting vulnerable migrant groups were particularly
prominent inGermany. The resulting overall daily sentiment of the discussion in individual countries
was thus relatively balanced.
We presented evidence of a predominantly negative immigration sentiment in Germany, the United
Kingdom and the United States, particularly in Spain, and to a lesser extent in Italy. These findings are
consistent with existing research indicating predominantly negative attitudes in varying intensity in
Europe (Dennison and Geddes, 2019; Heath et al., 2020) and a pattern of declining but persistent
opposition to immigration in the United States (Heath et al., 2020). We also presented evidence which
expands existing work by showing the short-term shifts in immigration sentiment. These changes
highlight how the prevalent sentiment may experience temporary changes in response to key political,
xenophobic and abusive incidents, but also statements fueling misinformation.
We also contributed to existing research by capturing the multidimensional views of immigration.
While studies have explored various dimensions of immigration attitudes, prior empirical work tends to
focus on a single dimension of immigration, particularly on the perceived level or salience of immigration
(Dempster et al., 2020). Yet, public perception varies widely across various dimensions (Ueffing et al.,
2015). We provided a more holistic perspective of the range of topics which contribute to the manifestation
of immigration attitude. We presented a classification of 11 topics which help to characterize the
themes and issues discussed in individual countries. These topics capture persistent discussions around
racism, xenophobia, illegal immigration, and human right abuses, but also issues which emerge more
recently, such as COVID-19, Brexit, vulnerable EU migrants and Trump’s stance on immigration. As a
result, our measure of immigration sentiment can be conceived as a composite indicator of attitudes
toward immigration comprising a range of perceptions.
Taken together, our findings underscore the dynamic and responsive nature of Twitter users, and the
capacity of tweets with negative sentiment content to generate immediate positive sentiment responses
and vice versa (Bae and Lee, 2012). Second, they also highlight that Twitter data can serve as a useful
e36-16 Francisco Rowe et al.
#
#resource to measure and monitor immigration-related sentiment in real or near real-time. Global trends
indicate that newmicrobial threats will continue to emerge at an accelerating rate (Jones et al., 2008), and
we know that pandemics trigger stigmatization (Link and Phelan, 2006). The unpredictable emergence
and rapid geographical proliferation of stigmatization during pandemics mean that developing effective
strategies for inhibiting the spread of stigmatization is challenging. Traditional sources of information on
public attitudes are expensive and hard to collect in pandemic settings, especially if we are dealing with
stigmatization on a global scale that accounts for multiple contexts. We now have time to prepare and
anticipate the next pandemic outbreak or events that may cause sudden shifts inmigration sentiment—and
develop analytical frameworks to monitor these shifts, to appropriately inform policy interventions as to
when and where shocking racist and xenophobic incidents are occurring.
We also presented evidence of immigration as a highly divisive societal issue, particularly in Spain.
Evidence of increasing political polarization exists in the United Kingdom and the United States, and
immigration has featured as a main divisive topic (Conover et al., 2011; Hong and Kim, 2016; Bail et al.,
2018). Social media is likely to reinforce and geographically expand this trend to other national settings.
Social media echo chambers are expected to reinforce pre-existing beliefs by restricting exposure to
opposing views (Bail et al., 2018). Social media may thus lead to increased social polarization regarding
immigration, and rising cases of xenophobia, racism and prejudice. Understanding social media networks
is thus critical to determine how anti-immigration sentiment content is shared and spread, as well as
identifying key spreaders of negative sentiment and active bots sharing misinformation. Such understanding
is essential to disrupt the spread of misconceptions on which negative immigration sentiment is
usually based (European Commision, 2019). Recent evidence demonstrated that knowing key pieces of
information underpinning immigration-related misbeliefs is key to fostering a more favorable view
toward immigration (European Commision, 2019). Effective and timely monitoring of public opinion
about immigration policy changes and impacts may help formulate interventions to reduce anti-immigration
attitudes and foster social cohesion. If rigorously and responsibly handled, Twitter data would be
an important tool to expand our existing knowledge of the misconceptions and contact network
underpinning the formation of attitudes toward immigration, especially if they are used to complement
existing data sources. Facilitating access to, and handling of, Twitter data represent major challenges that
need to be addressed to deliver this promise (Kim et al., 2013).
5.3. Challenges and limitations
Gauging the wider generalizability of our findings is challenging. Twitter users are not representative of
national populations, being over-represented by younger age groups, particular ethnicities and socioeconomic
status (McCormick et al., 2017; Sloan, 2017). Extending our analysis to incorporate other
platforms that each have different demographics and purposes can help to broaden the generalizability of
our results, as well as integrating these data with information from traditional survey sources. An
emerging approach to improve public opinion estimates based on Twitter data is the use of demographic
sample weights (McCormick et al., 2017), which may need prediction of demographic attributes from
Twitter users’ information profiles (Graells-Garrido et al., 2020) as these attributes are not always or rarely
available. As this area develops and best practices are established, the design and implementation of
sample weights represents a fruitful area for future research.
We analyzed changes in migration sentiment over early stages of the pandemic. Future research is
needed to establish long-term trend changes in attitudes toward immigration extending beyond the
pandemic, and assess if the changes identified in our analysis were indeed temporary. The pandemic
may have actually triggered long-term scarring effects on the way in which East Asian populations are
perceived. Additionally, we identified key topics and events related to these changes in migration
attitudes. Future research could expand this analysis by identifying the key factors influencing shifts in
migration sentiment during the pandemic. This is however a challenging task requiring a causal
experimental design and integration of disparate data sources, if available, to measure and isolate the
influence of individual-, family-level and contextual factors on attitudes toward migration. Such analysis
Data & Policy e36-17
#
#could enable assessing the impact of specific social, political, economic and media events on attitudes
toward immigration, and testing of hypotheses from theories on attitudes formation.
A key challenge in measuring sentiment from text is the appropriate interpretation of sentiment scores.
Not all messages associated with negative (positive) sentiment scores convey negative (positive)
sentiment toward immigration. Messages may contain multiple opinion targets with contradicting
sentiment expressions. Sentiment may also relate to other textual features in a tweet, such as processes
of slavery and human trafficking, rather than immigration or immigrants. Antimigrant groups may use
expressions containing positive sentiment to celebrate events which restrict migrant rights (Rowe et al.,
2021a). Interpretation of sentiment polarity scores must thus be coupled with a systematic analysis of the
original tweets as we have proposed in this article through the use of topic modeling.
Additionally, an unpredictable amount of “noise” (i.e., tweets which are not directly relevant to a
search criteria) is likely to infiltrate into the data collection.We identify two key sources of “noise”: (a) the
use of search terms; and, (b) the use ofAPIs. The use of specific wordswill necessarily entail the collection
of tweets which contain a term but relating to a very different context. For example, our search terms
captured a small number of tweets containing the term “migration” in the context of bird migration.
Similarly, the use of API implies some noise will be introduced into the resulting data sets as the default
option of the Twitter keyword operator is to search wound and unwound URLs within tweets (Twitter,
2020), and while URLs tend to contain words included in the search criteria, the tweet message may not.
Future research based on Twitter should consider strategies to mitigate this unpredictable amount of noise
in the data collection design while minimizing the loss of some relevant tweets.
6. Conclusion
COVID-19 has inflicted a significant health cost and triggered awave of prejudiced behavior and physical
attacks against individuals of Asian descendent and appearance. Lack of timely data has prevented
detailed analysis of the evolution of immigration sentiment since the start of the pandemic. Yet, robust and
scalable approaches are needed to understand the scale and spread of these discriminatory acts and
develop appropriate counter-measures. Drawing on Twitter data, this article proposed a framework to
measure and monitor changes in attitudes toward immigrants during early stages of the COVID-19
outbreak in Germany, Italy, Spain, the United Kingdom, and the United States. We presented evidence of
growing social polarization concerning migration, showing high concentrations of strongly positive and
strongly negative sentiments. We also found evidence of an increase in the number of tweets relating to
immigration and COVID-19, but no evidence of a significant increase in anti-immigration sentiment is
apparent in our sample as rises in the volume of negative messages are offset by comparable increases in
positive messages. These results do not mean that governments should ignore the potential impact of
misinformation fuelling misconceptions and prejudiced behavior against immigrants. Rather, a systematic
approach is needed to monitor immigration sentiment and identify when and where pandemic-related
racist and xenophobic incidents are occurring. Social media has become a key battleground in the fight
against misinformation and social hazards during COVID-19. It is important that physical as well as
digital spaces remain safe, accessible and free from abuse, and that people are not stigmatized, and their
fears and distress are not exploited during crises.
Supplementary Materials. To view supplementary material for this article, please visit http://dx.doi.org/10.1017/dap.2021.38.
Data Availability Statement. The data, code, and relevant description to replicate the analysis and results reported in this article
can be found in an open-access Github repository registered on the Open Science Framework with DOI https://osf.io/84jwv/
Acknowledgments. We are grateful to Mark Green for sharing some base code for data collection and pre-processing. We also
acknowledge a pre-print of the current article (Rowe et al., 2021b; Rowe et al. 2021c).
Funding Statement. This research was supported by grants from the University of Liverpool Global Challenges Strategy Group
Grant; Liverpool COVID-19 Partnership Strategic Research Fund.
e36-18 Francisco Rowe et al.
#
#Competing Interests. The authors declare no competing interests exist.
Author Contributions. F.R. devised the project, the main conceptual ideas, and proof outline. F.R. and M.M. performed the
numerical calculations and wrote the first draft. F.R., M.M., and E.G.-G. discussed the technical details. M.M. collected the data.
F.R., M.M., E.G.-G., M.R., and N.S. discussed the results, revised the manuscript, and approved the final version.
Ethical Standards. The research meets all ethical guidelines, including adherence to the legal requirements of the study country.
Ethical approval for the project was granted by the University of Liverpool Research Ethics Committee (ref: 7654).
References
AchrekarH,GandheA,LazarusR,YuSHandLiuB (2011) Predicting flu trends using Twitter data. In 2011 IEEEConference on
Computer Communications Workshops, INFOCOM WKSHPS 2011. Shanghai: IEEE, pp. 702–707.
Allport GW, Clark K and Pettigrew T (1954) The nature of prejudice.
ArunR,SureshV,MadhavanCVandMurthyMN (2010)On finding the natural number of topicswith latent Dirichlet allocation:
Some observations. In Pacific-Asia Conference on Knowledge Discovery and Data Mining. Berlin: Springer, pp. 391–402.
BaeYandLeeH (2012) Sentiment analysis of twitter audiences:Measuring the positive or negative influence of popular twitterers.
Journal of the American Society for Information Science and Technology 63(12), 2521–2535.
Bail CA, Argyle LP, Brown TW, Bumpus JP, Chen H, Fallin Hunzaker MB, Lee J, Mann M, Merhout F and Volfovsky A
(2018) Exposure to opposing views on social media can increase political polarization. Proceedings of the National Academy of
Sciences of the United States of America 115(37), 9216–9221.
Bartlett J and Norrie R (2015) Immigration on Twitter: Understanding Public Attitudes Online. Technical Report. London:
Demos.
Basile V, Bosco C, Fersini E, Debora N, Patti V, Pardo FMR, Rosso P and Sanguinetti M (2019) SemEval-2019 task 5:
Multilingual detection of hate speech against immigrants and women in Twitter. In 13th International Workshop on Semantic
Evaluation. Minneapolis, MN: Association for Computational Linguistics, pp. 54–63.
BBC (2020) Coronavirus: New Zealand Nurse Who Treated Boris Johnson Says It Was ‘Surreal’. Available at https://www.bbc.
co.uk/news/uk-52268341 (accessed 9 December 2020).
Berger BE, Ferrans CE and Lashley FR (2001) Measuring stigma in people with HIV: Psychometric assessment of the HIV
stigma scale. Research in Nursing and Health 24(6), 518–529.
Bhattacharya P, Banerjee D and Rao TS (2020) The “untold” side of COVID-19: Social stigma and its consequences in India.
Indian Journal of Psychological Medicine 42(4), 382–386.
Bhatti J and Apostolou N (2020) Turkey Opens Borders, Reignites Refugee Crisis in Europe. Available at https://www.
washingtontimes.com/news/2020/mar/4/turkey-opens-borders-reignites-refugee-crisis-euro/ (accessed 9 December 2020).
Bird S,KleinE andLoperE (2009)Natural Language Processingwith Python: Analyzing Text with the Natural Language Toolkit.
Newton, MA: O’Reilly Media.
Blei DM, Ng AYand Jordan MI (2003) Latent Dirichlet allocation. Journal of Machine Learning Research 3, 993–1022.
Blinder DS and Richards DL (2020)UK Public Opinion toward Immigration: Overall Attitudes and Level of Concern. Technical
Report. Oxford: The Migration Observatory.
Blumer H (1958) Race prejudice as a sense of group position. Pacific Sociological Review 1(1), 3–7.
Bosco C, Patti V, Bogetti M, Conoscenti M, Ruffo GF, Schifanella R and Stranisci M (2017) Tools and resources for detecting
hate and prejudice against immigrants in social media. In Symposium III. Social Interactions in Complex Intelligent Systems
(SICIS). Bath: AISB, pp. 79–84.
Bruns A and Liang YE (2012) Tools and methods for capturing Twitter data during natural disasters. First Monday 17(4), 1–8.
Burns P and Gimpel JG (2000) Economic insecurity, prejudicial stereotypes, and public opinion on immigration policy. Political
Science Quarterly 115(2), 201–225.
Calderón CA, de la Vega G and Herrero DB (2020) Topic modeling and characterization of hate speech against immigrants on
twitter around the emergence of a far-right party in Spain. Social Sciences 9(11), 188.
Campan A, Atnafu T, Truta TM and Nolan J (2019) Is data collection through Twitter streaming API useful for academic
research? In Proceedings - 2018 IEEE International Conference on Big Data, Big Data 2018. Piscataway, NJ: Institute of
Electrical and Electronics Engineers, pp. 3638–3643.
Campbell Collaboration (2020) The Campbell Collaboration. Available at http://www.campbellcollaboration.org/. [online;
accessed 09-December- 2020].
Cao J,XiaT,Li J,ZhangYandTang S (2009)A density-basedmethod for adaptive LDAmodel selection.Neurocomputing 72(7–
9), 1775–1781.
Coates M (2020) Covid-19 and the rise of racism. BMJ 369, m1384.
Comandini G and Patti V (2019) An impossible dialogue! Nominal utterances and populist rhetoric in an Italian twitter corpus of
hate speech against immigrants. In Third Workshop on Abusive Language Online. Florence: Association for Computational
Linguistics, pp. 163–171.
Conover M, Ratkiewicz J, Francisco M, Gonçalves B, Menczer F and Flammini A (2011) Political polarization on twitter.
Proceedings of the International AAAI Conference on Web and Social Media 5, 89–96.
Data & Policy e36-19
#
#Cowper A (2020) Covid-19: Are we getting the communications right? BMJ 368, m919.
DavtyanM,BrownB and FolayanMO (2014) Addressing ebola-related stigma: Lessons learned fromHIV/AIDS.Global Health
Action 7(1), 26058.
DelGobboE,Fontanella S, SarraA andFontanella L (2021) Emerging topics inBrexit debate on Twitter around the deadlines: A
probabilistic topic modelling approach. Social Indicators Research 156, 669–688.
DempsterH,LeachA andHargraveK (2020)Public Attitudes Towards Immigration and Immigrants.Technical Report. London:
Overseas Development Institute.
Dennison J and Dražanová L (2018) Public Attitudes on Migration: Rethinking How People Perceive Migration. Technical
Report. Florence: European University Institute.
Dennison J and Geddes A (2019) A rising tide? The salience of immigration and the rise of anti–immigration political parties in
Western Europe. The Political Quarterly 90(1), 107–116.
Deveaud R, SanJuan E and Bellot P (2014) Accurate and effective latent concept modeling for ad hoc information retrieval.
Document Numérique 17(1), 61–84.
Elbagir S andYang J (2020) Sentiment analysis on Twitter with Python’s natural language toolkit and VADER sentiment analyzer.
In IAENGTransactions onEngineering Sciences: Special Issue for the International Association of Engineers Conferences 2019.
Singapore: World Scientific, p. 63.
European Commision (2019) 10 Trends Shaping Migration. Technical Report. Brussels: European Commission.
Fasani F andMazza J (2020) Immigrant KeyWorkers: Their Contribution to Europe’s COVID-19 Response. Available at SSRN:
https://ssrn.com/abstract=3584941. [online; accessed 09-December- 2020].
Flores RD (2017) Do anti-immigrant laws shape public sentiment? A study of Arizona’s SB 1070 using twitter data. American
Journal of Sociology 123(2), 333–384.
Fortuna P and Nunes S (2018) A survey on automatic detection of hate speech in text. ACM Computing Surveys (CSUR) 51(4),
1–30.
Freire-Vidal Y and Graells-Garrido E (2019) Characterization of local attitudes toward immigration using social media. In
Companion Proceedings of the 2019 World Wide Web Conference. New York: ACM, pp. 783–790.
Freire-Vidal Y,Graells-Garrido E andRowe F (2021) A framework to understand attitudes towards immigration through Twitter.
Applied Sciences 11(20), 9689.
Gelatt J (2020) Immigrant Workers: Vital to the US COVID-19 Response, Disproportionately Vulnerable. Washington, DC:
Migration Policy Institute.
GhaniNA,HamidS,Hashem IATandAhmedE (2019) Socialmedia big data analytics: A survey.Computers inHumanBehavior
101, 417–428.
Google (2020) Cloud Translation Documentation. Available at https://cloud.google.com/translate/docs/.
Gower M (2020) The Immigration Health Surcharge. Available at https://commonslibrary.parliament.uk/research-briefings/cbp7274/
(accessed 9 December 2020).
Goyder J (1986) Surveys on surveys: Limitations and potentialities. Public Opinion Quarterly 50(1), 27.
Graells-Garrido E,Baeza-Yates R andLalmasM (2020) Every colour you are: Stance prediction and turnaround in controversial
issues. In WebSci 2020 - Proceedings of the 12th ACM Conference on Web Science. New York: Association for Computing
Machinery, pp. 174–183.
Green M, Musi E, Rowe F, Charles D, Darlington-Pollock F, Kypridemos C, Morse A, Rossini P, Tulloch J, Dearden E,
Maheswaran H, Singleton A, Vivancos R and Sheard S (2020) Identifying how the spread of misinformation reacts to the
announcement of the UK national lockdown: An interrupted time-series study. Big Data & Society 8(1), 205395172110138.
GreenM, Pollock FD and Rowe F (2021) New forms of data and new forms of opportunities to monitor and tackle a pandemic. In
COVID-19 and Similar Futures. Heidelberg: Springer, pp. 423–429.
Griffiths TL and Steyvers M (2004) Finding scientific topics. Proceedings of the National Academy of Sciences of the United
States of America 101(SUPPL. 1), 5228–5235.
Grigorieff A,Roth C and Ubfal D (2020) Does information change attitudes toward immigrants?Demography 57(3), 1117–1143.
Grün B and Hornik K (2011) Topicmodels: An R package for fitting topic models. Journal of Statistical Software 40(13), 1–30.
GualdaE andRebollo C (2016) The refugee crisis on twitter: A diversity of discourses at a European crossroads. Journal of Spatial
and Organizational Dynamics 4(3), 199–212.
Guo Y, Barnes SJ and Jia Q (2017) Mining meaning from online ratings and reviews: Tourist satisfaction analysis using latent
Dirichlet allocation. Tourism Management 59, 467–483.
Haidt J (2007) The new synthesis in moral psychology. Science 316(5827), 998–1002.
Hale T, Petherick A, Phillips T and Webster S (2020) Variation in Government Responses to COVID-19. Blavatnik School of
Government Working Paper, 31, 2020-11.
Heath A, Davidov E, Ford R, Green EG, Ramos A and Schmidt P (2020) Contested terrain: Explaining divergent patterns of
public opinion towards immigration within Europe. Journal of Ethnic and Migration Studies 46(3), 475–488.
Home Affairs Committee (2020) Oral Evidence: Home Office Preparedness for Covid-19 (Coronavirus), HC 232. Available at
https://committees.parliament.uk/oralevidence/359/default/ (accessed 9 December 2020).
Hong S and Kim SH (2016) Political polarization on twitter: Implications for the use of social media in digital governments.
Government Information Quarterly 33(4), 777–782.
e36-20 Francisco Rowe et al.
#
#Hoosuite & We Are Social (2020) Digital 2020: Global Digital Overview. Available at https://blog.hootsuite.com/social-mediausers-pass-4-billion
(accessed 8 February 2021).
Hutto C and Gilbert E (2014) VADER: A parsimonious rule-based model for sentiment analysis of social media text. In Eighth
International Conference on Weblogs and Social Media (ICWSM-14). Menlo Park, CA: Association for the Advancement of
Artificial Intelligence, pp. 216–225.
IOM (2015) How the World Views Migration. Technical Report. Geneva: International Organization for Migration.
Jones KE, Patel NG,LevyMA, StoreygardA,Balk D,Gittleman JL andDaszak P (2008) Global trends in emerging infectious
diseases. Nature 451(7181), 990–993.
Kaur-Ballagan K,Gottfried G andAslaksen AP (2017) Shifting Ground: Attitudes towards immigration and Brexit. Available at
https://www.ipsos.com/ipsos-mori/en-uk/shifting-ground-attitudes-towards-immigration-and-brexit (accessed 9 December
2020).
KimAE,HansenHM,Murphy J,RichardsAK,Duke J andAllen JA (2013)Methodological considerations in analyzing twitter
data. JNCI Monographs 2013(47), 140–146.
Link BG and Phelan JC (2006) Stigma and its public health implications. The Lancet 367(9509), 528–529.
Loper E and Bird S (2002) Nltk: The Natural Language Toolkit. arXiv preprint cs/0205028.
McCormick TH, Lee H, Cesare N, Shojaie A and Spiro ES (2017) Using twitter for demographic and social science research:
Tools for data collection and processing. Sociological Methods & Research 46(3), 390–421.
Mohammad SM, Salameh M and Kiritchenko S (2016) How translation alters sentiment. Journal of Artificial Intelligence
Research 55, 95–130.
Moor N (2020) ldatuning: LDA Models Parameters Tuning. Available at https://github.com/nikita-moor/ldatuning (accessed
9 December 2020).
Nature (2020) Stop the coronavirus stigma now. Nature 580(7802), 165.
NegaraES,Triadi D andAndryani R (2019) Topicmodelling twitter datawith latent dirichlet allocationmethod. In ICECOS 2019
- 3rd International Conference on Electrical Engineering and Computer Science, Proceeding. Batam: Institute of Electrical and
Electronics Engineers, pp. 386–390.
NLTK (2020) nltk.stem package. Available at http://www.nltk.org/api/nltk.stem.html?highlight=wordnetlemmatizer (accessed
10 December 2020).
Ochieng P (2009) An analysis of the strengths and limitation of qualitative and quantitative research paradigms. Problems of
Education in the 21st Century 13, 13.
ODI (2020) Key Workers: Migrants’ Contribution to the COVID-19 Response. Overseas Development Institute. Available at
https://www.odi.org/migrant-key-workers-covid-19/ (accessed 10 December 2020).
OECD (2020) What is the impact of the COVID-19 pandemic on immigrants and their children? In OECD Policy Responses to
Coronavirus (COVID-19). Paris: OECD, pp. 1–26.
Olzak S (1994) The Dynamics of Ethnic Competition and Conflict. Stanford: Stanford University Press.
Ostrowski DA (2015) Using latent Dirichlet allocation for topic modelling in twitter. In Proceedings of the 2015 IEEE 9th
International Conference on Semantic Computing, IEEE ICSC 2015. Anaheim, CA: Institute of Electrical and Electronics
Engineers, pp. 493–497.
Peisenieks J and Skadins R (2014) Uses of machine translation in the sentiment analysis of tweets. Frontiers in Artificial
Intelligence and Applications 268, 126–131.
Perkins J (2010) Python Text Processing with NLTK 2.0 Cookbook, Number 1. Birmingham: Packt.
Rankin J (2020) Migration: EU Praises Greece as ‘Shield’ after Turkey Opens Border. Available at https://www.theguardian.com/
world/2020/mar/03/migration-eu-praises-greece-as-shield-after-turkey-opens-border (accessed 10 December 2020).
Ribeiro M, Calais P, Santos Y, Almeida V and Meira Jr W (2018) Characterizing and detecting hateful users on twitter. In
Proceedings of the International AAAI Conference on Web and Social Media, vol. 12. Menlo Park, CA: Association for the
Advancement of Artificial Intelligence.
Righi A (2019) Assessing migration through social media: A review. Mathematical Population Studies 26(2), 80–91.
Robinson M (2020) Donald Trump Announces Plan to Suspend Immigration to US. Available at https://www.thetimes.co.uk/
article/donald-trump-announces-plan-to-suspend-immigration-to-us-t6km6vzn3 (accessed 10 December 2020).
Rosa H, Pereira N, Ribeiro R, Ferreira PC, Carvalho JP,Oliveira S, Coheur L, Paulino P, Simão AVand Trancoso I (2019)
Automatic cyberbullying detection: A systematic review. Computers in Human Behavior 93, 333–345.
Rowe F (2021) Big data. In Concise Encyclopaedia of Human Geography, Elgar Encyclopedias in the Social Sciences Series.
Rowe F,MahonyM,Graells-Garrido E,RangoM and Sievers N (2021a) Using twitter Data toMonitor Immigration Sentiment.
arXiv preprint arXiv:1911.03795.
Rowe F,MahonyM,Graells-Garrido E,RangoM and Sievers N (2021b) Using twitter to Track Immigration Sentiment During
Early Stages of the Covid-19 Pandemic. SocArXiv.
Rowe F, Mahony M, Sievers N, Rango M, and Graells-Garrido E. (2021c) Sentiment towards Migration during COVID-19:
What Twitter Data Can Tell Us. International Organization for Migration, Geneva, Switzerland.
Sanguinetti M, Poletto F,Bosco C, Patti Vand Stranisci M (2018) An Italian twitter corpus of hate speech against immigrants. In
Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). Miyazaki: ELRA.
Schwartz C, Simon M,Hudson D and Van-Heerde-Hudson J (2020) A populist paradox? How Brexit softened anti-immigrant
attitudes. British Journal of Political Science 51, 1–21.
Data & Policy e36-21
#
#Sehl K (2020) Top Twitter Demographics That Matter to Social Media Marketers. Available at https://blog.hootsuite.com/twitterdemographics/
(accessed 10 December 2020).
Shalunts G,Backfried G andCommeignes N (2016) The impact of machine translation on sentiment analysis.Data Analytics 63,
51–56.
Sloan L (2017) Who tweets in the United Kingdom? Profiling the twitter population using the British social attitudes survey 2015.
Social Media þ Society 3(1), 205630511769898.
Statista (2020) Leading Countries Based on Number of Twitter Users as of October 2020. Available at https://www.statista.com/
statistics/242606/number-of-active-twitter-users-in-selected-countries/ (accessed 9 December 2020).
Tajfel H (1982) Social psychology of intergroup relations. Annual Review of Psychology 33(1), 1–39.
Twitter (2020) Premium and Enterprise Operators | Guides | Search Tweets. Available at https://developer.twitter.com/en/docs/
twitter-api/v1/tweets/search/guides/premium-operators (accessed 9 December 2020).
Ueffing P,Rowe F andMulder CH (2015) Differences in attitudes towards immigration between Australia and Germany: The role
of immigration policy. Comparative Population Studies 40(4), 437–465.
UN Global Pulse (2017) Social Media and Forced Displacement: Big Data Analytics & Machine-Learning. Technical Report.
New York: UNHCR Innovation Service.
United Nations (2020) International Migration 2020 Highlights. United Nations Department of Economic and Social Affairs,
Population Division, (ST/ESA/SER.A/452).
Vosoughi S, Roy D and Aral S (2018) The spread of true and false news online. Science 359(6380), 1146–1151.
WangW,FengYandDaiW (2018) Topic analysis of online reviews for two competitive products using latent Dirichlet allocation.
Electronic Commerce Research and Applications 29, 142–156.
Wikipedia (2020) List of Incidents of Xenophobia and Racism Related to the COVID-19 Pandemic. Available at https://
en.wikipedia.org/wiki/List_of_incidents_of_xenophobia_and_racism_related_to_the_COVID-19_pandemic (accessed
9 December 2020).
Zagheni E,Garimella VRK,Weber I and State B (2014) Inferring international and internal migration patterns from Twitter data.
InWWW 2014 Companion - Proceedings of the 23rd International Conference on World Wide Web. New York: Association for
Computing Machinery, pp. 439–444.
Cite this article: Rowe F, Mahony M, Graells-Garrido E, Rango M and Sievers N (2021). Using Twitter to track immigration
sentiment during early stages of the COVID-19 pandemic. Data & Policy, 3: e36. doi:10.1017/dap.2021.38
e36-22 Francisco Rowe et al.
ARTICLE
Summary Judicial Proceedings as a Measure
for Electoral Disinformation: Defining the
European Standard
Adam Krzywoń*
Faculty of Law and Administration, University of Warsaw, Warsaw, Poland and German Research Institute for Public
Administration, Speyer, Germany
*Corresponding author: adam.krzywon@gmail.com
(Received 21 July 2020; accepted 19 October 2020)
Abstract
Electoral disinformation has become one of the most challenging problems for democratic states. All of them
are facing the phenomenon of - both online and offline - dissemination of false information during preelectoral
period, which is harmful for individual and collective rights. As a consequence, some European
countries adopted special measures, including summary judicial proceedings in order to declare that information
or materials used in electioneering are false and to prohibit its further dissemination. There are already
three rulings of the ECtHR concerning this expeditious judicial examination provided in the Polish law. In
December 2018 France passed complex regulation against manipulation of information that include similar
mechanisms. This article, basing on the ECtHR’s case law and some national experiences, attempts to define
the minimal European standard for measures targeted at electoral disinformation, especially judicial summary
proceeding. It contains the analysis of the notion of electoral disinformation, defines the state’s positive
obligations in this sphere, and indicates mayor challenges for the legal framework. The principal argument
is that summary judicial proceedings – if adequately designed – cannot be questioned from the Convention
standpoint and provide a partial solution to the problem of electoral disinformation.
Keywords: Electoral disinformation; summary judicial proceedings; European Court of Human Rights; freedom of expression;
free elections
A. Introduction
The phenomenon of electoral disinformation has become one of the most challenging and
preoccupying problems for policymakers, courts, and legal scholars. As the manipulation and dissemination
of false information online plays an important role in many elections,1 some countries
© The Author(s) 2021. Published by Cambridge University Press on behalf of the German Law Journal. This is an Open Access article,
distributed under the terms of the Creative Commons Attribution licence (http://creativecommons.org/licenses/by/4.0/), which permits
unrestricted re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
1See Sanja Kelly, Mai Truong, Adrian Shahbaz, Madeline Earp & Jessica White, Freedom on the Net 2017: Manipulating
Social Media to Undermine Democracy, FREEDOM HOUSE (2017) https://freedomhouse.org/report/freedom-net/2017/
manipulating-social-media-undermine-democracy. One of the first cases when the global public faced the enormous scale
of online disinformation was the US presidential election in 2016. See Hunt Allcott & Matthew Gentzkow, Social Media
and Fake News in the 2016 Election, 31 J. ECON. PERSP. 211 (2017). See also KATHLEEN HALL JAMIESON, CYBERWAR:
HOW RUSSIAN HACKERS AND TROLLS HELPED ELECT A PRESIDENT: WHAT WE DON’T, CAN’T, AND DO KNOW (2018).
Other significant examples of this phenomenon were: The 2018 United States midterm elections, the 2016 Brexit referendum,
the 2017 French presidential elections, the 2017 German federal elections, the 2018 Italian general elections, and the 2018
Brazilian presidential elections. See Disinformation and Propaganda—Impact on the Functioning of the Rule of Law in
the EU and its Member States, EUR. PARL. DOC. (PE 608.864) 39–50 (2019), https://www.europarl.europa.eu/RegData/
etudes/STUD/2019/608864/IPOL_STU(2019)608864_EN.pdf.
German Law Journal (2021), 22, pp. 673–688
doi:10.1017/glj.2021.23
#
#have adopted special measures that would limit the disruptive effect of these practices on democratic
processes and institutions. Until now, perhaps the most advanced and complex legislation
has been passed in France, where in December 2018 a law against the manipulation of information
was issued.2 The French law has obviously gained worldwide attention as it responds to the weaponization
of social media and its ability to create a virtual electoral reality. The measures adopted in
the French law, including the possibility to issue a judicially expeditious ban on the
dissemination of online disinformation and disclosure obligations, reflect the powerful nature of
the internet and new media when used as tools to generate false stories, which then enter the public
domain under the guise of legitimacy. That said, it is worth noting that some of these mechanisms
have also been present in the electoral law of other members of the Council of Europe.
In Poland, for example, a special twenty-four-hour summary judicial proceeding was introduced
in 1998. Although it has been successfully used to declare that information or materials
used in electioneering, both online and offline, are false, it has not attracted much attention
and is generally absent from the current legal scholarship and international reports on electoral
disinformation. The implications of the Polish law become even more complex when we consider
that in mid-2019, the European Court of Human Rights (“ECtHR” or “Court”) ruled that Poland
had violated the European Convention on Human Rights (“ECHR” or “Convention”) for
convicting an applicant in one of these extraordinary twenty-four-hour judicial proceedings.3
This decision marked the Court’s third ruling finding a breach of the right to freedom of expression
(Article 10 ECHR) because the decisions issued in the expeditious judicial examination
amounted to a disproportionate interference and were not necessary in a democratic society.4
In this respect, these rulings, together with current scholarship and soft-law instruments of the
Council of Europe, may be used as a basis upon which to reconstruct the minimal European positive
standard for measures targeted at electoral disinformation, especially summary judicial pro-
ceedings. To realize this main aim of the present study, it is necessary to analyze the notion of
electoral disinformation and then the judicial practice of the ECtHR concerning freedom of
expression, right to free elections, and these extraordinary twenty-four-hour electoral proceedings.
Moreover, national regulatory instruments, such as the Polish and French laws, seem to be
relevant. The principal argument of this Article is that summary judicial proceedings—if adequately
designed—cannot be questioned from the Convention standpoint. Judicial intervention can provide
a partial solution to the problem of both online and offline electoral disinformation. Nonetheless,
judicial summary proceedings are not a remedy for all types of electoral abuses.
In the first section, this Article sets out how to analyze the concept of disinformation and
related terminology; how these phenomena can weaken the power of constitutional states and
undermine the protection of fundamental rights; and how to identify the reaction of the
international community. This Article’s second section seeks to present the European protection
of the freedom of expression in the electoral context, including the state’s positive obligations
towards disinformation. Finally, the third section is dedicated to a comparative look at some
2Loi 2018-1202 du 22 décembre 2018 relative à la lutte contre la manipulation de l'information [Law 2018-1202 of
December 22, 2018 on the fight against the manipulation of information], JOURNAL OFFICIEL DE LA RÉPUBLIQUE
Francaise [J.O.] [OFFICIAL GAZETTE OF FRANCE], Dec. 23, 2018.
3Brzeziński v. Poland, App. No. 47542/07 (July 25, 2019), http://hudoc.echr.coe.int/eng?i=001-194958.
4See Kwiecień v. Poland, App. No. 51744/99 (Jan. 9, 2007), http://hudoc.echr.coe.int/eng?i=001-78876; Kita v. Poland, App.
No. 57659/00 (July 8, 2008), http://hudoc.echr.coe.int/eng?i=001-87424. It is necessary to also mention the judgment Jezior v.
Poland, App. No. 31955/11 (June 4, 2020), http://hudoc.echr.coe.int/eng?i=001-202614, in which the applicant was convicted in
summary judicial proceedings. The Court found that it amounted to a disproportionate interference with the freedom of
expression and was not necessary in a democratic society. Nonetheless, in this case, the main issue was the liability of the person
who ran a blog for the content posted by third parties, not the nature of the electoral proceedings. The Court’s conclusion was that
holding him liable for false anonymous comments about the candidate in local elections was not compatible with Article 10 of the
ECHR, as he was only an intermediary and had put in place adequate measures in order to detect potentially illegal content.
674 Adam Krzywoń
#
#national legislative responses within the member states of the Council of Europe and includes an
analysis of recent ECtHR case law concerning summary judicial proceedings against electoral disinformation
in Poland. That section also determines the minimal European standard for these
proceedings and explains how they should be configured in light of the Convention.
B. What is Electoral Disinformation?
I. The All-Encompassing Term “Fake News” and its Misuse
Where democracy and fundamental rights are concerned, it is crucial to first clearly define and
employ precise terms when evaluating a state’s positive actions and when seeking to adopt
appropriate legal measures.5 Moreover, in Brzeziński v. Poland, the ECtHR referred for the very
first time to the notion of “fake news.” The Court noted that the summary nature of the judicial
electoral proceedings established by Polish statutory law is justified by the need to ensure that
“fake news” and statements that harm the reputation of the candidates, as well as those which
are likely to distort the result of elections, are rectified as soon as possible.6
The Court’s determination that the spread of false information in electoral processes can be
considered detrimental to democracy is a positive development. That said, one might anticipate that
a professional body such as the ECtHR would employ a more sophisticated and clear terminology. It
has to be kept in mind, however, that both legal scholarship as well as international organizations
reject the term “fake news” as being too vague and inadequate for describing the complex phenomena
of untruthful information campaigns. “Fake news” is not a homogenous concept and it can
manifest in different ways.7 It encompasses a broad spectrum of information types, from low-risk
(honest mistakes or inadvertent sharing of false information) to high-risk (malicious fabrications,
including textual content and convincing fake audio and/or video content, so called “deep fakes”
produced by artificial intelligence or even simple programs that allow users to superimpose images
and videos onto unrelated images or videos). Moreover, some politicians have appropriated this
broad notion in order to dismiss independent media coverage that they find disagreeable.8
In this respect, in both legal scholarship and case law it is preferable to use the term “disinformation”
in order to describe false, inaccurate, or misleading information that is designed,
presented, and promoted to intentionally cause public harm.9 Such information can be understood
as being created and distributed with the express intent to threaten and undermine the
democratic political processes and rights of others, and possessing a clear political/ideological
and/or economic goal. This phenomenon is not restricted in its scope, though it has been found
to intensify both before and during democratic decision-making processes.10 Disinformation
should also be distinguished from misinformation, which is an accidental sharing of false
information, and could be the result of an honest mistake or negligence.11 Disinformation
obviously does not include reporting errors or weak reasoning in political or electoral debate.
The element of intentional harm is crucial in the definition of disinformation. It is decisive as
far as interference in the freedom of expression (Article 10.2 ECHR) and positive state actions are
5Fernando Nuñez, Disinformation Legislation and Freedom of Expression, 10 U.C. IRVINE L. REV. 783, 785 (2020).
6Brzeziński, App. No. 47542/07 at para. 35.
7Irini Katsirea, “Fake News:” Reconsidering the Value of Untruthful Expression in the Face of Regulatory Uncertainty, 10 J.
MEDIA L. 159, 166 (2018).
8EUR. COMM’N, A Multi-Dimensional Approach to Disinformation: Report of the Independent High Level Group on Fake
News and Online Disinformation, (2018), https://ec.europa.eu/digital-single-market/en/news/final-report-high-level-expertgroup-fake-news-and-online-disinformation.
9Claire Wardle & Hossein Derakhshan, Information Disorder: Toward an Interdisciplinary Framework for Research and
Policymaking, COUNCIL OF EUROPE (2017) https://rm.coe.int/information-disorder-toward-an-interdisciplinary-frameworkfor-researc/168076277c;
EUR. PARL., supra note 1, at 131.
10JEAN-BAPTISTE JEANGENE VILMER, ALEXANDRE ESCORCIA, MARINE GUILLAUME & JANAINA HERRERA, INFORMATION
MANIPULATION: A CHALLENGE FOR OUR DEMOCRACIES 39 (2018).
11Don Fallis, What is Disinformation? 63 LIBR. TRENDS 401, 402 (2015).
German Law Journal 675
#
#concerned. From a Convention point of view, prohibiting, prosecuting, and criminalizing information
solely based on its lack of truthfulness seems to be highly problematic.12 False information
is not illegal per se; it must also be accompanied by individual or collective damage caused by its
production and dissemination. Article 10 of the ECHR does not prohibit discussion or further
distribution of information received even if it is strongly suspected that this information might
not be truthful. To suggest otherwise, as confirmed by the Court, would be to deprive persons of
the right to express their views and opinions about statements made in the mass media and would
thus place an unreasonable restriction on the freedom of expression.13 The best illustration of this
doctrine is the ECtHR permitting the criminalization of falsifying history but only under the condition
that this kind of expression constitutes an incitement to hatred or intolerance, or if there is
evidence of a serious threat to public order.14
From the national perspective, it is clear that interference with freedom of expression does not
necessarily entirely depend on the falsity or truthfulness of the information, but rather on the
harm that it could cause (see, for example, Spanish constitutional doctrine15 and constitutional
French case law16). As a recent Organization for Security and Co-Operation in Europe (“OSCE”)
report demonstrates,17 existing national criminal laws ban the fabrication and spreading of false
statements, by any means, if they endanger the legitimate and protected interests of individuals,
the public, and the state.
II. The Harmful Effect of False Electoral Information
Against this backdrop, it is clear that intentionally false information, spread before elections, may
be considered as having a disruptive or harmful effect on collective or individual interests. Further,
a mass proliferation of disinformation could threaten a state’s sovereignty and its political independence
and undermine the integrity and fairness of elections. As this phenomenon has no
territorial boundaries, harm can be perpetrated both by internal and external actors. If the latter
become significantly active in obstructing and influencing national electoral processes, this raises
questions of whether such actions can be treated as undermining a nation’s sovereignty,18 and
what the proper response by international law should be.19 It is also important to note that
the purpose of external interference may not necessarily be to damage or otherwise incapacitate
the nation’s electoral infrastructure, but rather to gather information that can be used as a part of a
larger, more comprehensive, or ongoing disinformation campaign designed to undermine the
public’s trust in democratic and governmental institutions.20
For the individual citizen, there are two negative aspects of electoral disinformation. First, from
the voter’s point of view, such intrusions risk distorting the process by which free opinions are
12See Katsirea, supra note 7, at 171–79. See also Human Rights Committee, General Comment No. 34, U.N. Doc. GE.1145331,
at para 47 (2011) (indicating that as far as comments about public figures are concerned, they should be avoided from
rendering unlawful untrue statements that have been published in error but without malice).
13Salov v. Ukraine, App. No. 65518/01, para. 113 (Sept. 6, 2005), http://hudoc.echr.coe.int/eng?i=001-70096.
14See, e.g., Perinçek v. Switzerland, App. No. 27510/08 (Oct. 15, 2015), http://hudoc.echr.coe.int/eng?i=001-158235.
15Cristina P. Chulvi, Noticias Falsas y Libertad de Expresión e Información: El Control de los Contenidos Informativos en la
Red, 41 TEORIA Y REALIDAD CONSTITUCIONAL 297, 310–13 (2018).
16See Conseil constitutionnel [CC] [Constitutional Court] decision no. 2018-773 DC, Dec. 20, 2018 (Fr.). Measures against
disinformation are legitimate if they protect the credibility of elections. For the analysis of the judgment, see Rachael Craufurd
Smith, Fake News, French Law and Democratic Legitimacy: Lessons for the United Kingdom? 11 J. MEDIA L. 52, 63–76 (2019).
17Andrey Rikhter, International Standards and Comparative National Approaches to Countering Disinformation in the
Context of Freedom of the Media, ORG. FOR SEC. & COOP. IN EUR. 27–42 (July 1, 2019), https://www.osce.org/files/f/
documents/2/1/424451.pdf.
18Ashley C. Nicolas, Taming the Trolls: The Need for an International Legal Framework to Regulate State Use of
Disinformation on Social Media, 107 GERMAN L. J. 36, 37 (2018).
19Manuel Rodriguez, Disinformation Operations Aimed at (Democratic) Elections in the Context of Public International
Law: The Conduct of the Internet Research Agency During the 2016 US Presidential Election, 47 INT’L J. LEGAL INFO. 149 (2019).
20Drew Marvel, Protecting the States from Electoral Invasions, 28 WM. & MARY BILL RTS. J. 197, 198 (2019).
676 Adam Krzywoń
#
#formed, and therefore can have a negative effect on the manner through which citizens elect their
representatives. In a democratic state, there is diversity and contradiction in public debate and
participants are entitled to access to contradictory views and to be informed of differing perspectives.21
Participants are capable of, and willing to, distinguish truth from falsehood, as well as able
to compare conflicting ideas.22 Second, there is the impact on candidates running for election.
Disinformation can negatively influence their reputation, a value protected by Article 8
ECHR.23 Significant false allegations before elections can have far-reaching effects, cause irreparable
damage, lower public esteem, and deprive a person of necessary public trust. Another value
that is threatened is the equality of opportunities, which is crucial in all electoral processes and
protected by the Convention.24 Democratic systems demand fair electoral competition as well as
the removal of all unfair barriers to the electioneering process. Uncontrolled dissemination of
disinformation adversely affects this balance.
III. Disinformation, Internet-Based Communication, and Populism
A further important characteristic of currently observed electoral disinformation is the speed of its
dissemination, thanks to the use of the internet and social media—digital amplificationmechanisms.
False stories can easily enter the public domain and have the appearance of legitimacy, in an unprecedentedmanner,
without any checks on their accuracy. The fabrication and spread of disinformation
in the digital age is straightforward and the cost is negligible. Those producing it do not have to
undertake cross-referencing or fact-checking and there are no investigative or verification burdens
like there are for professional news agencies.25 Moreover, the very nature of this newmedia enables it
to be used for the purposes of political disinformation, polarization, and societal fragmentation.
Personal algorithms, based on a user’s previous behavior and preferences, “know a lot.” These tools
are able to customize information for the distribution of polarizing and false political content. Echo
chambers and information cocoons are being created and like-minded people speak only amongst
themselves.26 These voters have neither the need, nor the instruments, to verify the truthfulness of
the electoral information, and this is what makes it so easy to affect their political choices.
In this respect, it seems that the ECtHR is conscious that internet-based communication
implies structural differences not present in regular media, and this also has an important influence
on the electoral context. Rules governing traditional media cannot be automatically applied
to new media. As indicated in Editorial Board of Pravoye Delo & Shtekel v. Ukraine:
[T]he Internet is an information and communication tool particularly distinct from
the printed media, especially as regards the capacity to store and transmit information.
The electronic network, serving billions of users worldwide, is not and potentially will never
be subject to the same regulations and control. The risk of harm posed by content and communications
on the Internet to the . . . human rights and freedoms . . . is certainly higher than
that posed by the press.27
21Alain Zysset, Freedom of Expression, the Right to Vote, and the Proportionality at the European Court of Human Rights: An
Internal Critique, 17 INT’L J. CONST. L. 230, 236 (2019).
22Ari E. Waldman, The Marketplace of Fake News, 20 U. PA. J. CONST. L. 845, 848, 869 (2018).
23See, e.g., Chauvy & Others v. France, App. No. 64915/01 (June 29, 2004), http://hudoc.echr.coe.int/eng?i=001-61861.
24See, e.g., The Communist Party of Russia & Others v. Russia, App. No. 29400/05, para. 108 (June 19, 2012), http://hudoc.
echr.coe.int/eng?i=001-111522.
25Nuñez, supra note 5, at 787.
26CASS R. SUNSTEIN, #REPUBLIC: DIVIDED DEMOCRACY IN THE AGE OF SOCIAL MEDIA 13–16 (2017).
27Ed. Bd. of Pravoye Delo & Shtekel v. Ukraine, App. No. 33014/05, para. 63 (May 5, 2011), http://hudoc.echr.coe.int/eng?
i=001-104685. See also Węgrzynowski & Smolczewski v. Poland, App. No. 33846/07, para. 98 (July 16, 2013), http://hudoc.
echr.coe.int/eng?i=001-122365.
German Law Journal 677
#
#Similarly, the Court emphasizes the far-reaching harmful effects of hate speech when posted on
somebody’s Facebook profile.28
The last factor—of a political nature—that empowers electoral disinformation is the rising tide
of populism. This is particularly important in the Central and Eastern European context, although
no democracies are immune to this phenomenon.29 Populist rhetoric and disinformation are both
highly strategic communicative approaches. Populist politicians present emotional and personal
sources of truth as superior to the knowledge gained from science, academic inquiry, or discussion.30
According to such populist logic, “the remote elites”—not the “ordinary people”—use sci-
ence, statistics, and scholarship to hold on to power and preserve their hegemonic position. On the
one hand, it may be justly maintained that social media has given a voice to underprivileged
groups within society, resulting in greater democratization. But, on the other hand, it has also
created opportunities for populist politicians: They are able to use social media to share manipulative
content and fulfill particular political interests.31
IV. The Reliability of Electoral Information and International Soft-Law Standards
The harmful effects of the described phenomena make clear why a number of countries and
international organizations are concerned with the purity, authenticity, and truthfulness of
information disseminated in the media during the pre-electoral decision-making period. States
should adopt a proactive approach, because the absence of an adequate legal response towards
disinformation could lead to serious consequences, resulting in a loss of public confidence in
democratic procedures—the notion of contentious elections.32 When human rights and
democratic values are at stake, the law cannot lag behind technology.33
The state’s obligations to adopt legal measures against electoral disinformation have been
highlighted in some international soft-law documents. A quick overview of them is useful with
regard to the aim of this Article. Recommendations from various international organizations
demonstrate how to adequately design laws concerning both false electoral information and summary
judicial proceedings.
The European Union (“EU”) adopted various soft law instruments in the field34 and commissioned
a comprehensive study concerning disinformation.35 The authors of the latter recommended
taking positive measures by regulating both online political advertising and micro-targeting.
28Beizaras & Levickas v. Lithuania, App. No. 41288/15, para. 127 (Jan. 14, 2020), http://hudoc.echr.coe.int/eng?i=001200344.
See also Fatullayev v. Azerbaijan, App. No. 40984/07, para. 95 (Apr. 22, 2010), http://hudoc.echr.coe.int/eng?i=
001-98401 (pointing out that the internet is a medium which in modern times has no less powerful effect than print media).
29See, e.g., Matteo Monti, Italian Populism and Fake News on the Internet: A New Political Weapon in the Public Discourse,
in ITALIAN POPULISM AND CONSTITUTIONAL LAW: CHALLENGES TO DEMOCRACY IN THE 21ST CENTURY 177–97 (Giacomo
Delledonne et al. eds., 2020) (describing the Italian experience of the spread of false information in the digital media by
populist movements).
30JAMES BALL, POST-TRUTH: HOW BULLSHIT CONQUERED THE WORLD 4 (2017).
31Sven Engesser, Nayla Fawzi & Anders Olof Larsson, Populist Online Communication: Introduction to the Special Issue, 20
INFO., COMMC’N & SOC’Y 1279, 1280 (2017).
32See PIPPA NORRIS, RICHARD W. FRANK & FERRAN MARTINEZ I COMA, CONTENTIOUS ELECTIONS: FROM BALLOTS TO
BARRICADES 1–22 (2015).
33JAN VAN DIJK, THE NETWORK SOCIETY 128 (2006); Molly Land, Toward an International Law of the Internet, HARV. INT’L
L.J. 393, 456 (2013).
34EUR. COMM’N, supra note 8; Communication from the Commission to the European Parliament, the Council, the European
Economic and Social Committee and the Committee of the Regions: Tackling Illegal Content Online: Towards an Enhanced
Responsibility of Online Platforms, COM (2017) 555 final (Sept. 28, 2017); Communication from the Commission to the
European Parliament, the Council, the European Economic and Social Committee and the Committee of the Regions:
Tackling online disinformation: a European Approach, COM (2018) 236 final (Apr. 26, 2018); Resolution on online platforms
and the digital single market, EUR. PARL. DOC. 2016/2276(INI) (2017).
35EUR. PARL., supra note 1.
678 Adam Krzywoń
#
#Similarly, the Council of Europe has attempted to tackle these phenomena, not only by defining
disinformation, but also by proposing a framework for research and policymaking.36 The issue of
the internet intermediaries’ responsibilities has been raised.37 The Parliamentary Assembly of the
Council of Europe has called upon states to consider actions concerning the accountability of
internet operators38 and has suggested that all third-party content posted on professional media
websites should fall under the editorial responsibility of these media.39 The Venice Commission
has also issued an exhaustive report on digital technologies and elections, indicating that states have
a positive responsibility to prevent undue interference with civil and political rights by third parties.40
C. Freedom of Electoral Expression in Europe
I. Interrelations of Freedom of Expression and Free Elections
Having examined the notion of disinformation, its negative influence on democratic values, and the
related international framework, it is crucial to analyze the general margin of appreciation conferred
on national authorities by the ECtHR as far as freedom of expression in the electoral context is concerned.
The aforementioned phenomena undoubtedly affect both the right to freedom of expression
(Article 10 ECHR) and the right to free elections (Article 3 of Protocol No. 1 to ECHR; “P1-3”).
Determining the necessary balance between those two interrelated rights makes it possible to answer
the question of how summary judicial electoral proceedings should be configured.
The guarantees arising from both Article 10 and P1-3 are crucial to establishing and maintaining
the foundations of an effective and meaningful democracy governed by the rule of law,
although they are subject to different interpretative methods adopted by the ECtHR: More activism
and a teleological approach towards Article 10 versus restraint and a consensus-based
approach with regard to P1-3.41 There is undoubtedly a link between these provisions, namely
the need to guarantee respect for pluralism of opinion in a democratic society through the exercise
of civic and political freedoms.42 The state’s obligation expressed in P1-3 to create “conditions
which will ensure the free expression of the opinion of the people in the choice of the legislature”
essentially implies the freedom of expression. The latter has become one of the essential prerequisites
for the guarantees established in P1-3.43 It is particularly important in the period preceding
36Wardle & Derakhshan, supra note 9.
37Committee of Ministers, Recommendation CM/Rec(2018)2 of the Committee of Ministers to Member States on the Roles
and Responsibilities of Internet Intermediaries, COUNCIL OF EUROPE (Mar. 7, 2018), https://search.coe.int/cm/Pages/result_
details.aspx?ObjectID=0900001680790e14.
38Parliamentary Assembly, Resolution 1970 (2014) Internet and Politics: The Impact of New Information and
Communication Technology on Democracy, COUNCIL OF EUROPE (Jan. 29, 2014), https://assembly.coe.int/nw/xml/XRef/
Xref-XML2HTML-en.asp?fileid=20447&lang=en.
39Parliamentary Assembly, Resolution 2143 (2017) Online Media and Journalism: Challenges and Accountability, COUNCIL
OF EUROPE (Jan. 25, 2017), https://assembly.coe.int/nw/xml/XRef/Xref-XML2HTML-en.asp?fileid=23455&lang=en.
40Venice Commission, Op. no. 925/2018, Joint Report of the Venice Commission and of the Directorate of Information
Society and Action Against Crime of the Directorate General of Human Rights and Rule of Law (DGI) on Digital
Technologies and Elections, COUNCIL OF EUROPE (June 24, 2019), https://www.venice.coe.int/webforms/documents/?pdf=
CDL-AD(2019)016-e. Another valuable initiative has been undertaken by the Organization of American States (“OAS”)
Special Rapporteur on Freedom of Expression, that recognizes the legitimate concern, and the importance of adopting proportional
measures against electoral disinformation. The response must not, however, be the establishment of new criminal types
to sanction the dissemination of false news, but positive actions aimed at raising public awareness about the existence of
disinformation. One of the recommendations is to strengthen judicial proceedings of a civil nature, by making them more
efficient and timely, which can become an effective tool to combat disinformation that affects officials and candidates for
public office. See Guide to Guarantee Freedom of Expression Regarding Deliberate Disinformation in Electoral Contexts,
Inter-Am. Comm’n H.R., Office of the Special Rapporteur on Freedom of Expression (Oct. 2019), http://www.oas.org/en/
iachr/expression/publications/Guia_Desinformacion_VF%20ENG.pdf.
41Zysset, supra note 21, at 232.
42Ždanoka v. Latvia, App. No. 58278/00, para. 115 (Mar. 16, 2006), http://hudoc.echr.coe.int/eng?i=001-72794.
43Mathieu-Mohin & Clerfayt v. Belgium, App. No. 9267/81, para. 54 (Mar 2, 1987), http://hudoc.echr.coe.int/eng?i=00157536.
German Law Journal 679
#
#an election that opinions and information of all kinds—not only that which is favorable, but also
that which shocks, offends, and disturbs—are permitted to circulate freely. In the context of
electoral debates, the unhindered exercise of freedom of speech by candidates has particular
significance.44 There is also a strong legitimate public interest to be informed about serious
allegations concerning electoral candidates.45
The P1-3 is the best example of the ECHR as a living instrument. Initially, its provisions were
considered only as an institutional guarantee, but finally it has developed into a fully enforceable
individual right.46 The scope of application is also variable, as the Court applies it not only to
parliamentary elections.47 A wide margin of appreciation has been granted to the national authorities
as far as the configuration of the electoral system is concerned. With regard to the latter, there
are numerous ways of organizing and running electoral systems, and a wealth of differences,
inter alia, in historical background, cultural diversity, and political thought within Europe.48
Moreover, concerning P1-3, the primary obligation is the adoption by the state of positive measures
to hold democratic elections.49 Unlike Article 10, which principally obliges states to practice
abstention and non-interference, in the light of the right to free elections, public authorities should
take active steps to organize and efficiently carry out voting so that all ballots are free of any pressure.
The states enjoy a margin of appreciation not only when determining the institutional
dimension of the electoral system, but also when regulating certain aspects of the freedom of
expression in the electoral context. The Court has indeed authorized the adoption of different
models of electioneering at the national level, in particular by establishing rules for electoral advertising
in the media, as a common approach to this issue has not yet been developed. In this field, a
“somewhat wider margin of appreciation than that normally afforded to restrictions on expression
on matters of public interest” is allowed.50
As far as Article 10 is concerned, the margin of appreciation is quite heterogeneous, and
depends on a number of factors—the author, content, form of expression—and their interrelationship.
Although, generally speaking, in the case of political expression or debate on a matter of
general public interest, the scope of national discretion is rather limited.51 The specific circumstances
of a case would therefore indicate which margin of appreciation should be accorded:
A broad one, related to the institutional electoral system and certain aspects of electioneering such
as electoral advertising, or a narrow one, deduced from the privileged position of the freedom of
political expression under Article 10. This is a consequence of the aforementioned distinction
made in the Court’s case law. On the one hand, the ECtHR emphasizes that in the period preceding
or during elections, it is legitimate to place certain restrictions, of a type which would not
usually be acceptable, on freedom of expression.52 Interference with this freedom may be necessary
to guarantee pluralism, the proper conduct of public debate, and fair competition between the
44Orlovskaya Iskra v. Russia, App. No. 42911/08, para. 110 (Feb. 21, 2017), http://hudoc.echr.coe.int/eng?i=001-171525.
45Ólafsson v. Iceland, App. No. 58493/13, para. 50 (Mar. 16, 2017), http://hudoc.echr.coe.int/eng?i=001-171974.
46JAVIER GARCIA ROCA, LA TRANSFORMACION CONSTITUCIONAL DEL CONVENIO EUROPEO DE DERECHOS HUMANOS 79–80
(2019).
47The ECtHR recently mentioned the implications of P1-3 in the context of a national referendum. See Magyar Kétfarkú
Kutya Párt v. Hungary, App. No. 201/17, para. 99 (Jan. 20, 2020), http://hudoc.echr.coe.int/eng?i=001-200657.
48Sitaropoulos & Giakoumopoulos v. Greece, App. No. 42202/07, paras. 65–66 (Mar. 15, 2012), http://hudoc.echr.coe.int/
eng?i=001-109579; Hirst v. United Kingdom, App. No. 74025/01, para. 61 (Oct. 6, 2005), http://hudoc.echr.coe.int/eng?i=
001-70442.
49Mathieu-Mohin & Clerfayt v. Belgium, App. No. 9267/81, para. 50 (Mar. 2, 1987), http://hudoc.echr.coe.int/eng?i=00157536.
50Animal Defenders International v. United Kingdom, App. No. 48876/08, para. 123 (Apr. 22, 2013), http://hudoc.echr.coe.
int/eng?i=001-119244.
51According to ECtHR case law, all opinions and information disseminated during an electoral period should be treated as
part of the debate on matters of public interest. See Filatenko v. Russia, App. No. 73219/01, para. 40 (Dec. 6, 2007), http://
hudoc.echr.coe.int/eng?i=001-83830.
52Bowman v. United Kingdom, App. No. 24839/94, para. 43 (Feb. 19, 1998), http://hudoc.echr.coe.int/eng?i=001-58134.
680 Adam Krzywoń
#
#candidates.53 On the other hand, the Court points out that the adoption of the wider margin of appreciation
under P1-3, when establishing the rules of electioneering, always entails the risk that it could
lead to results incompatible with the privileged position of free political speech under Article 10.54
Against this backdrop, the general European standard for freedom of expression during electoral
campaigns enables national authorities to adopt measures to combat disinformation. As mentioned
before, false information, massively disseminated on account of modern technologies, may cause
collective and individual harm. There is a pressing social need and a legitimate aim (Article 10.2
ECHR) in ensuring the truthfulness of electoral information. Nonetheless, all remedies must be proportional
and there must not be any unjustified curtailment of procedural guarantees. If possible,
judicial review of any state action limiting the flow of electoral information should be granted.
II. Positive Obligations Concerning Freedom of Electoral Expression
In this context, another important issue arises: Do national authorities also have, in light of the
European standard, a positive obligation to provide effective measures against electoral disinformation?
To tackle this question, an important reference to current legal scholarship should be
made. Some authors reject the strict distinction between positive and negative obligations,
indicating that privileging the latter could be anachronistic in contemporary societies.55 The
non-interference principle in itself does not suffice where structural inequalities are present.
In order to effectively guarantee human rights, the individual must also be protected from an
abuse of power from other societal actors. The digital environment is indeed a sphere of huge
horizontal imbalances where the power of private entities has weakened constitutional states.
To suggest otherwise is to not be aware of the paradigm shift brought about by new media:
An international concentration of capital and power, combined with the decentralization of
production and the creation of a global “flow economy.”56
According to ECtHR case law, Article 10 implies an obligation by governments to promote the
right to freedom of expression and to provide an environment conducive to participation in public
debates by all persons concerned, allowing them to express their opinions and ideas without fear.57
Although privately-owned media, including newspapers, must be free to exercise editorial discretion
when deciding which articles, comments, and letters submitted by private individuals they
publish, “exceptional circumstances” may arise in which an editor can legitimately be required to
publish, for example, a retraction, an apology, or a judgment in a defamation case.58 This “right to
reply,” considered a positive obligation,59 constitutes an effective reaction to false facts and statements,
and guarantees proper balance between freedom of expression, public interest to access
accurate and truthful information, and reputational rights.60 It therefore seems clear that the positive
obligation to create an adequate environment for free expression, derived from Article 10,
includes the possibility of responding to untruthful statements, especially in cases where collective
or individual interests are at stake. National authorities should provide a legal framework for such
actions, taking into consideration the inequalities that may arise between societal actors.
53Animal Defenders International, App. No. 48876/08 at para. 112; Erdoğan Gökçe v. Turkey, App. No. 31736/04, para. 40
(Oct. 14, 2014), http://hudoc.echr.coe.int/eng?i=001-147465.
54TV Vest AS & Rogaland Pensjonistparti v. Norway, App. No. 21132/05, para. 66 (Dec. 11, 2008), http://hudoc.echr.coe.
int/eng?i=001-90235; Orlovskaya Iskra, App. No. 42911/08 at para. 111.
55LAURENS LAVRYSEN, HUMAN RIGHTS IN A POSITIVE STATE: RETHINKING THE RELATIONSHIP BETWEEN POSITIVE AND
NEGATIVE OBLIGATIONS UNDER THE EUROPEAN CONVENTION ON HUMAN RIGHTS 305–06 (2016).
56VAN DIJK, supra note 33, at 129.
57Dink v. Turkey, App. Nos. 2668/07, 6102/08, 30079/08, 7072/09 & 7124/09, para. 137 (Sep. 14, 2010), http://hudoc.echr.
coe.int/eng?i=001-100383.
58See Melnychuk v. Ukraine, App. No. 28743/03, para. 2 (July 5, 2007), http://hudoc.echr.coe.int/eng?i=001-70089.
59See Felix Hempel, The Right of Reply Under the European Convention on Human Rights: An Analysis of Eker v. Turkey, 10
J. MEDIA L. 17, 28–33 (2018).
60Eker v. Turkey, App. No. 24016/05, para. 48 (Oct. 24, 2017), http://hudoc.echr.coe.int/eng?i=001-178387.
German Law Journal 681
#
#Another important positive obligation concerning horizontal relations has to do with the role of
the state as a guarantor of pluralism within society.61 Democracy, as the only political model
contemplated by the Convention,62 offers the possibility of resolving a state’s problems through dialogue,
without recourse to violence, even when the problems are irksome. The essence of democracy
is to allow diverse political programs to be proposed and debated, even those that call into question
the way in which a state is currently organized.63 In the light of Article 10, therefore, in a sensitive
sector such as the media, the public authorities have, in addition to their negative duty of
non-interference, a positive obligation to put in place an appropriate legislative and administrative
framework to guarantee effective pluralism.64 The responsibility of the public authorities “as ultimate
guarantor of pluralism” has also been recognized under P1-3. The Court requires the adoption of
positive measures to organize democratic elections under conditions that will ensure the free expression
of the opinion of the people in the choice of the legislature. Public intervention should take place
in order to open the media up to different viewpoints.65 If the public authorities allow false electoral
information to be produced and massively disseminated in the media, without affording legitimate
actors—for example, candidates—any effective measures, pluralism is directly affected. Remaining
passive towards disinformation and adopting only a policy of non-interference may also affect the
electoral equality of the candidates and the fairness of the electoral process. There is no doubt that
substantive political equality can be a basis for positive free speech rights, with an ideal of equal
distribution to communicative resources.66
D. Summary Judicial Proceedings as a Reaction Towards Electoral Disinformation:
National Experiences and Possible Solutions
I. First Examples: Polish Electoral Law and the ECtHR’s Reactions
Since 1998, Polish electoral law67 has enabled all candidates standing for election, as well as electoral
committee representatives, to initiate special judicial proceedings if disseminated electoral materials
such as posters, slogans, leaflets, and statements contain false data or information. The interested
party may request that the court assess the truthfulness of these statements and issue an order prohibiting
their dissemination. This might also involve the confiscation of such materials and the cor-
rection of false information, as well as a ruling that protects his or her individual rights; for example,
publication of an answer to statements that violate reputation or an apology to the aggrieved party.
It is also possible to demand payment of up to 100,000 złoty (about 25,000 euros) to a charity organization.
The petition must be examined by a court in non-contentious civil proceedings within
twenty-four hours. The first-instance ruling may be appealed within twenty-four hours from the
moment it is pronounced, and the second-instance court must rule within the same time limit.
The Electoral Code also stipulates how the court’s decision is to be enforced.
On three occasions, the ECtHR has reviewed the practical application of these provisions. All three
cases concerned a local election, and the alleged disinformation was disseminated in a traditional
61LAVRYSEN, supra note 55, at 94.
62Refah Partisi (the Welfare Party) & Others v. Turkey, App. Nos. 41340/98, 41342/98, 41343/98 & 41344/98, para. 86 (Feb.
13, 2003), http://hudoc.echr.coe.int/eng?i=001-60936.
63Manole & Others v. Moldova, App. No. 13936/02, paras. 95–99 (Sept. 17, 2009), http://hudoc.echr.coe.int/eng?i=001-94075.
64Centro Europa 7 S.r.l. & Di Stefano v. Italy, App. No. 38433/09, para. 134 (June 7, 2012), http://hudoc.echr.coe.int/eng?i=
001-111399.
65Communist Party of Russia & Others v. Russia, App. No. 29400/05, paras. 125–28 (June 19, 2012), http://hudoc.echr.coe.
int/eng?i=001-111522.
66Jacob Rowbottom, Positive Protection for Speech and Substantive Political Equality, in POSITIVE FREE SPEECH:
RATIONALES, METHODS AND IMPLICATIONS 25 (Andrew T. Kenyon & Andrew Scott eds., 2020).
67The actual legal basis of the summary judicial proceeding is Article 111 of the Electoral Code from 2011. It replaced
previous provisions of different acts concerning parliamentary, presidential, and local elections. All ECtHR judgments analyzed
in this section of the Article refer to Article 72 of the Law of 1998 on Elections to Municipal Councils, District Councils,
and Regional Assemblies.
682 Adam Krzywoń
#
#manner—in written media or in the form of electoral leaflets.68 In the first case, the ECtHR pointed
out that electoral proceedings, conducted within very short time-limits, are aimed at ensuring the
proper conduct of the electoral campaign by preventing infringements of the candidate’s personal
rights, which could affect the result of the elections.69 The provision of such a summary remedy serves
the legitimate aim of ensuring the fairness of the electoral process and, as such, cannot be questioned
from the Convention standpoint. The general compliance of the model of electoral court proceedings
with the European standard may not, however, result in the undue curtailment of the procedural
guarantees afforded to the parties. The Court also noted that the Polish courts in the first and second
instances failed to comply with the requirement to hand down a judgment within the twenty-fourhour
period. Both rulings were issued after the balloting had taken place, when the extraordinary
proceedings had lost their relevance with regard to the claimant’s electoral perspectives. Under these
circumstances, the courts should have discontinued the case, given the fact that the aggrieved party
had the right to start ordinary civil proceedings for the protection of individual rights.
In the second case, the ECtHR noted that the national courts, in their expeditious examination
of election-related disputes, had failed to distinguish between statements of fact and value judgments,
and had not considered that the impugned material formed part of a debate on matters of
public interest.70 The Court pointed out that all the rulings concerning false electoral statements
were delivered after the local elections—almost three weeks in the first instance, and eight months
in the second—and they were not relevant to the electoral processes.71
In the third and most recent judgment, the ECtHR basically repeated its previous arguments. As
mentioned above, this ruling referred to the notion of “fake news” which could damage the candidate’s
reputation and should be rectified as soon as possible to preserve the quality of public debate.
The examination of election-related disputes should not, however, curtail the parties’ procedural
guarantees.72 Analyzing the circumstances of the case, the Court indicated that the applicant, a candidate
in the municipal elections, made several critical comments about the mayor standing for re-
election. Further, the Court pointed out that the electoral campaign was undoubtedly the most
appropriate moment to make such allegations public. In this context, the Court did not share
the national courts’ point of view that the alleged accusation, which was of a political nature,
had gone beyond acceptable forms of electoral propaganda.73 As far as sanctions were concerned,
the ECtHR emphasized their cumulative character. Simultaneously ordering the prohibition of the
electoral material’s further dissemination, the obligation to publish an apology in the front pages of
two local newspapers, and the payment of 5,000 złoty (about 1,200 euros) to a charity organization
was deemed to be disproportionate, and the Court suggested that such sanctions could have a chilling
effect on other candidates running in the local elections.74
II. Second Phase: French Law and the Digital Environment: Other National Examples
In December 2018, France adopted legislation expediting electoral proceedings and introducing
higher transparency requirements for political advertising on social media.75 The fight against
electoral disinformation has entered the second phase, because the French law recognizes the
68In Jezior, the applicant was held liable in the electoral judicial proceedings for third-party comments posted on his blog.
See Jezior v. Poland, App. No. 31955/11 (Mar. 4, 2009), http://hudoc.echr.coe.int/app/conversion/pdf/?library=ECHR&id=
001-152345&filename=001-152345.pdf&TID=ihgdqbxnfi. Nonetheless, in Jezior, the main issue was not the nature of the
electoral proceedings but the internet intermediary liability and his obligation to put in place adequate measures to detect
potentially illegal content. See supra note 4 and accompanying text.
69Kwiecień, App. No. 51744/99 at para. 55.
70Kita, App. No. 57659/00 at paras. 44–46.
71Id. at para. 50.
72Brzeziński, App. No. 47542/07 at paras. 35, 55.
73Id. at para. 57.
74Id. at para. 61.
75See supra notes 2 and 16 and accompanying text.
German Law Journal 683
#
#importance of the digital environment and internet-based communication for this harmful
phenomenon.76
Generally speaking, there are three fundamental mechanisms provided for in the French law
against manipulation of information. First, there is a set of obligations for online platform operators,
who should implement measures to combat the dissemination of false information likely to
disturb public order and electoral processes. They are obligated to establish an easily accessible
and visible mechanism enabling their users to report such information. Moreover, the transparency
of the algorithms used for recommending, classifying, or referencing informational content
should be ensured, and accounts that massively promote false news are to be monitored. When it
comes to financial aspects, three months prior to an election, online platforms must provide users
with details concerning the identity of actors who finance the promotion of content related to the
debate of general interest. Information about remuneration received in return for the promotion
of such content must be made public.
Second, the French law calls for special judicial proceedings to halt the spread of online disinformation
within three months prior to elections. A wide range of actors, including ministère
public (public prosecutors), candidates, political parties, and persons who have a legal interest,
may petition the court for an order that prohibits its dissemination. The court must issue a ruling
within forty-eight hours when inaccurate or misleading allegations or imputations of fact, able to
affect forthcoming elections, are deliberately, artificially, or automatically disseminated on a mass
scale through an online public communication service (Article L 163-2 of the Electoral Code). In
practice, to prohibit such dissemination, the manifest falsity of the claim must be established
before the court. There should not be any evidence to the contrary, the transmission of the false
information has to be deliberate, and the content must influence voters and affect the reliability of
the forthcoming election.77 An appeal is possible, but the appellate court is also required to issue
its ruling within forty-eight hours.
Third, the French law provides measures against foreign state propaganda. If false information
which could affect the electoral process is deliberately disseminated, the administrative authority
can suspend a broadcasting service controlled by another state.
A brief mention of German legislation is also relevant. Because Germany was the first state to
provide complex regulation against illegal online content,78 some scholars claim that there is “an
anti-fake news law” that “penalizes social media networks for harboring false and hateful content
in their platforms.”79 Others consider that the regulation “gave teeth” to existing criminal sanctions
by imposing heavy fines on social media platforms that fail to speedily delete “fake reports
and hate speech.”80 Indeed, German law protects social media users and obligates the platforms to
provide measures in order to report problematic content. Nonetheless, it does not create any new
legal offenses such as “hate speech” or “fake news” and refers to the existing definitions of crimes
under the German Criminal Code. There is a long list of illegal content, mostly undermining public
safety and general interests, that must be deleted by social media operators.81 Although the
battle against disinformation was one of the main arguments for adopting the law, the notion
does not appear in the law itself;82 the closest offense is the dissemination of propaganda material
76Amélie Heldt, Let’s Meet Halfway: Sharing New Responsibilities in a Digital Age, 9 J. INFO. POL’Y 336, 346 (2019).
77Id. at 56–58, 61, 69–71.
78Netzwerkdurchsetzungsgesetz [NetzDG][TheNetwork Enforcement Act], Sept. 1, 2017, BUNDESGESETZBLATT [BGBL] (Ger.).
79Juan Carlos Escudero de Jesús, Fake News and the Systemic Lie in the Marketplace of Ideas: A Judicial Problem?, 87
REVISTA JURIDICA U. P.R. 1394, 1395, 1413 (2018).
80Katsirea, supra note 7, at 159.
81See Wolfgang Schulz, Regulating Intermediaries to Protect Privacy Online—the Case of the German NetzDG, 1 HIIG
DISCUSSION PAPER SERIES 5 (2018), https://www.hiig.de/wp-content/uploads/2018/07/SSRN-id3216572.pdf.
82See Thomas Wischmeyer, “What is Illegal Offline is Also Illegal Online”—The German Network Enforcement Act 2017, in
FUNDAMENTAL RIGHTS PROTECTION ONLINE: THE FUTURE REGULATION OF INTERMEDIARIES 44 (Byliana Petkova & Tuomas
Ojanen eds., 2020).
684 Adam Krzywoń
#
#by unconstitutional organizations. As the German law obligates social media and other internet
platforms to fight continuously against illegal online content, it does not have a direct electoral
dimension. Regulated offenses do not relate specifically to pre-election false information.83
Moreover, it introduces reliance on private companies to identify and remove problematic content,
so generally speaking, judicial intervention is not required.
III. How Should Summary Judicial Proceedings Against Electoral Disinformation be Configured?
Specific and in abstracto Models
Previous sections of this Article illustrated that current standards of European human rights protection
enable one to point out the existence of a state’s positive obligations which seek to safeguard
electoral pluralism and combat false information (P1-3 and Article 10). This is particularly important
in the changing online environment, which creates a breeding ground for disinformation campaigns.
The national legal framework should reflect this digital reality, although, as the Polish example shows,
basic measures such as summary judicial proceedings may be formulated in rather general terms,
covering both online and offline false electoral information. Moreover, it seems that the simple penalization
of false statements is insufficient. This traditional form of law enforcement, as the aforemen-
tioned OSCE report shows,84 exists in almost every European country and is no longer effective.
Similarly, the fact that the decision to initiate prosecution depends on the public authorities—rather
than on a candidate, political party, or other private actor—implies a risk of unequal and selective
application. With regard to electoral disinformation, as it is unlikely that criminal proceedings will
be concluded prior to election day, all subsequent procedural rights should be fully-guaranteed
and expeditious examinations of criminal charges can be questioned from the Convention standpoint.
Against this backdrop, measures such as the Polish and French summary judicial proceedings,
which are of a civil nature, seem to be a more adequate reaction, and move towards the aforementioned
positive obligations. Generally speaking, a court’s assessment of the truthfulness of
electoral information, when the basic sanction is an order to stop its dissemination or to apologize
to the aggrieved party, constitute a legitimate interference in the political electoral discourse.
Obviously, the legal framework for these proceedings should be properly defined, according to
the aforementioned margin of appreciation conferred on the national authorities by Article 10
and P1-3, and its practical application must also respect the general rules for the protection of
freedom of expression established in ECtHR case law.
Summary judicial proceedings can target either the author of electoral disinformation or its
mass dissemination. As the analysis of the Polish and French laws has shown, there are two theoretical
models: A specific review, where the author of the false information is known, initiating a
kind of inter partes proceeding, or an in abstracto one, which is more viable if the author is not
known, and seems to be more applicable to the online environment. The latter reflects one of the
main characteristics of online speech, namely, its anonymity.85 The in abstracto model appears to
be a reasonable remedy when it is impossible to establish the subjective intention of the author or
those re-publishing the problematic content.
When it comes to the definition of electoral disinformation, from the Convention standpoint it
is not absolutely necessary to include it in the positive legal framework concerning summary judicial
examination. As the Polish example shows, in the specific model of these proceedings, the
simple notion of “false information” can be applied. One of the main conclusions of the first section
of this Article was that the defining element of disinformation is individual or collective harm.
Enabling a candidate or political party to initiate court proceedings to address false electoral information
means that a protected interest exists and is in jeopardy. In this model, harm is therefore of
83See Smith, supra note 16, at 77.
84See supra note 17 and accompanying text.
85See Delfi AS v. Estonia, App. No. 64569/09, paras. 148–49 (June 6, 2015), http://hudoc.echr.coe.int/eng?i=001-155105
(noting the degrees of possible anonymity on the internet).
German Law Journal 685
#
#an individual nature, as false information affects a candidate’s rights, but also indirectly influences
the public’s decision-making process, as false information is disseminated and disorients voters.
If the judicial proceeding is of an in abstracto nature, a definition of electoral disinformation
should be included. As seen in the French context, its adequate construction is crucial for the
assessment of the proportionality of interference. The definition provided for in in abstracto
proceedings should therefore focus on the element of harm and must target the most problematic
forms of electoral disinformation, namely, deliberate and mass dissemination of false information
using artificial or automatic means.
Nonetheless, in both cases there should be some initiative to launch judicial proceedings. It is
hard to imagine that in a democratic state the truth of electoral information would be assessed
ex officio by public authorities. It would impose an impossible and disproportionate burden and
could lead to the discriminatory application of such provisions. In other words, there should be an
external initiative in the preliminary phase, although subsequently the proceeding can be either
of a specific nature, with the direct participation of an affected candidate or political party, or of an
in abstracto nature, when the court makes an abstract assessment of whether there is mass
dissemination of false information that negatively influences electoral processes.
Similarly, in both types of judicial electoral proceedings, the court examines the falsity or truthfulness
of the facts, not value judgments. According to well-established ECtHR case law, the truth
of value judgements is not susceptible of proof, whereas the existence of facts can be demonstrated.86
Consequently, proving the truth of a value judgment concerning political or electoral
issues is simply impossible. Moreover, the general idea of summary proceedings is to provide
truthful information about candidates or political parties, which can then serve as the basis
for voters as they form their own political opinions. Authorities’ rulings must also not be substitutes
for individual discernment as citizens form their value judgments before election day. In this
respect, the public task should only be to guarantee the truthfulness of information and leave its
evaluation to voters.
With regard to sanctions, the main consequence of a court’s conclusion that electoral information
is false should be the prohibition of its further dissemination and/or an obligation to rec-
tify it by publishing a statement or an apology. The latter is obviously possible only if the author of
the disinformation is known, so it is difficult to imagine its applicability in the in abstractomodel,
which is aimed at anonymous and mass dissemination of false information. Nonetheless, a court’s
order to take down false electoral content that disturbs a voter’s decision-making process, and to
stop its spread before the ballot day, cannot be questioned from the Convention standpoint. As far
as other types of sanctions are concerned, including those of a financial nature, this is possible, but
in these cases scrutiny must be stricter. It is crucial to prevent a chilling effect on the electoral
debate. According to the aforementioned strong guarantees on the freedom to engage in political
discourse, all participants in electoral debates, including candidates and voters, should not be discouraged
from exercising their rights by the threat of excessive sanctions. If there are additional
possible sanctions, as in the Polish electoral law, their application must be proportional to the
harm caused by the dissemination of the false electoral information. The court should have
the possibility to choose between more or less restrictive measures and must always consider
the use of the more lenient one. Also, any arbitrariness in the application of the sanctions should
be avoided, as it could affect the equality of opportunities of the candidates.
The most problematic aspect of summary judicial proceedings are sanctions that are aimed at
the protection of a candidate’s personal rights—individual interests; for example, reputation, as in
the Polish model. As indicated above, the rationale of summary judicial proceedings is to guarantee
the reliability and fairness of decision-making processes, as well as the truthfulness of elec-
toral information. Harm caused by disinformation can also be individual, although from this
perspective, the obligation to rectify false statements and the prohibition of their further
86See, e.g., Morice v. France, App. No. 29369/10, para. 126 (Apr. 23, 2015), http://hudoc.echr.coe.int/eng?i=001-154265.
686 Adam Krzywoń
#
#dissemination seems to be a sufficient reaction. Claims related to non-pecuniary damages, such as
the violation of a candidate’s reputation, may be problematic for a court, as they imply procedural
burdens. It is hard to imagine how a court in a twenty-four-hour or forty-eight-hour time frame
could, at the same time, decide on the truthfulness of the electoral information, apply the proportionate
sanction, and provide satisfaction to the injured candidate, after having assessed the scope
of the damages. Moreover, from the electoral perspective, it is not necessary to examine the claims
related to the individual rights and interests before election day. With regard to the latter, the
aggrieved party can always initiate ordinary proceedings of a civil nature, where there is no curtailment
of procedural rights, and the court is not obliged to make an expeditious examination of the
claims and damages.87
As far as procedural aspects are concerned, malpractice in Polish electoral law has shown that,
in light of the European standard, summary proceedings should not be continued after elections.
Their aim is to provide reliable electoral information and guarantee the equality of electoral
opportunities. An expeditious examination of allegations is therefore justified by the need to
deliver a ruling before the voting day. Under such circumstances, there is no justification for
the curtailment of the procedural right, and the case will not have an impact on the electoral campaign.
At the same time, this is a further argument as to why there should be separate proceedings
for the protection of a candidate’s personal rights, as previously mentioned.
Other procedural questions arise concerning the influence of summary proceedings on electoral
and campaign silence. As in some European countries, electoral campaigns must end before
election day, usually twenty-four hours before. Using this standard, the court should be able to
determine exactly when to drop the case: By the end of the campaign or by election day. Similarly,
the legal framework should indicate if the rectification of false information—for example, publication
of a statement—could be done when the campaign is over, and voters have an opportunity
to reflect undisturbed on their political choices.
E. Concluding Remarks
Because judicial summary proceedings provide only a partial solution to the problem of disinformation,
they are not a remedy for all types of electoral abuses. They are insufficient, for instance,
when game-changing false information—such as statements about the closure of the polling stations
or the length of voting queues—is disseminated by automated and mass means just hours
before voting. Similarly, important questions remain unanswered concerning the enforcement of
judicial decisions. The prohibition on further dissemination of electoral disinformation, issued by
a national court, should be implemented using social media or other internet platforms, usually
based in different countries. This brings up the question of quick enforceability in the
international digital environment.
As the present analysis shows, summary electoral proceedings cannot be questioned from the
Convention standpoint and their implementation on the national level is one example of how to
comply with the state’s positive obligations. They must be adequately designed, as, on the one
hand, states enjoy a margin of appreciation concerning the institutional aspects of the electoral
system, and, on the other hand, the strongly protected freedom of political or electoral expression
is at stake. As far as legal frameworks are concerned, there is a choice between different models of
summary judicial proceedings, as the examples of Polish and French law show. The definition
of disinformation also poses a challenge, as it has to avoid vague terms and include the elements
of public and individual harm. Both from the theoretical and practical standpoints, any sanctions
87Jezior shows that in the Polish system it is possible to claim a violation of reputation in summary judicial proceedings and,
after the election, initiate ordinary proceedings of a civil nature based on the same allegations. See Jezior, App. No. 31955/11 at
paras. 11–22. As such, there is a risk of being held liable twice for the same violation, which is misaligned with the Convention.
See supra note 4 and accompanying text.
German Law Journal 687
#
#should be proportionate and aim to rebuild public trust in electoral information. Moreover, a
court should always make the distinction between facts and value judgments—the truth of the
latter is not susceptible to proof—and know exactly when to discontinue the case.
This analysis has also illustrated that there are other relevant spheres where positive action
would eventually be necessary. The current communication ecosystem calls for disclosure and
transparency policies, because the lack of transparency in algorithmic decision-making processes
increases the asymmetry between the actors, namely, internet platforms and their users or voters.88
There is no doubt that public authorities should take positive action in order to reduce this gap.89
Similarly, combating electoral disinformation requires that the bots (automated online accounts)
be clearly labelled and that users are able to critically analyze the content they produce.90 All of
these actions must consider the paradigm shift in communication. More attention should be paid
not to the power exercised by the state, but to the power exercised by the strong private actors who
currently share governance in the digital sphere.91 There is a need for “new-school speech regulation”
directed at the internet infrastructure, as well as for a new set of social responsibilities to
defend democratic values in the digital sphere.92
Adam Krzywoń Adjunct Professor at the Law Faculty of the University of Warsaw and Research Fellow at the German
Research Institute for Public Administration (FÖV), Speyer.
88SUNSTEIN, supra note 26, at 199–201.
89Giovanni De Gregorio, From Constitutional Freedoms to the Power of the Platforms: Protecting Fundamental Rights
Online in the Algorithmic Society, 11 EUR. J. LEGAL STUD. 65, 92 (2019).
90OSCAR SANCHEZ MUÑOZ, LA REGULACION DE LAS CAMPAÑAS ELECTORALES EN LA ERA DIGITAL: DESINFORMACION Y
MIROSEGMENTACION EN LAS REDES SOCIALES CON FINES ELECTORALES 18 (2020).
91NICOLAS P. SUZOR, LAWLESS: THE SECRET RULES THAT GOVERN OUR DIGITAL LIVES 169 (2019).
92Jack M. Balkin, Free Speech is a Triangle, 118 COLUM. L. REV. 2011 (2018); Jack M. Balkin, Free Speech in the Algorithmic
Society: Big Data, Private Governance, and New School Speech Regulation, 51 U.C. DAVIS L. REV. 1149 (2018).
Cite this article: Krzywoń A (2021). Summary Judicial Proceedings as a Measure for Electoral Disinformation: Defining the
European Standard. German Law Journal 22, 673–688. https://doi.org/10.1017/glj.2021.23
688 Adam Krzywoń
Legal risks of using regulatory technologies in 
business and professional activities 
Olga Sushkova* 
Kutafin Moscow State Law University (MSAL), Moscow, Russia  
Abstract. When using artificial intelligence technology in RegTech or 
SupTech solutions to prevent, detect and control financial crimes such as 
money laundering, it is necessary to be aware that due to compliance costs, 
many online financial firms are prohibited from providing financial advice, 
especially if they process transactions on behalf of clients or provide p2p 
investment platforms. RegTech streamlines KYC/CDD processes and 
therefore has the ability to reduce compliance costs. This, in turn, will allow 
more firms to enter the market to offer services. The regulatory goal of 
ensuring market integrity directly conflicts with the rights of individuals and 
the Data Protection Acts. It is argued that data governance will need to be 
established to protect individual rights and public safety. Furthermore, the 
question remains unresolved as to whether fiduciary duties can be assumed 
by robo-advisors or consultants using algorithms. Fiduciary duties may also 
be modified and limited by the parties. The fiduciary duty concerns what the 
fiduciary (investment adviser) cannot do (conflict of interest) but should not 
do (act in the best interest of the client). Consequently, the use of common 
law principles to protect consumer investors is currently underdeveloped, 
particularly if the AI seeks to provide access to finance and fill gaps in 
guidance.  
1 Introduction 
As Artificial Intelligence (hereinafter referred to as AI) can optimize KYC (Know Your 
Customer)/CDD processes to reduce the compliance costs of financial intermediaries, it can 
also create more investment firms to provide services to investors [1]. This will increase 
competition in the market, allow for more financial innovation, and improve access to finance 
for consumers and investors [2]. The development of financial innovation will increase the 
growth rate of the country's economy; increase the availability of capital; strengthen the 
national position in the global economy [3].  
AI technologies can be used to identify activities that compromise market integrity, such 
as market manipulation, including price-fixing, misinformation, insider trading, and money 
laundering. In addition, such AI technologies can be used by financial institutions, regulators, 
and private market observers to detect and prevent misconduct. When such AI technologies 
are used for this purpose, they are called Regulating Technologies (RegTech) [4]. RegTech 
will also include SupTech, which is mainly used for market surveillance purposes [5]. When 
                                                          
* Corresponding author: ovsushkova@msal.ru   
© The Authors, published by EDP Sciences. This is an open access article distributed under the terms of the Creative Commons 
Attribution License 4.0 (http://creativecommons.org/licenses/by/4.0/).
SHS Web of Conferences 106, 02013 (2021)
MTDE 2021
https://doi.org/10.1051/shsconf/202110602013
#
#RegTech is used to detect market misconduct, it includes elements of market surveillance 
that includes the collection of individual data.  
Shevchenko O.M. distinguishes two types of market manipulation of financial services, 
using the actions that are subject to administrative or criminal liability [6].  
The first type - information manipulation - in Russia, this is market manipulation through 
the dissemination of deliberately false information - deliberate dissemination through mass 
media with unrestricted access (including the Internet) or through any other means of 
deliberately false information that caused price, demand, supply or trade volume of financial 
instruments, foreign currency, and/or goods to deviate from the level or support significantly 
different from from the level that would have developed without the dissemination of such 
information. 
The second type is market manipulation through transactions in organized trading (trade 
manipulation). The Law 224-FZ [7] provides for 6 types of such manipulations, which differ 
not only in the ways they are committed but also in the consequences and types of organized 
trading where they are committed.  
2 Methodological basis of the study  
This paper has applied the following regulatory methods related to the management systems 
and processes in which AI is applied in securities trading and investment services through 
general scientific principles and approaches. HFT is used as an example, to explore how AI 
is regulated on a non-consumer-oriented trading platform. The main purpose of regulation is 
to deal with systemic risks - sudden failures, liquidity risks, and pro-cyclical behavior. A 
secondary goal is to protect investors from market manipulation. The main regulatory 
approach requires operators - specialist HFT firms, securities firms, private traders, and 
trading venues - to have internal risk management systems and processes in place. Thus, these 
operators are also required to consider the security and integrity of the market.  
A comprehensive approach and comparative research method identified the regulatory 
objective of market security, which is the basis for the continued regulation of AI for trading 
platforms. However, HFT firms, securities firms, and trading venues are subject to higher 
regulatory scrutiny. These regulatory methods may not be appropriate for those developers 
who provide AI FinTech services operating on p2p trading platforms. New p2p trading 
platforms, whether on a distributed ledger technology (DLT) network or networks like 
Amazon, will require greater consumer protections, including price discrimination and 
privacy rights protections. In a p2p trading platform where consumers trade securities, 
security and market integrity should apply the same regulatory goals. Platform providers that 
use algorithms to perform client transactions, such as allocating their portfolios, must ensure 
that clients are protected. To ensure there is no manipulation of the market, including price 
manipulation and price discrimination, platform providers will also need to bear the burden 
of identifying those who use algorithms to trade or distribute securities. Unlike regulated 
trading platforms, users of p2p platforms are likely to rely on algorithms designed to interact 
on the platforms. Individuals are unlikely to be able to implement risk management systems 
and controls. Consequently, to increase financial inclusion, the trading platform will monitor 
transactions and set parameters for where these algorithms will operate. There must be an 
effective mechanism in place to exclude participation in the platforms by anyone found to be 
using algorithms to create systemic problems or to manipulate the market. 
 
2
SHS Web of Conferences 106, 02013 (2021)
MTDE 2021
https://doi.org/10.1051/shsconf/202110602013
#
#3 Main part 
In this situation, the value of protecting individual rights and dignity may conflict with market 
integrity and the public interest [8]. However, this may be contrary to the spirit of the General 
Data Protection Regulation (GDPR) [9]. RegTech's primary goal is to protect the integrity of 
the marketplace. Exchanges use AI as a SupTech service [10], and as a RegTech service by 
financial institutions [11]. In addition to providing the previously mentioned suitable 
development regimes to protect investors' rights, another emerging challenge is the need for 
data governance that protects privacy and data protection [12].  
Collecting personal data for RegTech may violate data protection rules and privacy rights 
[13]. While consent is required for data control and processing, data collected for market 
integrity may be processed and transmitted without the individual's consent. Individuals may 
not have the right to prevent unauthorized sharing of their personal information under the 
GDPR and the Data Protection Act 2018. [14] The right to privacy may be violated when 
personal data is collected for RegTech development and deployment. 
4 Artificial intelligence and the fight against money laundering 
Some regulators use AI for fraud detection and anti-money laundering, and counter-terrorist 
financing (AML/CTF) [15].  
It appears that self-learning software and digital analytics as a mandatory regulatory 
requirement for systems aimed at combating money laundering, terrorist financing in the next 
5-10 years. Here it should noted that money launderers will also use software to build the 
logistics of money laundering and terrorist financing transactions [16]. 
Using AI and self-learning software concerning AML/CFT state financial control for 
business and professional entities [17] raises several questions, which are not yet answered 
by the international community either. 
First, how much confidence can be placed in the conclusions and recommendations that 
the AI will convey? 
Second, how can the effectiveness of AI performance be verified, and what legal criteria 
should be used for such evaluation? 
Third, to what extent should AI be integrated into the financial monitoring agent's 
procedures? Should the AI replace it completely or partially? What requirements should the 
state financial supervisory authority have for AI systems? Similar questions are asked by 
foreign financial control authorities [18]. 
Finally, it is imperative to define the distinction between the areas of responsibility of an 
AI and a financial monitoring officer, body, or agent [19]. 
The Australian Securities and Investments Commission (ASIC) is investigating the 
quality of results and the potential use of natural language processing (NLP) technology to 
identify and extract items of interest from evidentiary documents [20]. ASIC uses NLP and 
other technologies to visualize and explore their essence and emerging relationships. To 
combat criminal activities conducted through the banking system (e.g., money laundering), 
detailed information on bank transfers is collected, and this information is correlated with 
information from newspaper articles. Similarly, the Monetary Authority of Singapore (MAS) 
explores the use of AI and machine learning in analyzing suspicious transactions to identify 
those transactions that require further attention [21], allowing supervisors to focus their 
resources on higher-risk transactions. It takes a long time to investigate suspicious 
transactions.  
 
3
SHS Web of Conferences 106, 02013 (2021)
MTDE 2021
https://doi.org/10.1051/shsconf/202110602013
#
#5 Optimization of compliance processes 
The KYC process is often costly, time-consuming, and duplicative, involving many services 
and agencies [22]. According to Thomson Reuters, some major financial institutions spend 
$500 million annually on KYC and CDD; the top 10% of financial institutions worldwide 
spend at least $100 million annually, with an average of $48 million globally. For example, 
the application of AI in the KYC process can detect any attempt to use forged documents to 
perform KYC in real time. The AI could perform facial, documentary and any other checks 
in real time in a single cycle. Thus, AI helps financial institutions perform background 
checks, and machine learning is used to ensure that in real-time to avoid unwanted 
inspections by regulators and monetary penalties.  
Machine learning is used in two instances: (1) when assessing whether images in 
identifying documents match each other, and (2) when calculating risk assessments by which 
firms determine which individuals or applications need additional verification.  
Money-laundering-based risk assessments are also used in ongoing periodic checks based 
on publicly available and other data sources, such as police offender registries and social 
media services [23]. Using these sources can allow for a quick and cheap assessment of risk 
and confidence. Thus, research is now needed to determine how regulators are adopting this 
approach and their concerns. 
6 Study results 
Current legislation does not prohibit government agencies or financial institutions from 
collecting individual data in the public domain, which can help them build a customized 
profile for KYC purposes and detect suspicious transactions. However, the ethical basis for 
individual profiling for financial market supervision, to date, is not only firmly established 
but also spot regulated. The setting of parameters for agencies should be based on human 
rights and principles of human dignity to ensure individual and community safety [24]. Such 
profile information could fall into the wrong hands and be used maliciously. 
Individual consent is insufficient to protect an individual for the following reasons: firstly, 
an individual cannot, on general constitutional principles, consent to harm; secondly, an 
individual may not be aware of the risk; and thirdly, an individual may not know what and, 
what he or she is consenting to. Consequently, [25] there is also a need to revise the definition 
of 'individual consent' and develop clear criteria for determining the consent method, the 
purpose of consent, and the possible review and withdrawal of consent. 
Even for KYC processes conducted to protect the individual, such as assessing clients' 
risk appetite under client eligibility rules, consent to individual profiling is also problematic. 
Problems can arise with the quality and accuracy of the data, affecting the quality and 
accuracy of the profiling. Data can be collected through social media and other smart devices. 
These integrated datasets containing information about an individual, possibly with enhanced 
information, can be easily seized by third parties upon legal requests, such as a request from 
a foreign government. 
As clients have not consented to share their datasets with third-party government 
agencies, transferring these data or making them available to government agencies and 
gaining access to these datasets could have detrimental consequences for citizens' rights in 
litigation (or in bankruptcy) [26]. For example, the initial data collector, even with full 
compliance with statutory obligations at the outset, would still breach its legal obligations if 
it shared data with the next government agency for further processing of that data if the first 
collector did not provide a detailed explanation of further data sharing and did not obtain 
consent at the time of collection [27].  
4
SHS Web of Conferences 106, 02013 (2021)
MTDE 2021
https://doi.org/10.1051/shsconf/202110602013
#
#7 Discussion of the results of the study 
Analyzing the situation with digital banking in Russia, O.A. Tarasenko rightly notes that "the 
availability of remote customer identification affects the growth of digital banking, so banks 
in all countries are looking for ways to implement such tools. Two key areas of technology 
development in this area should be noted. The first direction is the proliferation of "single 
identification" systems, whereby a person once identified by a bank is considered to be 
identified to others. The second direction is the development of biometrics" [28]. 
Russia has had a Unified Biometric System (UBS) in place since 2018, with Rostelecom 
as the operator and the Russian Ministry of Digital Development, Communications, and Mass 
Media as the regulatory body. According to the Central Bank's Financial Inclusion Strategy, 
Tarasenko O.A., the purpose of the UBS is to digitalize financial services, make financial 
services more accessible to consumers, and increase competition in the financial market. 
In foreign jurisdictions, funds using specific investment techniques, such as AIFs 
(alternative investment funds), attract many organizations and individuals to invest. When 
individual investors or corporations make investments, they may also be asked for their 
personal information, including personal details, proof of income, details of dividend 
payments, and repayment details. proceeds and tax residency information. They are collected 
for various purposes, such as identification or assurance of commitment [29]. Thus, personal 
information is controlled, processed and stored not only by the investment fund companies, 
management companies or transfer agencies, but also by the directors of these companies or 
other persons. 
To guarantee the security of fund transactions, MiFID II requires fund companies to use 
six data collection criteria [30]. For example, to prevent money laundering, clients may be 
asked to provide a certificate of income. In addition, the Anti-Money Laundering, CounterTerrorist
Financing, and Transfer of Funds Regulations 2017 require companies to ensure 
that assets belonging to their customers are held securely. According to these provisions, 
companies must keep records for at least 5 years with as much detail as possible [31]. This 
includes personal information regarding relationships, order processing, reports, assets, etc. 
To delete personal data without undue delay is the duty of the information controller (Art. 
17(1), GDPR). This principle conflicts with MiFID II, which requires a company to retain all 
records held by it concerning its MiFID business for a period of at least 5 years. (FCA's 
Systems and Control Sourcebook). The purpose of the information collected is paramount. 
8 Conclusion 
AI will bring benefits and risks to the financial services sector. AI should continue to be 
regulated to ensure continuity, market safety, investor protection, and market integrity. In 
addition to this, access to finance should be a regulatory goal so that AI can be used to benefit 
financial intermediaries and provide greater social benefit to those who were previously 
financially disenfranchised. Access to finance will help the use and regulation of AI gain 
wider public acceptance. For this purpose, AI can optimize capital on p2p platforms to help 
consumers have cheaper access to additional information through robo-advisors and by using 
RegTech services to streamline KYC/CDD processes, which reduces compliance costs. More 
detailed rules are needed to certify good algorithms and good platforms, strengthen ex-ante 
and ex-post protections for individuals using robo-advisors, and address how individual 
rights, such as privacy rights and data rights, can be enforced, protected to conduct more 
effective KYC processes. 
 
5
SHS Web of Conferences 106, 02013 (2021)
MTDE 2021
https://doi.org/10.1051/shsconf/202110602013
#
#References 
1. J. Kingston, Artif Intell Law 25, 429 (2017) 
2. I.V. Ershova, General provisions on self-regulation of business and professional 
activities. Chapter 2. Self-regulation of entrepreneurial and professional activity: 
textbook. M.: Prospect, 35 (2020) 
3. R.O. Voskanyan, T.V. Vaschenko, Azimuth of Scientific Research: Economics and 
Management 4(21), 75 (2017) 
4. According to the definition given by FCA, 'RegTech applies to new technologies 
developed to help overcome regulatory challenges in financial services'. FCA (2017a) 
RegTech. https://www.fca.org.uk/firms/innovation/regtech. 
5. P. Armstrong, Developments in RegTech and SubTech. 
https://www.esma.europa.eu/sites/default/files/library/esma71-991070_speech_on_regtech.pdf
(2018) 
6. O.M. Shevchenko, Legal regulation of the securities market and collective investments: 
textbook, 372 (2021) 
7. Federal Law of 27.07.2010 No. 224-FZ (as amended on 01.04.2020) "On Counteracting 
the Unlawful Use of Insider Information and Market Manipulation and on Amending 
Certain Legislative Acts of the Russian Federation" // Collected Legislation of the 
Russian Federation. 2010. No. 31. Art. 4193. 
8. Yu. G. Leskova, Jurist 11, 13 (2013) 
9. Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 
2016 on the protection of natural persons with regard to the processing of personal data 
and on the free movement of such data, and repealing Directive 95/46 / EC (General 
Data Protection Regulation) OJ L 119/1 (2016) 
10. M. Polinsky, S. Shavell, Harvard Law Rev. 123, 1437 (2010) 
11. Information Commissioners Office (2017) Big data, artificial intelligence, machine 
learning and data protection. https://ico.org.uk/media/fororganisations/documents/2013559/big-data-ai-ml-and-data-protection.pdf.
12. O.V. Sushkova, Features of the influence of the institution of protection of commercial 
information and personal data on innovations developed by startups. In the book: Digital 
transformation: challenges to law and vectors of scientific research: monograph, under 
total. ed. A.N. Savenkov; otv. ed. T.A. Polyakova, A.V. Minbaleev. M .: Prospect, 159 
(2021) 
13. Information Commissioners Office (2019b) Data protection by design and default. 
https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-dataprotection-regulation-gdpr/accountability-and-governance/data-protection-by-
design-
and-default / 
14. FCA, Personal data and market oversight. Last updated 15 May 2020. 
https://www.fca.org.uk/privacy/personal-data-and-market-oversight. Accessed 5 Oct 
2020 (2018a) 
15. Deloitte, The case for artificial intelligence in combating money laundering and terrorist 
financing: a deep drive into the application of machine learning technology. 
https://www2.deloitte.com/content/dam/Deloitte/sg/Documents/finance/sea-fasdeloitte-uob-whitepaper-digital.pdf
(2018) 
16. Federal Law of 07.08.2001 No. 115-FZ (as amended on 30.12.2020) "On Counteraction 
to Legalization (Laundering) of Criminally Obtained Incomes and Financing of 
Terrorism" // Collected Legislation of the Russian Federation. 2001. No. 33 (part I). Art. 
3418 
17. M.Yu. Chelyshev, A.V. Mikhaylov, The Rule of Law 1(13), 54 (2013) 
6
SHS Web of Conferences 106, 02013 (2021)
MTDE 2021
https://doi.org/10.1051/shsconf/202110602013
#
#18. Speech by Rob Gruppetta, Head of the Financial Crime Department at the FCA, 
delivered to the FinTech Innovation in AML and Digital ID regional event, London // 
https://www.fca.org.uk/news/speeches/using -artificial-intelligence-keep-criminalfunds-out-financial-system
19. K.T. Anisina, B.G. Badmaev, I.V. Bit-Shabo and others, Financial law in the context of 
the development of the digital economy: monograph; ed. I.A. Tsindeliani. M .: Prospect, 
36 (2019) 
20. FSB, Artificial intelligence and machine learning in financial services: market 
developments and financial stability implications. https://www.fsb.org/wpcontent/uploads/P011117.pdf
(2017) 
21. Medici, Risk management: the most important application of AI in the financial sector. 
https://gomedici.com/risk-management-most-important-application-of-ai-in-financialsector
(2018) 
22. J. Callahan, Know your customer (KYC) will be a great thing when it works. 
https://www.forbes.com/sites/forbestechcouncil/2018/07/10/know-your-customer-kycwill-be-a-great-thing-when-it-works/#1f3fe9a98dbb
(2018) 
23. FSB, Artificial intelligence and machine learning in financial services: market 
developments and financial stability implications. https://www.fsb.org/wpcontent/uploads/P011117.pdf
(2017) 
24. Tadros, The Principle of Prevention of Harm under EU Ethics Guidelines for 
Trustworthy AI (2011) 
25. O.V. Sushkova, Features of legal, ethical and social characteristics when testing a 
person's personal genome. In the book: Genetic technologies and law during the 
formation of bioeconomics: monograph, otv. ed. A. A. Mokhov, O. V. Sushkova. M.: 
Prospect, 536 (2020) 
26. O.V. Sushkova, Vlast 'zakona: scientific and practical journal 2(42), 99 (2020) 
27. European Commission, Intrusive surveillance technologies would be considered ‘high 
risk’, 18 (2020) 
28. O.A. Tarasenko, Digital Banking in Russia (§2). In the book: Digital economy: 
conceptual basic legal regulation of business in Russia: monograph, otv. ed. V.A. 
Laptev, O. A. Tarasenko. M.: Prospect, 351 (2020) 
29. Deloitte, GDPR for funds. 
https://www2.deloitte.com/content/dam/Deloitte/ie/Documents/FinancialServices/inves
tmentmanagement/GDPR%20for%20Funds%20FINAL.pdf (2017a) 
30. ESMA Technical advice to the Commission on MiFID II and MiFIR. 
https://www.esma.europa.eu/document/technical-advice-commission-mifid-ii-andmifir
(2014) 
31. FCA Safe custody services and money laundering. https://www.fca.org.uk/firms/moneylaundering/safe-custody-services
(2017) 
7
SHS Web of Conferences 106, 02013 (2021)
MTDE 2021
https://doi.org/10.1051/shsconf/202110602013
royalsocietypublishing.org/journal/rsosPerspective
Cite this article: Hopf H, Krief A, Mehta G,
Matlin SA. 2019 Fake science and the knowledge
crisis: ignorance can be fatal. R. Soc. open sci. 6:
190161.
http://dx.doi.org/10.1098/rsos.190161
Received: 25 January 2019
Accepted: 19 March 2019
Subject Category:
Chemistry
Subject Areas:
human-computer interaction
Keywords:
fake science, trust in science, knowledge crisis
Author for correspondence:
Stephen A. Matlin
e-mail: s.matlin@imperial.ac.uk& 2019 The Authors. Published by the Royal Society under the terms of the Creative
Commons Attribution License http://creativecommons.org/licenses/by/4.0/, which permits
unrestricted use, provided the original author and source are credited.This article has been edited by the Royal Society
of Chemistry, including the commissioning, peer
review process and editorial aspects up to the
point of acceptance.Fake science and the
knowledge crisis: ignorance
can be fatal
Henning Hopf1,2, Alain Krief1,3,4, Goverdhan Mehta1,5
and Stephen A. Matlin1,6
1International Organization for Chemical Sciences in Development, 61 rue de Bruxelles,
5000 Namur, Belgium
2Institute of Organic Chemistry, Technische Universität Braunschweig, Braunschweig 38106,
Germany
3Chemistry Department, Namur University, 5000 Namur, Belgium
4HEJ Research Institute, University of Karachi, Karachi City, Sindh 75270, Pakistan
5School of Chemistry, University of Hyderabad, Hyderabad 500046, India
6Institute of Global Health Innovation, Imperial College London, London SW7 2AZ, UK
HH, 0000-0001-7040-6506; AK, 0000-0002-9223-1644;
GM, 0000-0001-6841-4267; SAM, 0000-0002-8001-1425
Computers, the Internet and social media enable every individual
to be a publisher, communicating true or false information instantly
and globally. In the ‘post-truth’ era, deception is commonplace at all
levels of contemporary life. Fakery affects science and social
information and the two have become highly interactive globally,
undermining trust in science and the capacity of individuals and
society to make evidence-informed choices, including on life-ordeath
issues. Ironically, drivers of fake science are embedded in
the current science publishing system intended to disseminate
evidenced knowledge, in which the intersection of science
advancement and reputational and financial rewards for scientists
and publishers incentivize gaming and, in the extreme, creation
and promotion of falsified results. In the battle for truth, individual
scientists, professional associations, academic institutions and
funding bodies must act to put their own house in order by
promoting ethics and integrity and de-incentivizing the production
and publishing of false data and results. They must speak out
against false information and fake science in circulation and
forcefully contradict public figures who promote it. They must
contribute to research that helps understand and counter false
information, to education that builds knowledge and skills in
assessing informationand tostrengtheningscience literacy insociety.
1. Introduction
Seek truth from facts [1]
; shı́ shı̀ qiú shı̀
Quoted from the Han Dynasty Book of Han, AD 111
#
#royalsocietypublishing.org/journal/rsos
R.Soc.open
sci.6:190161
2Ignorance of the truth, or knowledge that is not acted upon, can be fatal. This basic principle applies at
levels from personal to planetary. Fakery affects science as well as everyday social information and, since
the two have become highly interactive globally, a vicious cycle is now operating on an increasing scale.
The fake news/fake science cycle undermines the credibility of science and the capacity of individuals
and society to make evidence-informed choices in their best interests.
At the individual level, lack of reliable knowledge about how to maintain personal physical,
nutritional and health security can result in avoidable harm or death. An illustration is the child
illnesses, permanent disabilities and deaths that have resulted worldwide from the fabricated scientific
report [2,3] that the measles, mumps and rubella (MMR) vaccine causes autism. Despite the proven
record of efficacy of vaccines in preventing infections by deadly diseases, the widespread
dissemination of this lie, especially through social media, resulted not only in record levels of measles
infections in Europe [4] in 2018 but has added fuel to a growing, broader phenomenon known as
‘vaccine hesitancy’ [5,6].
At a collective level, false information can alter attitudes and policies on crucial ecological, social and
political issues and, in the extreme, can place entire populations at national, regional and even global
levels at risk of harm. For example, the denial of anthropogenic climate change, dismissed without
counter-evidence as ‘fake science’, has resulted in the international agreement on climate change
losing universal acceptance and its impact on the level of global warming is likely to have disastrous
consequences worldwide in the twenty-first century [7]. Portrayal of ethnic groups, foreigners or
foreign states as enemies through fake stories is an old technique which has been given new potency
by modern methods of mass communication and can provoke genocides and wars [8]. Fake Twitter
accounts have reportedly been used to send millions of messages aiming to influence attitudes
towards Brexit in the 2016 UK referendum and views on candidates in the 2016 US presidential
election [9].2. Validated information needs to be acted upon
Awareness of the evidence that smoking causes severe illnesses and that tobacco kills up to half of its
users has not yet enabled the 1.1 billion smokers around the world to quit their addiction or
governments to introduce outright bans on smoking. Denial, fake news, the deliberate undermining of
true data by portrayal as ‘junk science’ to distort public health policy, fabricated information,
distortion of media framing and covert illicit trading have been documented in the decades-long
battle by the tobacco industry and its supporters to sustain their lucrative but fatal trade [10,11].
Examples such as climate change and tobacco illustrate the difficulties that both individuals and
society can have in determining what is factually correct, how to recognize the biases and vested
interests that may lie behind information available and how to balance risks against benefits at
personal, national and global levels.3. Evolution of scientific validation
A critical factor is the question of who is accorded authority to determine reliability of facts and make
judgement about the veracity of information on offer. Since ancient times, those with wealth, power
and elevated hierarchical positions were treated as privileged sources, as were some individuals
regarded as selfless seekers of wisdom in the domains of spirituality, scholarship or science.
Since its introduction by Francis Bacon (1561–1626), the evolved scientific method has included
unbiased observations that are evaluated for reproducibility and subjected to careful self-criticism and
logical thought about their meaning and implications, then offered for inspection by the world at
large. The establishment of learned societies and the publication of journals, beginning with the
Philosophical Transactions of the Royal Society, first published in 1665, provided a mechanism for
presenting information that could be critically examined by the scientific community. If disproved, the
prevailing models and theories would be replaced by new ones that were more consistent with
the contemporary state of knowledge. This provisional character of science is not a weakness but is
one of the key reasons for its strength.
The evolution of this process, into the second half of the twentieth century, established a ‘gold
standard’ for the reliability of knowledge. It has been the bedrock of the esteem in which science has
been held, as an honest and impartial source of evidence-based knowledge, not only to advance the
frontiers of the field but also to inform the public and politicians and aid their decision-making. The
#
#royalsociety
31918 landmark report by Richard Burdon Haldane to the British Prime Minister signalled the strength of
the evolving relationship between science and policy, with Haldane arguing for the principle that
politicians should stay out of decisions about research funding, listen to expertise, take time to think
and reflect before reaching a conclusion and, when asking scientists for advice, resist telling them
what that advice should be [12].publishing.org/journal/rsos
R.Soc.open
sci.6:1901614. The changing landscape: the revolution in knowledge production
While the degree to which scientific inputs to policy formulation are important continues to be debated,
the current revolution in knowledge production has further complicated the issue. An illustration of the
extent to which the landscape has changed was seen in the centenary year of the ‘Haldane Principle’,
which witnessed a major scientific report commissioned by the US government and issued by 13
federal agencies—warning of the consequences of climate change and therefore at odds with the
Administration’s policies—being rejected by a number of leading politicians, including the President,
on the grounds that they ‘don’t believe it’, while they (baselessly) accused climate scientists of being
driven by money [13,14].
In 1991, Harnad described four stages in the means of production of knowledge in human beings
[15]. The first three were the emergence of language (hundreds of thousands of years ago) and
invention of writing (several thousand years ago) and printing (over 500 years ago). The fourth had
only very recently begun, with the invention of the Internet and the capacity it provides for anyone
in the world to be a publisher—to communicate any information they wish, true or false, instantly
and globally.
Facts and their denial are no longer determined by any type of authority, but in principle by every
individual, regardless of his or her education and reputation or studiously acquired knowledge of a
field. The manipulation of data by anyone (including scientists) becomes ever easier. Due to the ready
availability of information and communication technology (ICT) tools and access to the Internet and
social media, there now exist countless ways to create and distribute products of unknown veracity,
including manipulated textual and pictorial material. The predictions of the anarchistic philosopher of
science Paul Feyerabend that ‘anything goes’ and of the conceptual artist Joseph Beuys that ‘every
human being is an artist’ have thus become reality.
In his 1943 essay on the Spanish Civil War, the writer George Orwell recognized the way that people
in politics and wars make use of the available propaganda mechanisms to create their own versions of the
truth, expressing his fear that ‘the very concept of objective truth is fading out of the world’ [16]. This
ongoing challenge has been exacerbated and accelerated greatly by ICT and the fourth revolution in
knowledge production. As Harnad recognized, each of these knowledge revolutions represented a
profound, qualitative change both in HOW human beings communicate and think and in WHAT
is thought.5. Consequences for science and for science publishing and assessment
The impacts of the fourth revolution, barely to be seen three decades ago, are now dramatically evident,
including in contemporary language. A signpost was the declaration by Ralph Keyes in 2004 that ‘we live
in a post-truth era’—a stage of social evolution that is ‘beyond honesty’, in which ‘deception has become
commonplace at all levels of contemporary life’ [17]. More recent signposts have been the emergence in
2017 of the term ‘alternative facts’ to describe inaccurate data and the designation of ‘truth isn’t truth’ as
the 2018 Quote of the Year.
There are growing impacts both on the interface between science and society and within the domain
of science itself. It has been argued that, in the current political and media environment, ‘distrust in the
scientific enterprise and misperceptions of scientific knowledge increasingly stem. . . from the widespread
dissemination of misleading and biased information’ [18]. The philosopher Bruno Latour has observed
that ‘facts remain robust only when they are supported by a common culture, by institutions that can
be trusted, by a more or less decent public life, by more or less reliable media’ [19]. While surveys
about the public’s view of the trustworthiness of scientists produce results that vary with time and
place [20,21], in his 2017 book on the ‘death of expertise’ Tom Nichols described the many forces
trying to undermine the authority of ‘experts’ [22], so that the term itself has started to be used in a
contemptuous way to justify dismissing their advice [23].
#
#royalsocietypublishing.org/journal/rsos
R.Soc.open
sci.6:190161
4Facing this challenge, it is especially important that the scientific world as a whole upholds the
highest standards of ethical behaviour, honesty and transparency, aiming to sustain the gold
standards of research integrity and validated information. Sadly, a range of forces are working
counter to this aspiration. People in the world of science are not immune from the personal ambitions
and prevailing pressures that drive behaviour in general.
As recently described [24], three closely inter-related sub-systems (science advancement, reputational
rewards and financial returns) collectively form an overall scientific publishing system that has become
heavily flawed. It encourages scientists to distort and exaggerate their results in striving for new grants,
promotions and distinctions; and encourages publishers to cherry-pick work, hype results and distort
refereeing in the competition for high status and correspondingly high profits from publication
charges. Both authors and publishers are incentivized to game the system to their mutual advantage.
In the extreme, the perverse incentives generated result in authors fabricating data, predatory journals
hunting for papers and fake journals being created that seek only the authors’ fees for article processing.
The scale of the fake science problem is becoming increasingly evident. The percentage of scientific
articles retracted because of fraud has increased by an order of magnitude since 2000 and high rates
of retraction are seen for the most prestigious journals, illustrating both the extent to which flawed
claims are perpetrated by scientists seeking prominence and weaknesses and even fakery in the
current practice of peer reviewing [25]. A recent investigation of publishing in predatory ‘open access’
journals and fake conferences has revealed a global ecosystem of predatory publishers churning out
‘fake science’ for profit [26]. The intrusion of such journals into the traditionally respected space of
science publishing seriously undermines the integrity and credibility of science and, if not stopped
and sanctioned immediately, could turn out to be fatal for the field as we know it.
It is a fundamental strength of the scientific system that knowledge that is incorrect will eventually be
discovered and discarded. However, the pace and scale at which material that is at best dubious and at
worst deliberately false is now being published is creating a crisis. The consequences are very damaging
for the science enterprise, with a loss of respect for the results of science and the scientific method leading,
inter alia, to a steep decline in funding, jobs and students wishing to enter the field. The crisis is also
damaging society, creating an ‘anything goes’ environment in which ‘alternative facts’ are not tested and
decisions affecting the lives of people everywhere are not informed by authentic data or valid conclusions.
Thus, in the new age of the fourth revolution in the means of production of knowledge, scientific
publishing has become a part of the problem of fake news, rather than a bulwark against it.
6. Ways forward
Fake science and fake news are complex phenomena involving a variety of causes, channels of
dissemination and consequences. Solving the challenges they pose will not be accomplished by a
single approach or simple set of measures, but will require concerted effort by a wide range of actors
across sectors.
To address the general societal problem of fake news, several initiatives now underway or being
discussed offer promising approaches. Apart from those directly involving science and scientists,
which are discussed separately below, they include the following.
Efforts are needed to counter the spread of false information via social media, through
modifications to computer algorithms that favour ‘trending’ of stories without a factual basis [27],
and development of tools that help identify and build skills in recognizing false claims [28–30].
The limitations of large-scale automated approaches and the ingenuity with which they can be
gamed must, however, be recognized [31].
There should be more efforts to increase the responsibility taken by social media services for the
content they permit online. The fundamental issue of whether social media should be regarded as
‘platforms’ that are not responsible for content (as the social media maintain) or as ‘publishers’ who
can, like traditional print publishers, be held liable for the content they disseminate (as some critics of
the present position propose), with many legal, regulatory, financial, ethical and operational
ramifications, remains in dispute [32,33]. Meanwhile, there has been widespread dissatisfaction with
the results of self-regulation by social media to date, and highly publicized failures in areas including
politics, racism and health have led to calls for more regulation and/or more action by social media
[34,35]. Initiatives required include efforts to increase the speed and scope of measures to remove
offensive and injurious materials and to develop algorithms to detect and exclude fraudulent sources.
Scientists must not remain bystanders in the battle against fakery in news generally as well as in their
own domains of expertise. They can contribute to understanding the phenomenon of fake news, which
#
#royalsocietypublishing.org/journal/rsos
R.Soc.open
sci.6:190161
5has typically been studied along four lines: characterization, creation, circulation and countering [36].
Multidisciplinary effort is needed to understand better how the Internet spreads content and how
readers process the news and information they consume, as well as how social media platforms are
manipulated to amplify particular stories through the use of fake accounts and ‘bots’ [37–40]. As an
example, WhatsApp has selected 20 research teams worldwide, including from India, to work
towards understanding how misinformation spreads and what additional steps the mobile messaging
platform could take to curb fake news [41].
Scientists must be willing to speak out when they see false information being presented in social
media, traditional print or broadcast press [42]. They must use these media fully themselves [43] to
offer facts and evidence in succinct layman’s language while emphasizing the breadth and depth of
the scientific consensus which underpins the present state of knowledge and pointing to the lack of
scientific rigour in the false information [44–46]. They must be willing to contradict public leaders
and opinion formers who condemn or dismiss valid science without offering verified evidence of their
own, as has happened, for example, in the USA and India [47–49].
For the longer term, scientists must be better advocates for and contributors to the generation of a
more scientifically literate society [50]. The ultimate defence against fake facts is the capacity of each
individual to examine critically the information on offer and to reach judgement about its
trustworthiness that is based on evidence and reasoning. Scientists can contribute to inculcating
‘scientific temper’ in society. This term, coined in 1946 by Jawaharlal Nehru, describes a way of life, a
process of thinking and acting which uses the scientific method and may, consequently, include
questioning, observing, testing, hypothesizing, analysing and communicating [51].
The role of journalism remains important and development by scientists of stronger links with
reputable journalists can encourage clearer and more accurate reporting of research [52].
Within the domain of science itself, individually and collectively through their professional
associations, academic institutions and funding bodies, scientists must act in order to put their own
house in order, through promoting ethical practices and research integrity, dealing with the problems
of reproducibility and retractions [53], developing policies and practices to de-incentivize the
production and publishing of false data and results and the use of ‘predatory’ journals that have
inadequate peer review, and making maximum use of emerging artificial intelligence capacities [54] to
detect and expose falsified data and images. Examples where measures are already being adopted or
explored include India’s use of a ‘white list’ to discourage researchers from publishing in predatory
journals [55].
Education—both broadly as part of the development of life skills and specifically in the culture and
methods of science—is an essential part of the long-term solution, so that young people are equipped
with knowledge, skills and tools to be able to critically examine information and assess its veracity
[56,57]. As noted by the President of the European Research Council, ‘We need to train a new
generation of critical minds. Science is not about learning facts by heart, established long ago; it is
about knowing how to call into question and move forward. The majority of youth rely mostly on
social media to get their news, so we must tackle this issue through improved news literacy, and it is
the task of our educators and society at large to teach children how to use doubt intelligently and to
understand that uncertainty can be quantified and measured’ [58].
Research indicates that pre-emptively inoculating people before they receive misinformation
(prebunking) is more effective than refutation after receipt (debunking) in reducing the influence of
misinformation. Synthesizing separate lines of research from education, cognitive psychology and
inoculation theory (a branch of psychological research) provides a coherent set of recommendations
for educators and communicators. Scientific explanations that involve clear communication of
scientific concepts and the current scientific consensus are ideally coupled with inoculating
explanations of how that science can be distorted [59].
Data accessibility. All data cited in this article are taken from the sources given in the references.
Authors’ contributions. All authors listed made substantial contributions to the conception, design, drafting and revision of
this article; gave final approval of the version to be published; and agreed to be accountable for all aspects of the work
in ensuring that questions related to the accuracy or integrity of any part of the work are appropriately investigated
and resolved.
Competing interests. We declare we have no competing interests.
Funding. We are grateful to the International Organization for Chemical Sciences in Development, the Gesellschaft
Deutscher Chemiker, the Royal Society of Chemistry and Syngenta for supporting a workshop held in the Indian
Institute of Chemical Technology, Hyderabad and hosted by its Director, Dr Srivari Chandrasekhar in January
2019, during which this article was prepared.
#
#6References
royalsocietypublishing.org/journal/rsos
R.Soc.open
sci.6:1901611. Ban G, Ban Z. 111AD Book of Han. See https://
www.revolvy.com/page/Seek-truth-from-facts.
2. Rao TSS, Andrade C. 2011 The MMR vaccine and
autism: sensation, refutation, retraction, and
fraud. Indian J. Psychiatry 53, 95 – 96. (doi:10.
4103/0019-5545.82529)
3. Hviid A, Hansen JV, Frisch M, Melbye M. 2019
Measles, mumps, rubella vaccination and
autism: a nationwide cohort study. Ann.
Intern. Med. 170, 513 – 520. (doi:10.7326/
M18-2101)
4. Measles cases hit record high in the European
Region. 2018 WHO European Region,
Copenhagen, Denmark, 20 August 2018. See
http://www.euro.who.int/en/media-centre/
sections/press-releases/2018/measles-cases-hitrecord-high-in-the-european-region.
5. Kestenbaum LA, Feemster KA. 2015 Identifying
and addressing vaccine hesitancy. Pediatr. Ann.
44, e71 – e75. (doi:10.3928/0090448120150410-07)
6. Larson HJ. 2018 The biggest pandemic risk?
Viral misinformation. Nature 562, 309. (doi:10.
1038/d41586-018-07034-4)
7. Watts J. 2018 We have 12 years to limit climate
change catastrophe, warns UN. The Guardian, 8
October 2018. See https://www.theguardian.
com/environment/2018/oct/08/global-warmingmust-not-exceed-15c-warns-landmark-un-report.
8. Propaganda and practice. 2017 Human Rights
Watch 19 July 2017. See https://www.hrw.org/
reports/1999/rwanda/Geno1-3-10.htm.
9. Romano A. 2018 Twitter released 9 million
tweets from one Russian troll farm. Here’s what
we learned. Vox, 19 October 2018. See https://
www.vox.com/2018/10/19/17990946/twitterrussian-trolls-bots-election-tampering.
10. Brownell KD, Warner KE. 2009 The perils of
ignoring history: Big Tobacco played dirty and
millions died. How similar is Big Food? Milbank
Q. 87, 259 – 294. (doi:10.1111/j.1468-0009.
2009.00555.x)
11. Smith J, Thompson S, Lee K. 2017 Death
and taxes: the framing of the causes and
policy responses to the illicit tobacco trade in
Canadian newspapers. Cogent. Soc. Sci.
3, 1325054. (doi:10.1080/23311886.2017.
1325054)
12. Masood E. 2018 Why academic freedom is
needed more than ever. Nature 563, 621 – 623.
(doi:10.1038/d41586-018-07518-3)
13. Cillizza C. 2018 Donald Trump buried a climate
change report because ’I don’t believe it’. CNN,
27 November 2018. See https://edition.cnn.com/
2018/11/26/politics/donald-trump-climatechange/index.html.
14. Qiu L. 2018 Fact check of the day: the baseless
claim that climate scientists are ‘driven’ by
money. New York Times, 27 November 2018.
See https://www.nytimes.com/2018/11/27/us/
politics/climate-report-fact-check.html.
15. Harnad S. 1991 Post-Gutenberg galaxy: the
fourth revolution in the means of production of
knowledge. Public-Access Comput. Syst. Rev. 2,
39 – 53. See http://cogprints.org/1580/1/
harnad91.postgutenberg.html.16. Orwell G. 1943 Looking back on the
Spanish War. New Road, London. See
http://orwell.ru/library/essays/Spanish_
War/english/esw_1.
17. Keyes R. 2004 The post-truth era: dishonesty and
deception in contemporary life. New York, NY: St
Martin’s Press. (http://www.ralphkeyes.com/thepost-truthera/contents/).
18. Iyengar S, Massey DS. 2019 Scientific
communication in a post-truth society. Proc.
Natl Acad. Sci. USA 116, 7656 – 7661. (doi:10.
1073/pnas.1805868115)
19. Kofman A. 2018 Bruno Latour, the post-truth
philosopher, mounts a defence of science.
New York Times Magazine, 25 October 2018. See
https://www.nytimes.com/2018/10/25/
magazine/bruno-latour-post-truth-philosopherscience.html.
20. Department for Universities, Innovations
and Skills. 2008 Public attitudes to science.
See https://assets.publishing.service.gov.uk/
government/uploads/system/uploads/
attachment_data/file/260669/bis-08p111-public-attitudes-to-science-2008-
survey.pdf.
21. Funk C. 2017 Mixed messages about
public trust in science. Issues Sci.
Technol. 34. See https://issues.org/realnumbers-mixed-messages-about-public-
trust-in-science/.
22. Nichols T. 2017 The death of expertise: the
campaign against established knowledge and
why it matters. Oxford, UK: Oxford University
Press.
23. Siegel E. 2018 Science is not fake news. Forbes,
29 November 2018. See https://www.forbes.
com/sites/startswithabang/2018/11/29/scienceis-not-fake-news/#746df4d16f0c.
24. Matlin SA, Mehta G, Krief A, Hopf H. 2017 The
scientific publishing conundrum: a perspective
from chemistry. Beilstein Magazine 3. (doi:10.
3762/bmag.9)
25. Brainard J, You J. 2018 What a massive
database of retracted papers reveals about
science publishing’s ‘death penalty’. Science, 25
October 2018. (doi:10.1126/science.aav8384)
26. Eckert S, Sumner C, Krause T. 2018 Inside the
fake science factory. 2018 Presentation at DEF
CON 26, Las Vegas, 11 August. See https://
media.defcon.org/DEF%20CON%2026/DEF%
20CON%2026%20presentations/Svea%
20Eckert%20-%20Chris%20Sumner%20-%
20Tim%20Krause/DEFCON-26-Eckert-SumnerKrause-Inside-the-Fake-Science-Factory-
Updated.pdf
27. Lazer DMJ et al. 2018 The science of fake news.
Science 359, 1094 – 1096. (doi:10.1126/science.
aao2998)
28. Ciampaglia GL, Menczer F. 2018 Misinformation
and biases infect social media, both
intentionally and accidentally. The Conversation,
20 June 2018. See https://phys.org/news/
2018 – 06-misinformation-biases-infect-socialmedia.html.
29. Temming MT. 2018 Scientists enlist
computers to hunt down fake news. ScienceNews For Students, 27 September 2018.
See https://www.sciencenewsforstudents.org/
article/scientists-enlist-computers-hunt-downfake-news.
30. Gonzalez R. 2018 Facebook opens its private
servers to scientists studying fake news. Wired,
11 July 2018. See https://www.wired.com/story/
social-science-one-facebook-fake-news/.
31. Rone J. 2018 Collateral damage: how
algorithms to counter ‘fake news’ threaten
citizen media in Bulgaria. London School of
Economics and Political Science, Media Policy
Project, published online 18 June 2018. See
https://blogs.lse.ac.uk/mediapolicyproject/2018/
06/18/collateral-damage-how-algorithms-tocounter-fake-news-threaten-citizen-media-in-
bulgaria/.
32. UK Government. 2018 Social media and online
platforms as publishers. House of Lords Debate
Res. Briefing, 11 January 2018. See https://
researchbriefings.parliament.uk/
ResearchBriefing/Summary/LLN-2018-0003.
33. Levin ST. 2018 Is Facebook a publisher? In
public it says no, but in court it says yes. The
Guardian, 3 July 2018. See https://www.
theguardian.com/technology/2018/jul/02/
facebook-mark-zuckerberg-platform-publisherlawsuit.
34. Jankowicz N. 2018 Social media self-regulation
has failed. Here’s what Congress can do about
it. Medium, 29 November 2018. See https://
medium.com/@nina.jankowicz/social-mediaself-regulation-has-failed-heres-what-congress-
can-do-about-it-5b38b6bf9840.
35. World Economic Forum. 2018 Digital wildfires in
a hyperconnected world. Global risks 2013, 8th
edn. See http://reports.weforum.org/globalrisks-2013/risk-case-1/digital-wildfires-in-a-
hyperconnected-world/?doing_wp_
cron=1551978334.8959839344024658203125.
36. Kalsnes B. 2018 Fake news. Oxford Research
Encyclopedias: Communication published online
September 2018. (doi:10.1093/acrefore/
9780190228613.013.809)
37. CNeTS member provides expertise on
misinformation battle at AAAS conference.
CNetS 15 February 2019. See http://cnets.
indiana.edu/blog/tag/misinformation/.
38. What’s trending in fake news? Tools show what
stories go viral, and if bots are to blame. Science
Daily, 17 May 2018. See https://www.
sciencedaily.com/releases/2018/05/
180517163327.htm.
39. Vosoughi S, Roy D, Aral S. 2018 The spread of
true and false news online. Science 359,
1146 – 1151. (doi:10.1126/science.aap9559)
40. Rutjens BT. 2018 What makes people distrust
science? Surprisingly, not politics. Aeon, 28 May
2018. See https://aeon.co/ideas/what-makespeople-distrust-science-surprisingly-not-politics.
41. WhatsApp selects 20 teams to curb fake news
globally, including in India. NDTV News
Service, 13 November 2018. See https://www.
ndtv.com/india-news/whatsapp-selects-20teams-to-curb-fake-news-globally-including-
in-india-1946482.
#
#royalsocietypublishing.org/journal/rsos
R.Soc.open
sci.6:190161
742. Mohan N. 2018 Scientists hold key to winning
fight against ’fake news’. Phys. org, 25 May
2018. See https://phys.org/news/2018-05scientists-key-fake-news.html.
43. Bradley S, Beall A. 2017 The best science
accounts to follow on Instagram and Twitter.
Wired, 3 November 2017. See https://www.
wired.co.uk/article/the-best-scientists-to-followon-instagram-and-twitter.
44. Fox F. 2018 Scientists must keep fighting fake
news, not retreat to their ivory towers. The
Guardian, 3 September 2018. See https://www.
theguardian.com/science/blog/2018/sep/03/
scientists-must-keep-fighting-fake-news-notretreat-to-their-ivory-towers.
45. Tattersall A. 2018 In the era of Brexit and fake
news, scientists need to embrace social media.
The Conversation, 17 July 2018. See https://
theconversation.com/in-the-era-of-brexit-andfake-news-scientists-need-to-embrace-social-
media-100040.
46. Cochrane Australia’s response to ’fake science’ fish
oil claims. Cochrane Australia, July 2018. See
https://australia.cochrane.org/news/cochraneaustralias-response-fake-science-fish-oil-claims.
47. Tsipursky G. 2018 (Dis)trust in science: can we
cure the scourge of misinformation? Sci. Am. 5
July 2018. See https://blogs.scientificamerican.
com/observations/dis-trust-in-science/.48. Center for Science and Democracy. 2019 Attacks
on science. Union of Concerned Scientists, March
2019. See https://www.ucsusa.org/centerscience-and-democracy/attacks-on-science.
49. Padma TV. 2019 Indian scientists protest against
unscientific claims made at conference. Nature
565, 274. (doi:10.1038/d41586-019-00073-5)
50. Scheufele DA, Krause NM. 2019 Science
audiences, misinformation, and fake news. Proc.
Natl Acad. Sci. USA 116, 7662 – 7669. (doi:10.
1073/pnas.1805871115)
51. Nehru J. 1946 The discovery of India, p. 512.
New York, NY: John Day.
52. Tattersall A. 2018 New research must be better
reported, the future of society depends on it.
The Conversation, 8 January 2018. See https://
theconversation.com/new-research-must-bebetter-reported-the-future-of-society-depends-
on-it-87407.
53. Brainard J, You J. 2018 What a massive
database of retracted papers reveals about
science publishing’s ‘death penalty’. Science,
25 October 2018. See http://www.sciencemag.
org/news/2018/10/what-massive-databaseretracted-papers-reveals-about-science-
publishing-s-death-penalty.
54. Impey C. 2018 Alarmed by fake news? Fake
science is just as serious of a problem. Salon, 27
November 2018. See https://www.salon.com/2018/11/27/alarmed-by-fake-news-fake-scienceis-just-as-serious-of-a-problem/.
55. Ravindranath P. 2017 India’s white list to curb
researchers from publishing in predatory
journals. Science Chronicle, 16 January 2017. See
https://journosdiary.com/2017/01/16/india-ugcpredatory-journals/.
56. Fryling-Indiana K. 2018 Scientists call for action
in fight against ‘fake news’. Futurity, 9 March
2018. See https://www.futurity.org/fightingfake-news-1699352/.
57. Maslog C. 2017 How can scientists fight the
tide of ‘fake news’? Open University,
2 November 2017. See https://www.open.edu/
openlearn/science-maths-technology/acrossthe-sciences/how-can-scientists-fight-the-tide-
fake-news.
58. Bourguignon J-P. 2018 Scientists can lead the
fight against fake news. World Economic Forum
Annual Meeting of the New Champions, 16
September 2018. See https://www.weforum.
org/agenda/2018/09/scientists-can-lead-thefight-against-fake-news/.
59. Cook J. 2016 Countering climate science
denial and communicating scientific
consensus. Oxford Research Encyclopedias:
Climate Science published online October
2016. (doi:10.1093/acrefore/9780190228620.
013.314)
Journal of Strategic
Security
Volume 12 | Number 1 Article 4
Halting Boko Haram / Islamic
State's West Africa Province
Propaganda in Cyberspace with
Cybersecurity Technologies
Sunday O. Ogunlana
Walden University, abovejordan@gmail.com
Follow this and additional works at: https://scholarcommons.usf.edu/jss
pp. 72-106
This Article is brought to you for free and open access by the Journals at Scholar Commons. It has been
accepted for inclusion in Journal of Strategic Security by an authorized editor of Scholar Commons. For more
information, please contact scholarcommons@usf.edu.
Recommended Citation
Ogunlana, Sunday O.. "Halting Boko Haram / Islamic State's West Africa
Province Propaganda in Cyberspace with Cybersecurity Technologies."
Journal of Strategic Security 12, no. 1 (2019) : 72-106.
DOI: https://doi.org/10.5038/1944-0472.12.1.1707
Available at: https://scholarcommons.usf.edu/jss/vol12/iss1/4
#
#Halting Boko Haram / Islamic State's West Africa
Province Propaganda in Cyberspace with
Cybersecurity Technologies
Author Biography
Sunday Oludare Ogunlana, Ph.D., is a counterterrorism policy analyst,
and currently a member of the Security Incident Management Team for
CitiGroup, as a Senior Security Analyst in Irving, Texas. Previously, he
was Group Managing Director of UESIRI Security Service Limited,
Abuja, Nigeria. He has served on numerous boards, including Council for
African Security Affairs (CASA) and Editorial of African Journal for
Counterterrorism. He is the initiator of African Security Forum and
Annual Symposium on Counterterrorism that holds at Kofi Annan
International Peace Keeping Training Center (KAIPTC) every year. He
was Security Advisor to the Chairman of Nigeria Presidential Amnesty
Council (PAC) between the year 2012 to 2015.
Abstract
Terrorists use cyberspace and social media technology to create fear and
spread violent ideologies, which pose a significant threat to public
security. Researchers have documented the importance of the application
of law and regulation in dealing with the criminal activities in
cyberspace. Using routine activity theory, this article assessed the
effectiveness of technological approaches to mitigating the expansion
and organization of terrorism in cyberspace. Data collection included
open-source documents, government threat assessments, legislation,
policy papers, and peer-reviewed academic literature and semistructured
interviews with fifteen security experts in Nigeria. The key findings were
that the new generation of terrorists who are more technological savvy
are growing, cybersecurity technologies are effective, and bilateral/
multilateral cooperation is essential to combat the expansion of terrorism
in cyberspace. The data provided may be useful to stakeholders
responsible for national security, counterterrorism, law enforcement on
the choice of cybersecurity technologies to confront terrorist expansion
in cyberspace.
This article is available in Journal of Strategic Security:
https://scholarcommons.usf.edu/jss/vol12/iss1/4
#
# 
72 
 
Introduction  
 
The Nigeria government has spent a significant amount of money to 
protect its cyberspace, including telecommunication infrastructures from 
terrorist attack. In addition, the government has given considerable 
attention to cybercrimes such as terrorists’ online financing and 
fundraising activities. However, these efforts have in the past failed to 
recognize the threats presented by terrorist propaganda in Nigerian 
cyberspace. Cyberspace is increasingly becoming the platform to promote 
terrorism because it allows easy access to actors to disseminate 
information beyond geographic borders. The internet and social media 
create alternative realities for actors, and audiences and users are 
influenced by the information given to them by strangers.1 Terrorists’ use 
of websites, blogs, YouTube, Twitter, and Facebook to influence people is 
on the rise globally, and the security community has not developed 
systematic measures to mitigate terrorist activities such as the 
manipulative use of new channels to influence the public.2 As Boko 
Haram, a Nigerian militant Islamist group has embraced technology for 
spreading violent religious ideology and hate messages, raising money, 
conspiring, planning, and executing their attacks,  the Nigerian 
government has just begun to incorporate robust active and passive 
defense measures against adversaries into the National Security Strategy.3 
 
Terrorist propaganda and networking in the Nigerian cyberspace give rise 
to the question of what measures that Nigerian government agencies 
should take to mitigate the effects of terrorist propaganda in in the society. 
Cyberspace has become a new battleground with governments all over the 
world in search of a solution with adequate cyber intelligence to confront 
and destabilize terror infrastructures. Information communication 
technology (ICT) is a new tool of attack in the hand of terrorist 
organizations. Indeed, terrorism is about information. The 21st-century 
terrorists are acquiring technological skills that enable them to engage in 
extremely destructive acts such as cyberterrorism, the spread of new 
doctrines and falsehood, blackmail, and exploitation, undergirded by 
extreme religious ideologies that are currently affecting the spectrum of 
conflict. Terrorists’ chief motive is to use fear to compel their targets to 
Ogunlana: Halting Boko Haram / Islamic State's West Africa Province Propaga
Produced by The Berkeley Electronic Press, 2019
#
# 
73 
 
comply with their demands or ideologies.4The present danger is that 
modern-day terrorists have transformed Information communication 
technology (ICT) into tools of attack with weaponized information. 
Terrorist websites serve as the virtual training ground that host messages 
and propaganda videos that help to boost morale and networking, drive 
fundraising efforts, recruitments, and call for terror actions. Kaplan 
pointed out that terrorist websites moved from fewer than 100 to 4,800 
between 2006 and 2008.5 The organizations attract attention by posting 
roadside bombing, and a significant portion of society views the 
decapitation of hostages and terror propaganda videos. In fact, some 
jihadist websites have video games where users pretend to be holy 
warriors killing government soldiers. Therefore, cyberterrorism has 
become a new focus because of the technology’s interface functionality, 
which makes it simple and efficient to accomplish terrorists’ goals. In 
Nigeria, terrorist organizations have shifted the battle to cyberspace, using 
social media platforms to coordinate attacks, communicate, and spread 
messages of hate and violent religious ideology. Research supports the 
notion of governing cyberspace using traditional models of law 
enforcement, including the enactment of legislation to deal with 
cybercrime, including other related offenses.6 In addition, the literature 
suggests that the future of fighting extremism, falsehoods, and bogus 
information in cyberspace depends on the deployment of robust 
technology.7 There is little information on how the experts make their 
choice of cybersecurity technologies that can be used effectively to halt the 
expansion of terrorism in cyberspace and the extent to which the 
government should use the expertise. Different countries, including 
Nigeria, used several kinds of technologies to mitigate the effect of the 
terrorists’ harmful messages in cyberspace. Hence, a need exists for a 
study to understand the effectiveness of  cyber technologies and the 
choices security experts and administrators make. This study strives to 
advance security research by integrating findings from experts and 
security administrators to improve understanding of how they choose 
cybersecurity technologies to mitigate terrorist propaganda and 
networking in Nigeria cyberspace. This qualitative interview study focused 
on experts from five sectors (Law enforcement, Intelligence, Military, 
Academic, and private sector) in Nigeria. Using a qualitative interview 
with 15 participants, I obtained individual experts’ perceptions about the 
Journal of Strategic Security, Vol. 12, No. 1
https://scholarcommons.usf.edu/jss/vol12/iss1/4
DOI: https://doi.org/10.5038/1944-0472.12.1.1707
#
# 
74 
 
effectiveness and integration of cybersecurity technologies with 
counterterrorism strategy to stop terrorists’ propaganda campaign and 
networking in Nigeria cyberspace. I collected empirical data on the 
situational factors, the thought, and the decision-making processes of 
experts by performing secondary data analysis and undertaking in-depth 
and intensive in-person interviews with known security experts. I 
examined the phenomenon through interviews with key public and private 
sector individuals. In addition, I reviewed relevant documents such as 
legislation, executive orders, research papers, and transcripts of public 
speeches by government officials.  
 
Assessing effectiveness and practicality of cybersecurity technologies in 
mitigating terrorists’ networking and propaganda in Nigeria’s cyberspace 
based on perceptions of experts enabled me to identify areas where 
improvements are required. The potential social implication of this study 
is that the outcome of the research will help public law enforcement and 
intelligence community in Nigeria to build capacity, using relevant 
cybersecurity technologies to confront a series of cyber threats, especially 
terrorist propaganda. In this article, I highlighted the report of this study, 
beginning with the background, routine activity theory discussion on 
terrorist propaganda, cyberterrorism, and terrorist use of social media 
technology. The study closes with a conversation on the interpretations of 
findings, policy recommendations, and conclusion. 
 
Routine Activity Theory 
 
The theoretical framework for this study is Cohen & Felson's (1979) 
routine activity theory, which is an environmental, place-based account 
that three elements must be available for a crime to occur. There must be 
an opportunity, motivation, and vulnerable platform. The role of 
criminology theory such as Cohen and Felson’s (1979) RAT in 
cybersecurity remains open for discussion. The applicability of traditional 
criminological theory to the study of cyber-related crimes is still in 
contention in the scholarly community. Several scholars advocate for the 
development of new criminology theories because the cyberspace 
environment is a new challenge to criminologists and represents new 
criminality.8 However, Peter Grabosky posits that the underlying 
Ogunlana: Halting Boko Haram / Islamic State's West Africa Province Propaga
Produced by The Berkeley Electronic Press, 2019
#
# 
75 
 
mechanism of criminality in cyberspace is the same as for real-world 
crimes.9 The motivation of computer criminals is not new in the sense that 
they are driven by greed, lust, power, revenge, and adventure.10 Moreover, 
Foley argued that routine activity is a resource that has been mostly 
untapped by students of counterterrorism.11  
 
Routine activities of everyday life present opportunities for crime to occur. 
Cohen and Felson found that most criminal acts require convergence in 
space and time, which implies that circumstances must be right for 
criminal activities to take place.12 For my study, the argument was based 
on the structural changes in routine activity patterns that can influence the 
rate of terrorist activities in cyberspace by altering the convergence in 
range and time of the three minimal elements of direct contact predatory 
violations. RAT requires three situations to be right and happen in space 
in order for criminal activities to transpire. These are motivated offenders, 
proper targets, and the absence of protectors against violations. Most 
crime prevention practice is based on an actor’s choice. RAT draws on the 
rational exploitation of “opportunity: in the context of the regularity of 
human behavior to design prevention strategies. Therefore, it assumes 
that criminals are reasonable when there is a capability to operate in the 
context of attractive high-value targets with weak protection. 
Journal of Strategic Security, Vol. 12, No. 1
https://scholarcommons.usf.edu/jss/vol12/iss1/4
DOI: https://doi.org/10.5038/1944-0472.12.1.1707
#
# 
76 
 
Figure 1. Application of Routine Activity Theory 
 
Source: Author. 
 
Ogunlana: Halting Boko Haram / Islamic State's West Africa Province Propaga
Produced by The Berkeley Electronic Press, 2019
#
# 
77 
 
Methodology 
 
The study collected data holistically. The data collection includes face-toface
interviews with security experts, document content analysis of open 
source, non-classified materials; government threat assessment; 
Legislative Report on Terrorism; policy papers, peer-reviewed academic 
works, and journals. The study was qualitative research interviews, which 
was designed to find answers to how experts see the effectiveness of 
cybersecurity technology to mitigate terrorist propaganda and networking 
in Nigeria cyberspace. Qualitative research supports the “purposive” 
selection of key informants in the field who can assist in identifying 
information-rich cases.13 With qualitative research interviews, the study 
focused on key players on how Nigerian government manages the 
terrorists networking and propaganda in cyberspace. The study examined 
experts’ selection strategy and perceptions about the effectiveness and 
practicality of cybersecurity technologies. There were collections of 
multiple forms of data in a natural setting through semi structured indepth
face-to-face interviews. In addition, I established a protocol for 
recording this information. Rubin and Rubin posited that the qualitative 
interview is appropriate when the study purports to answer “how” and 
“why” questions.14 Hence, the study gathered the needed data through a 
qualitative interview through which it assessed the effectiveness of 
cybersecurity technology from the professional experiences and 
perspectives of experts: which included law enforcement agents, 
intelligence personnel, cybersecurity experts, government officials, and 
private security administrators. Moreover, a qualitative method allows the 
researcher to view issues through a variety of lenses, which allow for 
multiple phases of any phenomena to be exposed and understood. Data 
were organized by research questions. This organization was done by 
sorting the data collected from document analysis and the in-depth 
interviews into an articulated format to infer causal links and connection 
of findings. 
 
Understanding Cyberterrorism 
 
The term cyberterrorism is a compound of the words cyberspace and 
terrorism.15 Cyberterrorism refers to a computer-aided terror act. 
Journal of Strategic Security, Vol. 12, No. 1
https://scholarcommons.usf.edu/jss/vol12/iss1/4
DOI: https://doi.org/10.5038/1944-0472.12.1.1707
#
# 
78 
 
Cyberterrorism is a politically motivated attack that generates fear or 
harm. It should result in violence or threat of violence against persons or 
properties. The Federal Bureau of Investigation (FBI) describes 
cyberterrorism as any criminal activities perpetrated with the aid of 
computer systems and telecommunication networks with the intention of 
provoking violence, including destruction and disruption of services. 
Hence, the terrorists’ agenda is to create excessive fear because of 
confusion and dilemma within a given group or community, with the goal 
of coercing a government or population to conform to their political, 
social, or ideological demand.16 The Department of Homeland Security 
(DHS) stated that terrorism is:  
 
. . . [An] activity that involves an act that: is dangerous to human 
life or potentially destructive of critical infrastructure or key 
resources; and . . . must also appear to be intended to (i) intimidate 
or coerce a civilian population; (ii) influence the policy of a 
government by intimidation or coercion; or (iii) affect the conduct 
of a government by mass destruction, assassination, or 
kidnapping.17  
 
Robert Murril posited that the term could be misleading, and the response 
could be based on who is defining it.18 “Cyberterrorism” means various 
things to different people, depending on the actors, which may result in 
different responses. Cyber terrorists are cyber actors or groups with a 
direct or indirect association with a formally recognized terrorist group. 
They often use a threat of violence to instill fear in general populations or 
victims in order for the targets to comply with their demands or ideology. 
This set of action in cyberspace falls within the definition of 
cyberterrorism and will result in counterterrorism responses. 
Transnational terrorists use online tactics like “cyber-mobilization,” and 
computer malware as economic weapons.19 Terror groups around the 
world, including Boko Haram in Nigeria, use technology to spread violent 
ideology and recruit members. Early research asserted that the use of the 
Internet to communicate, coordinate events and actions does not 
necessarily constitute cyberterrorism.20 Eliot Che points out that 
cyberterrorism is hard to define—just like terrorism; however, a cyber 
Ogunlana: Halting Boko Haram / Islamic State's West Africa Province Propaga
Produced by The Berkeley Electronic Press, 2019
#
# 
79 
 
hacktivist is different from a terrorist who uses technology to propagate 
radical ideology, hate, and violence.21  
 
Nevertheless, Al Mazari et al. developed cyberterrorism taxonomies to 
include cyber-attacks against social and national identity.22 Their study 
observed a number of acts that were deemed cyberterrorism. These 
included the defacement of government and organization websites, the 
spread of false rumors, violence, hate messages, and misrepresentation 
against a social target and entities, using social media technologies. 
 
Meanwhile, the literature often uses the term cyberterrorism to describe 
terrorists’ online activities, including communication and spread of 
propaganda.23 Moreover, Martin and Weinberg bridged the gap between 
academic thought on the areas of terrorism and mass political violence, 
taking time to explore and develop accepted definitions of many terms in 
the field.24 The study suggests that terrorist organizations engage in cybermobilization
and the use of computer malware as economic weapons. The 
study proposes that the method to stop cyberterrorists should encompass 
non-state and non-military actors and the need in which academic thought 
and theory can catch up to the realities of modern-day warfare. 
 
However, Goodman Seymour suggested that the concerns should be about 
what terrorist organizations will most likely do in cyberspace, which is to 
support their activities and infrastructure, and one of those activities is 
propaganda.25 Terrorist organizations use cyberspace for several things, 
including the spread of their ideology, promotion of violence, 
indoctrination of adherents, recruitment of members, perpetration of 
crimes, and misrepresentation to cause fear and panic.26 
 
Benson David has a different theme as the study argues that as terrorists 
have increased their use of the Internet, state security organs have far 
outpaced them, leading to a much less dramatic rise in cyberterrorism 
than is currently thought. The study suggests that a significant amount of 
terrorist activity is fundamentally a local endeavor and that local 
initiatives do not benefit from better access to transnational 
communications devices. As for transnational terrorism, such non-local or 
non-regional initiatives inherently draw support from a non-local base, 
Journal of Strategic Security, Vol. 12, No. 1
https://scholarcommons.usf.edu/jss/vol12/iss1/4
DOI: https://doi.org/10.5038/1944-0472.12.1.1707
#
# 
80 
 
and could therefore better benefit from a transnational support system 
buoyed by a transnational (and often anonymous) communications 
system.27 It has been consistently assumed in the standard literature that 
the Internet facilitates transnational terrorism—in particular with the 
influence of anonymity, abundance of information, and the inexpensive 
nature of online communications.  
 
Minei and Matusitz share similar thoughts and have discovered that such 
communications networks and social media allow groups to form, and the 
spread of vital information tolerates individuals (lone wolves) who are 
influenced by the terrorist messages to take action and attack their 
homeland.28 Meanwhile, researchers agreed and aligned with the previous 
sentiment that cyberterrorism remains a communicative process because 
both intentional and clandestine communications between cyberterrorists 
and their targets occur through different modes of propaganda.29 For 
instance, ISIS supporters used social media to call on their fellow 
terrorists to poison food in grocery stores across Europe and the United 
States of America.30 The group posted the graphic message through the 
encrypted messaging application Telegram, a platform favored by cyber 
jihadists to disseminate information to members while maintaining 
secrecy and privacy. 
 
Benson suggests that in the same way, governments can also disseminate 
information in support of their interests.31 Anonymity, thought to be a 
benefit to terrorists, can also serve to mask surveillance efforts and 
facilitate counter-surveillance. This study suggests that Internet 
anonymity is incorrectly assumed and that state-based organizations have 
ample resources to push through this assumed anonymity; monitor 
groups, and set up sting operations to catch users. Furthermore, the 
increased information available to terrorists may not be accurate, may lack 
filters, and may lead to an inability to make clear decisions. The research 
posits that just because a person has access to cookbooks does not make 
them a master chef able to put a gourmet dinner on the table. In the same 
way, access to terroristic information does not make a person a terrorist or 
bring destructive acts to the national stage. Benson asserted that Al Qaeda 
was dangerous and more powerful before the Internet. The study suggests 
that homegrown terrorism and an analysis of Al Qaeda in pre- and postOgunlana:
Halting Boko Haram / Islamic State's West Africa Province Propaga
Produced by The Berkeley Electronic Press, 2019
#
# 
81 
 
Internet periods gave a clear idea of how terrorists might use cyberspace to 
their advantage as additional weapons.32 The study determines that the 
Internet is a tool for civilization rather than chaos, and extends that idea to 
various local situations in Africa in which access to cyber resources did not 
coincide with an increase in terrorist actions. 
 
Terrorist Propaganda 
 
One of the essential terror instruments is media propaganda; there is no 
doubt that any terrorist operation without the media has limited effects on 
the targeted audience. The invention of social media technology, which 
enables terrorists to bypass middlemen before reaching their audience, is 
an added advantage for such groups. One significant objective of terror 
groups is to get maximum publicity for their terrorist activities. Instead, 
the devastating effect would be restricted to the immediate victims of their 
dastardly deeds. In the Global Terror Threat and Counterterrorism 
Challenges Facing the Next Administration, Hoffman Bruce argues that 
terrorism and media are joined together in an intrinsically symbiotic 
relationship, each feeding off, and utilizing the other for its purposes.33The 
terrorist always wants to communicate the revolutionary or divine 
messages to a broad audience, and the group has recognized the potential 
of new mass communication technology. 
 
The Internet and the media technology through social media platforms has 
been a useful tool for terrorist organizations. The power of images blended 
with text can cause panic and influence public opinion on major issues. 
Pictures of violence have a reasonable influence on the public and the 
policy maker and thereby affect both domestic and foreign policies. ISIS 
remains the most potent terrorist organization in modern history, 
possessing sophisticated cyber capability. ISIS recruits young jihadists 
using over 21 languages over the Internet. The group is using YouTube 
videos, memes, tweets and other social media postings and flooding 
cyberspace for their sympathizers to retweet, like, or endorse their 
materials to recruit members into their folds.34 In addition, Jack Moore 
uncovered that ISIS collaborated with other terrorist groups like Boko 
Haram to spread its messages and provided cyber and media training to 
them.35 Through this partnership, Boko Haram was exposed to and 
Journal of Strategic Security, Vol. 12, No. 1
https://scholarcommons.usf.edu/jss/vol12/iss1/4
DOI: https://doi.org/10.5038/1944-0472.12.1.1707
#
# 
82 
 
subsequently developed new tactics, and was provided with symbiotic 
relationships with other groups through which the Boko Haram message 
could be propagated. The mutual relationship between the two groups 
granted Boko Haram unfettered access to Al Qaeda’s Al-Andalus media 
arm, which assists in the area of the propaganda campaign. 
 
In the present media age, terror organizations have discovered social 
media technology as an extra and vital weapon in the sustenance of their 
struggle. In the past, particularly during the Cold War era, terrorist 
organizations could only depend on three primary communication 
techniques: secret rebel radio stations, clandestine publications such as 
posters and handbills, and traditional public media agencies, including 
state-owned mass media. However, the new media age has afforded terror 
organizations further opportunities to control their self-media propaganda 
machines.  
 
Bruce Hoffman proposed counterterrorism measures such as denial of the 
enemy, cyber sanctuary, and the elimination of terrorist resources that 
enable the group to conduct cyber mobilization and recruitment.36 
Moreover, the creation of a secure environment, including comprehensive 
and integrated information operation (IO) are critical factors to consider 
for counterterrorism operations. 
 
Terrorists’ use of Social Media Technology 
 
It appears that terrorists are shifting to cyberspace with every device 
becoming a battleground with the aid of social media technology. Research 
suggests that social media technology is the major tool used by terror 
organizations to recruit new members and spread their propaganda.37 In 
today’s world, social media outlets have become part of daily life with 
terrorists using these media to send messages of fear. The ways in which 
terrorists disseminate information to spread hate and violent religious 
messages to radicalize young people have assumed new dimensions in the 
last 10 years. Experts have pointed out that Twitter was the most popular 
platform among terrorist organizations. The British Jihadis working for 
ISIS in Syria threaten the United States of America, using Twitter.38 The 
attack on July 26, 2016 in France, where terrorists took nuns and 
Ogunlana: Halting Boko Haram / Islamic State's West Africa Province Propaga
Produced by The Berkeley Electronic Press, 2019
#
# 
83 
 
worshipers hostage and slit the throat of an 85-year-old priest, is a point of 
reference. The investigation established the fact that the two attackers 
involved were directed and stimulated by ISIS propagandist Rachid 
Kassim through an encrypted chat room on the digital application 
Telegram.39 Social media has fueled the recent upsurge of lone wolf 
terrorism around the world.40  
 
The ISIS has a significant influence on most of the terrorist organizations 
operating from Africa, including the notorious Boko Haram (Jamā’a Ahl 
al-sunnah li-da’wa wa al-jihād), a Sunni group preaching religious 
extremism and Jihad in Nigeria. The group renamed itself as “Islamic 
State’s West Africa Province” (ISWAP) in April 2015. The ISIS has taken 
over the propaganda function of Boko Haram against the Nigerian State, 
using advanced technology such as encrypted media such as Telegram to 
pass messages among members about clandestine operations. The group is 
using social media to recruit members and raise money for its activities, 
including the spread of violent ideologies.41 The organization uses 
YouTube to broadcast its activities as a way of threatening people with 
messages of fear to force the Nigerian government to concede to its 
demands. For instance, Boko Haram used YouTube to announce the 
abduction of more than 276 schoolgirls in 2014. The group usually uses 
YouTube videos to distribute jihadist sermons in northern Nigeria, calling 
people to deny girls modern education because women are slaves 
according to their ideology.42How terrorists use social media to perpetrate 
its agenda points to the future. Lohrmann found that ISIS has been using 
the Internet successfully to recruit new fighters through new media 
technology such as Facebook, Twitter, YouTube, and Telegram.43 The 
study suggests that social media technology is the central tool to spread 
hateful and violent messages. Attention-seeking terrorists have been using 
social media in new ways to reach out to mass audiences with their 
message. 
 
Moreover, electronic jihad via strategic messaging and communication has 
become the manifestation of modern-day terror with the use of online 
media technologies to disseminate sophisticated multidimensional 
information. Liang discovered that the application of social media 
technology such as Twitter, Facebook, Instagram, Telegram, and 
Journal of Strategic Security, Vol. 12, No. 1
https://scholarcommons.usf.edu/jss/vol12/iss1/4
DOI: https://doi.org/10.5038/1944-0472.12.1.1707
#
# 
84 
 
Chatroom facilitated communication and coordination at a global level 
outside of the control of governments.44 The increased connectivity 
created challenges that traditional law and international agreements could 
not easily resolve. Researchers have argued that the Internet frontline 
needs a proactive defense because censorship and the removal of terrorist 
content are reactive and not effective. 
 
Furthermore, in a research paper presented by former FBI Director of 
Intelligence and Counterintelligence at the13th Annual Conference of the 
International Association for Intelligence Education, the study examined 
why terrorists choose social media platforms. The research found that 
terrorists prefer this dynamic because it is hard to stop the spread of 
online misinformation.45  
 
In addition, a report titled “The evolution of terrorist propaganda: The 
Paris attack and social media” by the U.S. House Committee on Foreign 
Affairs’ Subcommittee on Terrorism, Nonproliferation and Trade 
questioned why social media companies would allow terrorist content on 
their platforms. The report found that Twitter remains the favorite and 
most widely used platform by terror organizations while Facebook has 
become the terrorist favorite platform to share photos on message 
boards.46 The report claimed that among social media companies, Twitter 
is far worse than the rest with regard to acting proactively to track and 
remove terrorist content. The committee’s report discovered that the ISIS 
is using new technology to escape detection and the eventual removal of its 
content when posted on YouTube. The ISIS uses a service known as 
“Vimeo” to post graphic violence. YouTube tried but did not succeed in 
removing them all. Among the counter radicalization strategies, that the 
White House published in 2011 was the commitment to devise means to 
deal with the new threat, including the use of intelligence led strategy, 
which consists of right resources, tools, and process to defend against 
cyberterrorism.47 What is not known is how the technology will be 
deployed and how effective it will be. Meanwhile, a coalition of top 
technology companies in the United States is making efforts to curb 
terrorists’ use of social media technology with the use of Artificial 
Intelligence (AI). Korolov explained that AI-based security applications 
can read and understand security — they can analyze every incident, 
Ogunlana: Halting Boko Haram / Islamic State's West Africa Province Propaga
Produced by The Berkeley Electronic Press, 2019
#
# 
85 
 
identify causes, methods, and trends, and predict the next pattern even 
before it happens.48 For instance, IBM developed Watson, which has been 
taught to read vast quantities of information online.49 Watson provides 
smart data analysis and visualization services, which makes it easy to 
detect patterns. It has an inbuilt capability that enables the user to interact 
with data in a conversation with a response the user can easily understand. 
In some countries of Global North, the Law Enforcement and intelligence 
agencies are using AI and machine learning to detect and respond to 
different kinds of cyber threats, including cyberterrorism. There are 
different kinds of cybersecurity tools available, but the user or 
organizations must know how to apply them and integrate them into the 
broader cybersecurity strategy. Isaac found that there is an ongoing 
project to create a shared digital database which includes “fingerprinting” 
or patterns of all suspicious terrorist content that raise red flags.50 This 
inter-company collaboration will ensure that content that has been flagged 
on Twitter will not appear on Facebook or another social media platform. 
 
Another aspect that creates challenges is the capability of cyberterrorists 
to remain elusive while perpetrating their act. Schultz argues that the 
ability to operate undetected while using online tactics makes terror 
groups real beneficiaries of cyberspace technology.51 Terrorist enjoys the 
anonymity the cyberspace provides them. Stealth is the most significant 
advantage of the Internet. Kaplan pointed out that terrorists swim in the 
ocean of bits and bytes, which make it difficult to identify the real culprits 
or bad actors.52 The secure means of communication through encryption 
tools, steganography, dead dropping (transmitting information through 
saved draft emails in an online email account, to anyone with the 
password) makes them elusive. The study suggests that the utilization of 
social media outlets gives terror organizations a global reach, and enables 
them to mobilize new members and instill loyalty among their followers 
through constant and clear communication. Terrorists have embraced 
cybertechnology, which empowers them to decentralize their activities and 
makes it hard to target them through conventional military capability. 
Current efforts to diminish online terrorist operations, including the 
spread of messages of violence are inadequate. The study suggests that 
there is a need for an innovative strategy to deal with online threats from 
cyberterrorists. There is no doubt that terrorists are susceptible to 
Journal of Strategic Security, Vol. 12, No. 1
https://scholarcommons.usf.edu/jss/vol12/iss1/4
DOI: https://doi.org/10.5038/1944-0472.12.1.1707
#
# 
86 
 
deception and failure in cyberspace just like the same protection that the 
cyber technology offers the group.53 
 
In Schultz’s study, it recommended false-flag operations (FFO) as one 
option to tackle terrorists’ online activities. FFO is a military deception 
method originated from naval warfare. The researcher found that FFO 
could be used to compromise terrorist narratives on social media 
platforms so that extremist groups will grow to distrust their websites 
because their ideological messages will be altered to deviate from their 
approved narratives. What is unknown is how effective FFO could be in 
mitigating this risk. 
 
Terrorists and Cybercrime 
 
There is a thin line between cybercrime and cyberterrorism as terrorists 
engage in both activities. Terrorists uses cyberspace to coordinate terrorist 
activities, which is regarded as cyberterrorism. Terror organizations 
manipulate the public, spread messages of hate and violence, and recruit 
members in cyberspace with the aid of social media technologies in 
furtherance of their agenda. They also inspire individual “lone wolves” to 
commit acts of terror against their homeland on their volition. In addition, 
terror organizations engage in cybercrime such as identity theft, hacking, 
extortion, phishing, and money laundry to fund their terrorist operations. 
Acts of violence against computer networks and the use of the social media 
technology to perpetrate violence can be regarded as cybercrime. It is 
important to make the distinction that not all cybercrimes are terrorist 
crime. It depends on the factors and the intention of the threat actors, 
which may fall within the definition of cybercrime or cyberterrorism. Holt 
corroborated several research arguments that there is no single accepted 
definition for both cyberterrorism and cybercrime. Holt’s study pointed to 
a framework with four distinct categories of cybercrimes, which are cyberpass,
breaches of computer networks and system boundaries, cyber 
deception, and cyber violence.54 The study found that the problem with 
defining cyberterrorism lies in distinguishing these acts from cybercrimes. 
Hence, the interconnectivity enabled by the Internet empowered the 
attackers to target their audience to create emotional harm or commit 
crime through identity theft, illegal gambling, money laundering, hacking 
Ogunlana: Halting Boko Haram / Islamic State's West Africa Province Propaga
Produced by The Berkeley Electronic Press, 2019
#
# 
87 
 
and cyber exploitation, and the distribution of child pornography. 
Terrorists are known to engage in the act of expropriation by robbing 
public institutions like banks, offices, businesses, and citizens to finance 
terror activities either through physical or virtual means.  
 
Adomi and Igun described cybercrime is an illegal act, which is carried out 
with the use of computers and computer networks. It involves the 
interruption of network traffic, denial of services, the creation, and 
distribution of malware, extortion, impersonation, and the distribution of 
child pornography.55 There is a thin line between cybercrime and 
cyberterrorism, which causes the media and researchers to use both terms 
interchangeably in many instances. Researchers distinguish the two based 
on actors, motives, and targets. Cybercriminals launch attacks for personal 
financial gain while cyberterrorists are driven by motives such as political 
change, ideology, religion, vengeance, or social change. 
 
A cyberterrorist is an actor who launches attacks to intimidate a 
government or a public in order to advance ideological, political or social, 
religious objectives. Terror organizations use cyberspace, especially social 
media technology to prepare, participate in, and coordinate terrorist 
agendas. Sageman argued that modern-day jihadists are self-recruited 
with the support of the Internet where they are able to locate their 
comrades on the cyber web.56 For instance, many of ISIS’ foreign fighters 
were recruited through social media platforms.57 In Nigeria, Boko Haram 
gained unauthorized access to the Nigeria Secret Police, popularly known 
as State Security Services (SSS) to obtain vital identities of government 
officials to target them for terror attacks.58 
 
Although terrorists tend to engage in cybercrimes like identify theft, online 
fraud, phishing scams, and cyber extortion to raise money to support their 
operations, cybercriminals differ in that they do not participate in those 
activities to promote ideological, religious or social change. While 
cyberterrorists are driven by political or ideological agendas, hacktivists 
mission are to draw attention for ideological cause or to express opinion 
though cyber protest or activist agenda. Cybercrime is a crime of 
opportunity where an individual seeks to gain personal benefit from the 
proceeds of crimes. Cyberterrorists are disciplined, trained, and 
Journal of Strategic Security, Vol. 12, No. 1
https://scholarcommons.usf.edu/jss/vol12/iss1/4
DOI: https://doi.org/10.5038/1944-0472.12.1.1707
#
# 
88 
 
committed actors who are motivated by ideology, religion, or political 
agendas. In addition, nation state can engage in cyberterrorism against 
another nation through information operation or can participate in 
cybercrime to steal proprietary information or trade secret from another 
country. 
 
Terrorist organizations are most likely to use cyber weapons than nationstate
actors. The most obvious way that the terrorists have been using 
cyber technology is for communication and planning. The group discusses 
in the open, using social media platform and coordinates their secure 
conversation with the encryption technology. Fink, Pagliery, and Segall 
pointed out that this method of secretive communication, which is known 
as “going dark,” remains one of the significant challenges facing 
intelligence community, police, and counterterrorism officials all over the 
world now.59 
 
Moreover, one of the most apparent differences between cybercriminals 
and cyberterrorists is their motives. The primary goal of cybercriminals is 
to commit a crime of opportunity and stay hidden to enjoy the proceeds of 
their exploits. Terrorists want to spread messages of hate and want their 
content to go viral to create fear and uncertainty. 
 
Table 1. Difference between Cyberterrorists and Cybercriminals 
Terrorists (Motives and methods) Criminals (motives and methods) 
Ideology, religion political Financial or personal gain 
Psychological warfare Data mining: identify theft, credit 
card scam 
Publicity, propaganda, and 
information sharing 
Espionage or competitive 
advantage 
Recruitment and 
training/networking 
Fun, curiosity, or pride 
Fundraising, money laundering Grudge or personal offense 
Data mining Money laundering, fraud 
Planning and coordination 
 
Source: Compiled by author. 
 
 
Ogunlana: Halting Boko Haram / Islamic State's West Africa Province Propaga
Produced by The Berkeley Electronic Press, 2019
#
# 
89 
 
Findings and Discussion 
 
Key findings of this study are as follows: that technology is useful in 
fighting the expansion and coordination of terrorism in cyberspace when 
properly integrated with other strategies. There has been little progress in 
countering the threat presented by terrorists’ propaganda in Nigerian 
cyberspace; a need exists to train and produce more experts with the 
requisite technical skills to dismantle terrorists’ websites and counter their 
messaging on social media platforms; and the government’s counternarrative
efforts on social media are ineffective. Cybersecurity 
technologies are useful and cost-effective tools to combat terrorist 
propagandas and networking in cyberspace. New artificial intelligence and 
machine learning tools enable prominent social media organizations to 
take down terrorist content faster than human moderators do. Nigeria 
constitution guarantees no protection for people spreading violence and 
hate speech in cyberspace. Law enforcement and intelligence agencies 
have inherent powers to listen to citizens’ communications, break 
offenders’ privacy, and collect evidence that can stop potential terrorist 
acts. The government of Nigeria is coordinating with the United States and 
other western allies in the areas of training and technology transfer. 
 
It is known that terrorist organizations are using online tactics to spread 
fear, panic, and present situations of uncertainty to the public. Social 
media is an efficient and convenient tool for terrorist groups because it has 
the capability of spreading short messages with blends of image, voice, and 
text. Every device such as laptop computers, desktop computers, mobile 
phones, and digital watches have Internet access capability and network to 
reinforce ideological beliefs and spin messages. Young people have been 
encouraged via social media to take terrorist action against their 
homelands. Several examples of the last few years indicate that ISIS 
mobilized young people via social media to travel and join jihadists in the 
war in Syria. Most fatawi issues by terrorist leaders are communicated to 
the public via social media. 
 
In addition, it is known that measures to counter online terror tactics 
remain inadequate and there is a need for a strategic solution to combat 
extremist narratives in cyberspace we know from the literature review that 
Journal of Strategic Security, Vol. 12, No. 1
https://scholarcommons.usf.edu/jss/vol12/iss1/4
DOI: https://doi.org/10.5038/1944-0472.12.1.1707
#
# 
90 
 
the cyberterrorists motivation is to instill fear so that targets will comply 
with their demands and ideology. The terrorist organizations are currently 
using cyber technologies to advance their programs such as recruiting, 
inciting, training, planning, and financial gain. There is a growing fear that 
terrorists can quickly acquire destructive capabilities. It is a known fact 
that the Internet provides unique opportunities to commit a crime and the 
terrorist organizations are using the opportunity to engage in information 
operation to terrorize the public with the message of fear, violence, and 
radical ideology. It is also known that the internet has become a routine 
activity in everyday life, which creates an opportunity for the 
cyberterrorists to target their victims with their messages. It is established 
from various studies that lack of guardianship both technical and 
legislative instruments may be part of the reasons for the rise in terrorist 
organizations uses of the social media to generate violent ideology and 
spread propaganda. 
 
What is not known, however, is how security experts and security 
administrators make their selection of cybersecurity technologies that best 
fit the status of technical guardianship in the cyberspace. Also, their 
perception of effectiveness and practicability in term of the kind of 
technology that has been most useful to stop terrorist networking in 
cyberspace. Routine Activity Theory may provide insights on how 
cyberspace can be best protected by excluding one of the three elements 
from the equation. Terrorists and cybercriminals are currently using social 
media technology to their advantage. Hence, there is a need to devise an 
adequate strategy that removes the protection that cyberspace offers to 
cyberterrorists. 
 
Ogunlana: Halting Boko Haram / Islamic State's West Africa Province Propaga
Produced by The Berkeley Electronic Press, 2019
#
# 
91 
 
Table 2. Names Provided by Interviewees of Selected Terrorist 
Organizations and Separatist Movements Known to Be Active in 
Nigerian Cyberspace  
Organizations Categories Modus (Website/Social media 
Boko Haram Terrorist 
organization 
Uses YouTube, Twitter, and 
Facebook and has an official web 
page in the form of a blog12 through 
which it publishes its propaganda 
and recruits members. 
http://www.usufislamicbrothers. 
blogspot.com 
Indigenous 
People of Biafra 
(IPOB) 
Nigeria 
government 
designated 
IPOB a terrorist 
organization on 
September 20, 
2017 
Active on the social media for 
recruitment, fundraising, and 
incitement. The group has official 
website: www.ipob.org 
Islamic State 
West Africa and 
the Movement 
for Unity & Jihad 
in West Africa 
Terrorist 
groups; both 
offshoots of 
ISIS and AlQaeda
in the 
Islamic 
Maghreb 
Social media platform. 
www.youtube.com 
Movement for 
Actualization of 
the Sovereign 
State of Biafra  
Separatist 
movement 
Active on social media: 
https://www.facebook.com/Massob170125269761711/.
Website: 
http://massob.biafranet.com/ 
Movement for 
the 
Emancipation of 
the Niger Delta  
Separatist 
movement 
Social media platform. 
www.youtube.com 
Source: Compiled by author. 
 
Journal of Strategic Security, Vol. 12, No. 1
https://scholarcommons.usf.edu/jss/vol12/iss1/4
DOI: https://doi.org/10.5038/1944-0472.12.1.1707
#
# 
92 
 
Figure 2. Cyber activities of Nigerian Terrorists Identified by 
Interviewees. 
 
Source: Author.  
 
What Experts are Saying About Cybersecurity Technologies 
 
The principal finding of this study is that measurement of the effectiveness 
of particular tools may not provide a holistic view of their performance. 
However, respondents explained that assessment of efficiency would be 
the ability of the law enforcement and intelligence agencies to manage 
both technology and human assets to achieve security goals. Experts 
interviewed posited that there is no standalone technology; agencies must 
develop skills, knowledge, and abilities needed leadership to make 
technology effective. In addition, respondents explained that the 
effectiveness of a given technology is measured based on cost and 
proportionality, including whether or not the technology achieved the 
expected security goal.  
 
Practicality.  
 
Participants interviewed argued that the application of cybersecurity 
technologies is practical in Nigeria’s context where law enforcement and 
42%
26%
17%
10%
5%
Propaganda
Surveillance
Recruitment
Fundraising
Crime
Ogunlana: Halting Boko Haram / Islamic State's West Africa Province Propaga
Produced by The Berkeley Electronic Press, 2019
#
# 
93 
 
intelligence agencies have acquired new legal powers to use cyber 
surveillance technologies to monitor potential terrorists’ communications.  
 
Terrorist organizations use all sorts of Internet-enabled communication 
technologies to expand their agenda. Experts interviewed spoke of “tools” 
identified to be useful, such as communication surveillance technologies, 
content monitoring tools, and listening devices such as frequency jammers 
and interception technologies. Security experts refer to all these 
equipment as “tools” or “technology.” 
 
Varieties of technologies are available for cyber security, depending on the 
technology used for terrorism activities. For instance, if it is known that 
terrorists use a mobile phone to communicate, frequency jammer may be 
appropriate. In addition, if the threat actors have posted offensive or 
violent messages on social media or private websites, in most cases, the 
choice is to work with the service providers to pull down the offensive 
communication with the aid of interception technology. All the experts 
agreed that content monitoring tools and installation of firewalls, coupled 
with other offensive applications are preventive tools that have been 
useful. The preventive techniques, which are synonymous with a 
hardening of the targets, are part of the government’s strategy of passive 
defense. Content monitoring applications are used to filter terrorist 
messages before they reach the public. The Nigerian Communications 
Commission, which is the regulator of the communications industry in 
Nigeria, can mandate all the service providers to implement the security 
measures that detect and discard terrorist communication before they 
become public. 
 
Computer-driven surveillance, such as voice and facial recognition with 
interpretation software application are excellent examples of a dynamic 
defense method. Technology-driven intelligence gathering approach is 
practical and less intrusive than traditional forms of surveillance and 
intervention. Security experts interviewed referred to surveillance 
technology as “tools” or “assets.” They described an asset as any 
equipment, person, facility, or information that has value and is controlled 
by a government. Intelligence agencies such as the Department of State 
Security Services, Nigeria Intelligence Agency, Directorate of Defense 
Journal of Strategic Security, Vol. 12, No. 1
https://scholarcommons.usf.edu/jss/vol12/iss1/4
DOI: https://doi.org/10.5038/1944-0472.12.1.1707
#
# 
94 
 
Intelligence, and the Office of the National Security Adviser coordinate 
and deploy various assets to gather intelligence in cyberspace to support 
counterterrorism operation. The importance of using intelligence tools is 
to gain advance knowledge of terrorist activities and dismantle their plan 
before an event occurs. In this context, the categories of surveillance 
technology identified are a range of technologies such as listening devices. 
In addition, it includes a frequency tracker, phone jamming tools, which 
monitor mobile phone calls, and content monitoring tools, which include 
emails and Internet activities. 
 
Furthermore, identification and authentication technology can reveal who 
used a computer or a phone to do what, including the location of the users. 
The question of non-repudiation can further be resolved through a legal 
process. In addition, the use of cryptographic technology for secured 
communication among law enforcement agents and intelligence and key 
stakeholders are the best options to avoid information leakages to 
terrorists and other bad actors. 
 
Overall, the experts viewed the Nigerian government as proactive about 
ensuring that all mobile SIM cards in Nigeria are registered. All the mobile 
phone companies operating in Nigeria are compelled to capture biometric 
features of their customers. The service providers can tell who is doing 
what with or using his/her mobile phone. In addition, they have to 
cooperate with the law enforcement agencies by providing all the 
information, such as call logs and information trails when necessary. For 
instance, the Chinese government has initiated a similar partnership with 
technology companies as a proactive measure. Kumar points out that 
China enacted a National Security Law that gives police the authority to 
partner with private technology companies to help them bypass encryption 
or other security tools to access sensitive personal data such as users’ 
emails, text messages, pictures, and the encryption keys that protect 
them.60 Giant technology companies like Apple are collaborating with the 
Chinese government to provide help when needed. Although this approach 
is raising significant privacy concerns among users and human right 
activists, the benefits outweigh the risks if the government is determined 
to combat terrorism in cyberspace. Meanwhile, law enforcement agents or 
investigators require a warrant or court order to obtain such information. 
Ogunlana: Halting Boko Haram / Islamic State's West Africa Province Propaga
Produced by The Berkeley Electronic Press, 2019
#
# 
95 
 
There are regulations and guidelines for obtaining citizens’ information to 
avoid the abuse of such powers. 
 
Experts support the notion that new media is a significant advantage to 
Boko Haram in Nigeria because it helps them keep their activities going 
while they remain elusive. The opportunity is out there already with the 
increasing sophistication in communication technologies. It is not possible 
for the authorities to deny bad actors access to internet infrastructures. 
However, the authorities can use cybersecurity technologies to thwart 
terrorists’ communication and expansion in cyberspace. 
 
With the evolution of artificial intelligence, there will be no more havens 
for terrorists operating in cyberspace. In fact, with artificial intelligence it 
is possible for security administrators to identify perpetrators and even 
block messages before they become public. In addition, the ability to trace 
activities back to the offenders with modern digital forensic tools is an 
added advantage. Meanwhile, all the participants pointed to lack of 
expertise within the intelligence and law enforcement agencies to apply 
the technologies effectively. Of course, while technology is good and 
effective, the authorities lack the expertise to deploy them effectively. 
 
As discussed earlier, one of the primary focuses of the Office of the 
National Security Adviser is institutional capacity building that includes 
facilities and human capacity. A need exist for the Nigerian government to 
engage indigenous technology companies and encourage research that can 
produce technologies that target adversaries based on the local threat 
landscape. At present, there is an apparent lack of confidence in locally 
made security solutions. The interpretation of these comments agreed 
upon by all participants indicates that the authority relies more on foreign 
experts and technologies.  
 
However, participants agreed that cybersecurity technologies are not a 
total panacea, but that the long-term strategy should be based on an 
educational program, deepening democratic culture, alleviating poverty, 
and expansion of opportunities for young people. The implication and 
interpretation are that technology alone cannot be used to fight terrorism 
and secure cyberspace. Their use must be comprehensive and 
Journal of Strategic Security, Vol. 12, No. 1
https://scholarcommons.usf.edu/jss/vol12/iss1/4
DOI: https://doi.org/10.5038/1944-0472.12.1.1707
#
# 
96 
 
accompanied by other social strategies, which include education and 
awareness programs.  
 
The security experts explained that the effectiveness of technologies is 
measured based on its technical capability to achieve the security goals. 
Cayford and Pieters point out that effectiveness is an impact that is 
desirable and recognized as contributing towards sought-after security 
goals.61 In addition, experts viewed the efficiency of cybersecurity 
technologies from the point of cost-benefit analysis. The justification is 
that the application of cybersecurity technologies is cheaper and easy to 
deploy compared with using human assets to perform the same function. 
Therefore, a risk assessment will enable a user to gauge the cost of security 
measures against the threat landscape and apply the best method. It is 
essential that the benefit of the outcome of the possible actions outweigh 
the risk to make it useful. All the experts interviewed pointed to the fact 
that technologies were helpful in the prevention and detection of several 
online attacks, especially the blocking and removal of terrorist contents 
before they reach mass audiences. They argue that the enormous spending 
on building infrastructures and training to support the use of technologies 
to combat cyberterrorism in Nigeria cyberspace is reasonable and the 
outcome is desirable.  
 
Ogunlana: Halting Boko Haram / Islamic State's West Africa Province Propaga
Produced by The Berkeley Electronic Press, 2019
#
# 
97 
 
Figure 3. Evaluating the Effectiveness of Cybersecurity 
Technologies as Described by Security Experts. 
 
Source: Author. 
 
Moreover, a key measure of the effectiveness of technologies arises from 
the number of lives saved based on intelligence collected through those 
tools to foil terrorists’ plans and recruitment agendas in cyberspace. In 
addition, a drone was used successfully as one of the cyber technologies to 
detect the location of the schoolgirls kidnapped by the terrorist group 
inside the expansive Sambisa Forest of Nigeria. The Global 
Counterterrorism Working Group (as cited in GCTF, 2017) states that it is 
essential for the government to recognize the role of ICT and technology 
companies as regards the availability and accessibility of terrorist content 
online62. My study has confirmed that law enforcement and intelligence 
agencies are helpless without the full cooperation of the technology 
companies. In addition, the study confirms that weak leadership, 
nepotism, corruption, and lack of technological expertise continue to 
plague the agencies responsible for security in Nigeria. Political instability, 
ethnic and religious influence, and inefficiency are compounding the 
potential vulnerabilities to terrorism. Terrorist organizations in Nigeria 
will recruit more “lone wolf” attackers, including suicide bombers in 
 
 
Measures of Effectiveness 
Support 
Output use in 
prosecution of 
terrorism cases 
Decision Makers 
informed 
Foiled 
attacks 
Results 
Journal of Strategic Security, Vol. 12, No. 1
https://scholarcommons.usf.edu/jss/vol12/iss1/4
DOI: https://doi.org/10.5038/1944-0472.12.1.1707
#
# 
98 
 
cyberspace. Boko Haram and ISIS are frequently using encryption 
technology to communicate unnoticed, including the dark web, and 
cryptocurrencies to recruit new members and spread propaganda. Given 
the level of increasing sophistication of cyber capabilities of young people 
in Nigeria, it would be easy in future to acquire a capacity to make 
weapons of mass destruction that can facilitate their operations, including 
weapons to conduct physical attacks against targets. Given the level of 
poverty, government repression, and lack of opportunities for young 
people, the country is a fertile ground for terrorist recruitment. In future, 
terrorists and armed groups in Nigeria will attack critical infrastructures, 
including financial and aviation systems, using cyberspace. It is just a 
matter of when it will happen, hence the need for adequate response. 
 
The findings revealed that the integration of technology in 
counterterrorism strategy creates a new opportunity to develop new 
technologies and technical skills. Emerging technologies such as artificial 
intelligence and machine learning algorithm are pointers to the future 
possibilities of combating enemies in the cyberspace. Technologies remain 
viable instruments to subdue terrorists’ influence in cyberspace. The 
technological approach is a top priority for law enforcement and 
intelligence communities. The study findings provide insights into the 
application of a combination of strategies in support of technology to 
secure Nigerian cyberspace. Nigeria’s national policy on cybersecurity is a 
guiding document for intelligence organizations, law enforcement 
agencies, and other security apparatuses in Nigeria.  
 
In addition, the findings revealed that the effectiveness of technologies is 
measured based on its technical capability to achieve the security goals. 
Experts clarified that technologies are selected based on risks and must be 
combined with other strategies to make a good result. Content monitoring 
tools, firewalls, and identification and authentication tools that guarantee 
non-repudiation and other technology-driven intelligence gathering tools 
have been useful. A combination of surveillance and interception 
technologies are considered effective based on the numbers of attacks 
thwarted by the Nigerian Secret Police. Findings on the practicality of the 
application of the technologies reveal that law enforcement and 
intelligence agencies have acquired new legal powers to use cyber 
Ogunlana: Halting Boko Haram / Islamic State's West Africa Province Propaga
Produced by The Berkeley Electronic Press, 2019
#
# 
99 
 
surveillance technologies to monitor potential terrorists’ communications. 
Therefore, the use of technology is feasible in all situations as it is known 
to be less intrusive and regulated by law against abuse. Based on the 
findings above, I present some recommendations below with a focus on 
how to advance the efforts to mitigate the expansion and organization of 
terrorism in cyberspace. 
 
Recommendations 
 
Research and analysis enable academia to provide innovative thinking and 
perspectives on threats. Therefore, academia plays a significant role in 
counterterrorism. Given the strengths and limitations of this study, the 
following recommendations are presented in two segments. The first part 
focuses on recommendations to the government of Nigeria as related to 
the findings, while the second section focuses on areas for future research. 
 
Recommendations for Future Study 
 
First, this research focused on the role of technology in mitigating the 
expansion of terrorism in cyberspace. There is a need for further 
quantitative research to measure the effectiveness of cybersecurity 
technologies in Nigerian cyberspace. 
 
Second, there is a need for research in the area of international law 
enforcement cooperation on the use of technologies. Future research 
should include analysis of jurisdictional problems regarding the 
investigation of acts of terrorism in the West African region. Consideration 
should be given to how law enforcement cooperation should be put into 
practice. Future studies should focus on how international bodies, such as 
Interpol, Europol, and other regional training centers facilitate 
cooperation by examining the principles of those entities and how they 
implement them. 
 
Third, future research should focus on how terrorist organizations might 
use ICT infrastructure for prospective attacks after being frustrated out of 
online platforms. Fourth, given the Russian cyber-operation noticeable in 
Journal of Strategic Security, Vol. 12, No. 1
https://scholarcommons.usf.edu/jss/vol12/iss1/4
DOI: https://doi.org/10.5038/1944-0472.12.1.1707
#
# 
100 
 
the last US election, nation-state actors like Russia should replicate this 
study with a focus on information operations. 
 
Recommendations for the Nigerian Government 
 
First, based on the assessment of Research Question, technology is 
significant in fighting the expansion and organization of terrorism in 
cyberspace. As part of a long-term strategy, the government of Nigeria 
must work with the ministry of education in Nigeria to develop curricula to 
groom future cyber experts. This step will create expertise and inspire 
students to pursue the profession. Technology is a national security tool. 
Hence, Nigeria’s cyber-capabilities must be established and well 
documented. 
 
Collaboration is essential in the fight against the organization and 
expansion of terrorism in cyberspace. As recommended in the Country 
Report 2016 released by the US Department of State, it is essential for 
Nigeria’s government to strengthen its bilateral and multilateral 
relationships.63Such ties will facilitate technology transfer and information 
sharing among ally states and partners. Cooperation and training to 
prepare for contingencies is the only proper way to guarantee results in the 
event of a terrorist emergency. 
 
In addition, there is a need to create a military cyber command or an 
agency that will mirror the United State Defense Advanced Research 
Projects Agency (DARPA). While a cyber command would safeguard 
information security in the armed forces and across the entire 
infrastructure of Nigeria, a similar agency to DARPA would focus on 
developing emerging technologies for use by the military. The agency will 
facilitate technological research, including capacity building to train 
people and create varieties of software that will enable the government to 
develop a centralized protection system shielding. 
 
Public partnerships and industry alliances are critical for technology and 
human development. Nigeria’s government should implement a series of 
information security measures based on a public-private partnership 
strategy in order to overcome the technological lag in its cybersecurity 
Ogunlana: Halting Boko Haram / Islamic State's West Africa Province Propaga
Produced by The Berkeley Electronic Press, 2019
#
# 
101 
 
science. Further, it is essential to strengthen the criminal justice system 
with a focus on prioritizing how to investigate and prosecute suspected 
terrorism cases. 
 
Finally, there is a need to commit to the rule of law. It is essential to 
establish and maintain international standards of accountability. 
Commitment to the rule of law is highly crucial in the war against 
terrorism.  
 
Conclusions 
 
Emerging technologies play an essential role in countering the expansion 
and organization of terrorism in cyberspace. The study’s findings suggest 
that the application of technologies would be a solution to combat 
cyberterrorism activities. Terrorist organizations have shifted the 
battleground to cyberspace because it is a cheap alternative to 
communicate and coordinate activities with a high level of anonymity. 
Boko Haram and other terrorist groups operating from Nigeria have used 
cyberspace to talk to the world, sending fearful content, incitement, and 
violent messages. 
 
The technological approach complemented with other strategies is the 
future of the fight against the expansion of extremist ideology, 
mobilization, coordination, and terrorist influence in cyberspace. For 
instance, technologies such as artificial intelligence are crucial and will 
have enormous impact and boost counterterrorism efforts. As such, the 
role of cybersecurity technologies in tackling myriad cyber threats, 
including terrorists’ activities in online platforms cannot be 
overemphasized. 
 
The findings reveal that technologies are useful for cyberspace patrol, 
surveillance, intelligence gathering, and prevention. Technologies are 
selected based on the risks and must be tailored towards deterrence, 
detection, prevention, and response. The findings confirm that the issue of 
civil liberty is better overcome when the technological strategy is 
combined with other strategies, including regulation, law, and procedures. 
 
Journal of Strategic Security, Vol. 12, No. 1
https://scholarcommons.usf.edu/jss/vol12/iss1/4
DOI: https://doi.org/10.5038/1944-0472.12.1.1707
#
# 
102 
 
The study identified the importance of international cooperation and 
proposed that the Nigerian government strengthen its bilateral and 
multilateral cooperation. International collaboration will facilitate 
technology transfer, training, funding assistance, and information sharing. 
Nigeria should establish a standard in line with international laws while 
dealing with the issue of terrorism. 
 
  
Ogunlana: Halting Boko Haram / Islamic State's West Africa Province Propaga
Produced by The Berkeley Electronic Press, 2019
#
# 
103 
 
Endnotes 
                                                 
1 Osho Oluwafemi, Falaye Adeyinka Adesuyi, and Shafi’I M. Abdulhamid ,"Combating 
Terrorism with Cybersecurity: The Nigerian Perspective" World Journal of Computer 
Application and Technology 1, no. 4 (2013): 103-109, 
http://www.hrpub.org/journals/article_info.php?aid=909.  
2 Aliyu Odamah Musa, "Socio-economic Incentives, New Media and the Boko Haram 
Campaign of Violence in Northern Nigeria," Journal of African Media Studies 4, no. 1 
(2012): 111-124., https://doi.org/10.1386/jams.4.1.111_1. 
3 Aliyu, “Socio-economic Incentives, New Media and the Boko Haram Campaign of 
Violence in Northern Nigeria.” 
4 US Army Training and Doctrine Command. "Critical Infrastructure." 
5 Kaplan, Eben. "Terrorists and the Internet." Council on foreign Relations 8 (2009). 
6 Adomi, Esharenana E., and Stella E. Igun. "Combating cyber crime in Nigeria." The 
Electronic Library 26, no. 5 (2008): 716-725. 
7 Berger, John M. "The evolution of terrorist propaganda: The paris attack and social 
media." The Brookings Institution (2015). 
8 Reyns, Bradford W., Billy Henson, and Bonnie S. Fisher. "Being pursued online: 
Applying cyberlifestyle–routine activities theory to cyberstalking victimization." Criminal 
justice and behavior 38, no. 11 (2011): 1149-1169, doi.org/10.1177/0093854811421448. 
9. Grabosky, Peter N. "Virtual criminality: Old wine in new bottles?." Social & Legal 
Studies 10, no. 2 (2001): 243-249. 
10 Grabosky, "Virtual criminality: Old wine in new bottles?"  
11 Foley, Frank. "Reforming counterterrorism: Institutions and organizational routines in 
Britain and France." Security Studies18, no. 3 (2009): 435-478, 
doi:10.1080/09636410903132920 
12 Cohen, Lawrence E., and Marcus Felson. "Social Change and Crime Rate Trends: A 
Routine Activity Approach (1979)." In Classics in Environmental Criminology, pp. 203232.
CRC Press, 2016. http://www.personal.psu.edu/users/e/x/exs44/597b-
Comm%26Crime/Cohen_FelsonRoutine-Activities.pdf 
13 Creswell, John W. "Qualitative Inquiry & Research Design Choosing Among Five 
Approaches. Sage Publications." Thousand Oaks, CA (2007). 
14 Rubin, Herbert J., and Irene S. Rubin. Qualitative interviewing: The art of hearing 
data. Sage, 2011. 
15 Minei, Elizabeth, and Jonathan Matusitz. "Cyberspace as a new arena for terroristic 
propaganda: an updated examination." Poiesis & Praxis 9, no. 1-2 (2012): 163-176, 
https://link.springer.com/article/10.1007/s10202-012-0108-3. 
16. Al Mazari, Ali, Ahmed H. Anjariny, Shakeel A. Habib, and Emmanuel Nyakwende. 
"Cyber terrorism taxonomies: Definition, targets, patterns, risk factors, and mitigation 
strategies." In Cyber Security and Threats: Concepts, Methodologies, Tools, and 
Applications, pp. 608-621. IGI Global, 2018. 
17. Douglas, Schweitzer. "Be prepared for cyberterrorism," Computerworld, March 28, 
2005,https://www.computerworld.com/article/2556419/security0/be-prepared-forcyberterrorism.html.
18. Robert, Murrill. "The Question Of Cyber Terrorism," Forensic Focus, July 23, 2011, 
https://articles.forensicfocus.com/2011/07/23/the-question-of-cyber-terrorism/ 
19. Martin, Susanne, and Leonard B. Weinberg. "Terrorism in an era of unconventional 
warfare." Terrorism and political violence 28, no. 2 (2016): 236-253. 
20. Denning, Dorothy E. "Cyberterrorism." (2000), 
http://palmer.wellesley.edu/~ivolic/pdf/Classes/Handouts/NumberTheoryHandouts/Cy
berterror-Denning.pdf 
Journal of Strategic Security, Vol. 12, No. 1
https://scholarcommons.usf.edu/jss/vol12/iss1/4
DOI: https://doi.org/10.5038/1944-0472.12.1.1707
#
# 
104 
 
                                                                                                                                     
21. Che, Eliot. Securing a Network Society: Cyber-terrorism, International Cooperation 
and Transnational Surveillance. Research Institute for European and American Studies 
(RIEAS), 2007. 
22. Al Mazari et al., "Cyber terrorism taxonomies." 
23. Rogan, Hanna. "JIHADISM ONLINE-A study of how al-Qaida and radical Islamist 
groups use the Internet for terrorist purposes." FFI/Report 915 (2006): 2006., 
http://aseanregionalforum.asean.org/files/Archive/16th/ARF-Conference-on-TerroristUse-of-the-Internet-Bali-6-
8November2008/Stinson's%20Presentation/JIHADISM%20ONLINE%20AQ%20AND%
20GPS.pdf 
24. Martin et al., "Terrorism in an era of unconventional warfare." 
25. Goodman, Seymour E. "Cyberterrorism and Security Measures." Kumar, Arvind et al, 
eds (2007): 43-54. 
26. Conway, Maura. "Reality bytes: cyberterrorism and terrorist'use'of the Internet." First 
Monday 7, no. 11 (2002). 
https://ezp.waldenulibrary.org/login?url=https://search.ebscohost.com/login.aspx?dire
ct=true&db=conedsqd6&AN=edsair.od.......119..1adb6237029a4fd3e6efc42e0a68a250&si
te=eds-live&scope=site 
27. Benson, David C. "Why the internet is not increasing terrorism." Security Studies 23, 
no. 2 (2014): 293-328. 
28. Minei, Elizabeth, and Jonathan Matusitz. "Cyberspace as a new arena for terroristic 
propaganda" 
29. Weimann, Gabriel. "Virtual disputes: The use of the Internet for terrorist 
debates." Studies in conflict & terrorism 29, no. 7 (2006): 623-639. 
doi.org/10.1080/10576100600912258 
30. Jack, Moore. "ISIS supporters call for poisoning..." 
31. Benson, "Why the internet is not increasing terrorism."  
32. Benson, "Why the internet is not increasing terrorism."  
33. Hoffman, Bruce. "The Global Terror Threat and Counterterrorism Challenges Facing 
the Next Administration." CTC Sentinel 9, no. 11 (2016): 1-8. 
34. McDowell-Smith, Allison, Anne Speckhard, and Ahmet S. Yayla. "Beating ISIS in the 
digital space: Focus testing ISIS defector counter-narrative videos with American college 
students." Journal for Deradicalization 10 (2017): 50-76. 
35. Jack, Moore. "ISIS supporters call for poisoning of food in grocery stores across U.S. 
and Europe," Newsweek, September 7, 2017, https://www.newsweek.com/isissupporters-call-poisoning-grocery-stores-us-and-europe-
660750?utm_campaign=NewsweekFacebookSF&utm_source=Facebook&utm_medium
=Social 
36. Hoffman, Bruce. "The Coming ISIS–al Qaeda Merger." Foreign Affairs (2016), 
https://www.foreignaffairs.com/articles/2016-03-29/coming-isis-al-qaeda-merger 
37. Rogan, Hanna. "JIHADISM ONLINE-A study of how al-Qaida and radical Islamist 
groups use the Internet for terrorist purposes." FFI/Report 915 (2006): 2006., 
http://aseanregionalforum.asean.org/files/Archive/16th/ARF-Conference-on-TerroristUse-of-the-Internet-Bali-6-
8November2008/Stinson's%20Presentation/JIHADISM%20ONLINE%20AQ%20AND%
20GPS.pdf 
38 Berger, John M. "The evolution of terrorist propaganda: The paris attack and social 
media." The Brookings Institution (2015). 
39 "Encrypted app allows extremists to plot attacks without detection," Homeland 
Security News Wire, August 22, 2017, 
http://www.homelandsecuritynewswire.com/dr20170809-encrypted-app-allowsextremists-to-plot-attacks-without-detection
Ogunlana: Halting Boko Haram / Islamic State's West Africa Province Propaga
Produced by The Berkeley Electronic Press, 2019
#
# 
105 
 
                                                                                                                                     
40 Aly, Anne, Stuart Macdonald, Lee Jarvis, and Thomas M. Chen. "Introduction to the 
special issue: Terrorist online propaganda and radicalization." Studies in Conflict & 
Terrorism, 40(1), 1–9. doi:10.1080/1057610X.2016.1157402 (2017): 1-9. 
41 Baken, D. "Cyber warfare and Nigeria’s vulnerability’." E-International Relations 3 
(2013). 
42 Agbiboa, Daniel, and Benjamin Maiangwa. "Why Boko Haram kidnaps women and 
young girls in north-eastern Nigeria." conflict trends 2014, no. 3 (2014): 51-56. 
43 Dan, Lohmann. "How terrorists’ use of social media points to the future," GOVTECH, 
June 20, 2016, http://www.govtech.com/em/safety/Terrorists-And-Social-Media.html 
44. Liang, Christina Schori. "Cyber Jihad: Understanding and Countering Islamic State 
Propaganda." GSCP Policy Paper 2 (2015): 4., 
https://www.jugendundmedien.ch/fileadmin/user_upload/3_Medienkompetenz/Gegen
narrative/Cyber_Jihad__Understanding_and_Countering_Islamic_State_Propaganda.pdf
45. Hubler, David. IAFIE conference concludes with calls for more intelligence education 
programs. IN Homeland Security, May 26, 2017,  http://inhomelandsecurity.com/iafieconference-concludes-calls-intelligence-education-programs/
46. Berger, "The evolution of terrorist propaganda.” 
47. Berger, "The evolution of terrorist propaganda.” 
48. Maria Korolov. "How AI can help you stay ahead of cybersecurity threats," CSO Online, 
October 19, 2017, https://www.csoonline.com/article/3233951/machine-learning/howai-can-help-you-stay-ahead-of-cybersecurity-threats.html.
49. Newton Lee. "Artificial Intelligence and data mining,". Counterterrorism and 
Cybersecurity (pp. 323–341). Cham.: Springer, 2015. 
50. Mike Isaac. "Facebook and Other Tech Companies Seek to Curb Flow of Terrorist 
Content," New York Times, December 5, 2016, 
https://www.nytimes.com/2016/12/05/technology/facebook-and-other-tech-companiesseek-to-curb-flow-of-terrorist-content.html
51. Schultz, Robert William. "Countering Extremist Groups in Cyberspace." Joint Forces 
Quarterly 79, no. 4, 2015. http://ndupress.ndu.edu/Portals/68/Documents/jfq/jfq79/jfq-79_54-56_Schultz.pdf
52. Kaplan, Eben. "Terrorists and the Internet." Council on foreign Relations 8 (2009). 
53. Schultz, "Countering Extremist Groups in Cyberspace."  
54. Thomas Holt. "Here's how terrorist groups use technology to recruit new members," 
Business Insider, April 28, 2016, http://www.businessinsider.com/heres-how-terroristgroups-use-technology-to-recruit-new-members-2016-4
55. Adomi, Esharenana E., and Stella E. Igun. "Combating cyber crime in Nigeria." The 
Electronic Library 26, no. 5 (2008): 716-725. 
56. Sageman, Marc. "Understanding terror networks." International journal of emergency 
mental health 7, no. 1 (2005): 5-8. 
57. Foley, Frank. "Reforming counterterrorism: Institutions and organizational routines in 
Britain and France." Security Studies18, no. 3 (2009): 435-478, 
doi:10.1080/09636410903132920 
58. Osho, Oluwafemi, and Agada D. Onoja. "National Cyber Security Policy and Strategy of 
Nigeria: A Qualitative Analysis." International Journal of Cyber Criminology 9, no. 1 
(2015). 
59. Jose Pagliery, Jamie Crawford and Ashley. "CENTCOM Twitter account hacked, 
suspended," CNN, January 12, 2015, http://www.cnn.com/2015/01/12/politics/centcomtwitter-hacked-suspended/index.html
60. Goodman, Seymour E. "Cyberterrorism and Security Measures." Kumar, Arvind et al, 
eds (2007): 43-54. 
Journal of Strategic Security, Vol. 12, No. 1
https://scholarcommons.usf.edu/jss/vol12/iss1/4
DOI: https://doi.org/10.5038/1944-0472.12.1.1707
#
# 
106 
 
                                                                                                                                     
61. Cayford, Michelle, and Wolter Pieters. "The effectiveness of surveillance technology: 
What intelligence officials are saying." The Information Society 34, no. 2 (2018): 88-103. 
https://doi-org.ezp.waldenulibrary.org/10.1080/01972243.2017.1414721 
62. "Countering violent extremism: Zurich-London recommendations on preventing and 
countering violent extremism and terrorism online," Global Counterterrorism Forum, 
September 15, 2015, 
https://www.thegctf.org/Portals/1/Documents/Framework%20Documents/A/GCTF%20
-%20Zurich-London%20Recommendations%20ENG.pdf?ver=2017-09-15-210859-467. 
63. US Department of State, “Country Report 2016.” 
Ogunlana: Halting Boko Haram / Islamic State's West Africa Province Propaga
Produced by The Berkeley Electronic Press, 2019
